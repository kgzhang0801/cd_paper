\documentclass[twoside,11pt]{article}
\usepackage{jmlr2e}
\usepackage{setspace}
%\singlespacing
\onehalfspacing % Line space
\usepackage{mathtools} % vertical centered colon
\usepackage{amsmath} % For \argmax
\DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays
\usepackage{lineno}
\linenumbers
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\setlength{\marginparwidth}{2.5cm} % For notes in margin, can be delete for final version.

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\DeclareMathOperator{\Tr}{Tr}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\jmlrheading{1}{2018}{1-48}{4/00}{10/00}{meila00a}{Kungang Zhang, Anh T. Bui, and Daniel W. Apley}

\definechangesauthor[name={Per cusse}, color=brown]{Kungang}
\definechangesauthor[name={Per cusse}, color=orange]{Anh}
\definechangesauthor[name={Per cusse}, color=red]{Apley}
% \setremarkmarkup{(#2)}

% Short headings should be running head and authors last names

\ShortHeadings{Zhang, Bui, and Apley}{}
\firstpageno{1}

\begin{document}

\title{Concept Drift Monitoring and Diagnostics of Supervised Learning Models via Score Functions}

\author{\name Kungang Zhang \email zkg@u.northwestern.edu \\
	\name Anh T.\ Bui \email buiat2@vcu.edu \\
	\name Daniel W.\ Apley \email apley@northwestern.edu \\
       \addr Department of Industrial Engineering and Management Sciences\\
       Northwestern University\\
       Evanston, IL 60208-3119, USA\\
       Department of Statistical Sciences and Operations Research\\ 
       Virginia Commonwealth University\\
       Richmond, VA 23284-3083, USA}

\editor{EDITOR NAMES}
% Author guide: http://www.jmlr.org/format/authors-guide.html
\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
Supervised learning models are widely adopted in many applications to automate business processes or decision-making. They are also one of the most fundamental classes of models used to achieve other machine learning tasks, such as unsupervised learning and reinforcement learning. Viewing supervised learning from a probabilistic perspective, the set of training data to which the model is fitted is assumed to follow a stationary distribution, regardless of when the observations are collected. However, this stationarity assumption is often violated in a phenomenon as known as concept drift, which refers to changes over time in the predictive relationship between covariates $\bm{X}$ and a response variable $Y$ and can render trained models suboptimal or obsolete. In this study, we develop a comprehensive and computationally efficient framework for detecting, monitoring, and diagnosing concept drift. Specifically, we monitor the Fisher score function, defined as the gradient of the log-likelihood for the fitted model, using a form of multivariate EWMA, which was originally designed to monitor for general changes in the mean of a random vector. In spite of the substantial performance advantages that we demonstrate over popular error-based methods, a score-based approach appears to have not been previously considered for concept drift monitoring. Advantages of the proposed score-based framework include versatility for use with any parametric supervised learning model, faster detection of changes in the predictive relationship, better sensitivity as shown in theory and experiments, and inherent diagnostic capabilities for helping identify the nature of the changes, which we also develop in this work.
\end{abstract}

\begin{keywords} 	
  Concept Drift, Score Function, Control Chart, Streaming Data, Predictive Model, Multivariate EWMA
\end{keywords}

\section{Introduction}
Supervised learning is widely used in many applications, in which models are trained to predict a response variable $Y$, given an observed set of covariates $\bm{X} \in \mathbb{R}^p$. The modeling goal is usually to make test prediction accuracy metrics (i.e. $R^2$, classification accuracy, F1-score, etc.) as high as possible. The supervised learning model can also act as an intermediate module in other machine learning tasks, such as dimension reduction in unsupervised learning or Q-Learning in reinforcement learning. Training the supervised learning model can be viewed as an optimization problem. From a probabilistic perspective, a training sample of observations, $\{\bm {x}_i, y_i\}_{i=1}^n$, is assumed to be drawn from some joint distribution, $P (\bm {X}, Y)$, such that the conditional distribution, $P(Y|\bm{X};\bm{\theta})$, is stationary, no matter when those samples are collected~(\cite{hulten2001mining}). Here, {$\bm {x}_i \in \mathbb{R}^{p}$ and $y_i \in \mathbb{R}$ are the covariates and response variable in the $i$th observation respectively}\footnote{The case with scalar response can easily be extended to vector response variable $\bm {y}_i$.}, and we have parametrized the conditional distribution via the vector $\bm{\theta}$. However, the stationarity assumption is often violated in real applications, a phenomenon as known as concept drift~(\cite{moreno2012unifying,vzliobaite2016overview}). For example, models used to predict customers' probabilities of defaulting by credit-scoring agencies are usually fitted to training data collected over a three to five years period, so that changing financial environments may result in an outdated predictive relationship by the time the trained model is used to score new customers~(\cite{crook1992degradation,vzliobaite2016overview}). 

Concept drift poses many challenges in constructing trustworthy supervised learning models. Concept drift during training (historical) and testing (future) data both can degrade performance of supervised learning models: during training, concept drift results in there being no single predictive model, since the model is changing over time; while concept drift during testing degrades the accuracy of the predictive model, relative to what it could be with an updated model for $P(Y|\bm{X};\bm{\theta})$. In order to obtain and maintain the highest possible predictive performance of supervised learning models, \textit{retrospective} concept drift analysis of the training data and \textit{prospective} concept drift monitoring of the test data are important and should be a standard component of the predictive modeling process.

Concept drift is fairly common in practice and often originates from changes of some hidden effects in generating the response outcomes of interests~(\cite{tsymbal2008dynamic,vzliobaite2012beating,widmer1996learning,kukar2003drifting,donoho2004early,carmona2010gnusmail,fong2015change,vzliobaite2016overview}). For example, in the problem of antibiotic resistance in nosocomial infections, supervised learning models are trained and validated using a data set, with response labels indicating whether the level of sensitivity of a pathogen to an antibiotic is sensitive, resistant, or intermediate, and the covariates being patients' demographical data and conditions of hospitalization~(\cite{pechenizkiy2005knowledge}). After properly training and validating such a model, it can with high accuracy determine whether or not a pathogen is sensitive to an antibiotic or not. However, according to medical experts~(\cite{kukar2003drifting}), hidden effects due to failures and/or replacements of some medical equipment, changes in personnel, and seasonal bacterial outbreaks could result in unexpected changes over time in resistance of new pathogen strains to antibiotics. In other words, there is likely a significant level of concept drift in nosocomial infections. Consequently, bacteria that are predicted to be sensitive to a particular antibiotic may now be resistant, and the errors of the predictive model becomes unacceptable. 

The problem of concept drift is gaining increasing attention, because increasingly data are organized in the form of data streams, and it is often unrealistic to expect that data distributions remain stable for a long period of time. Most state-of-the-art concept drift detection and adaptation methods, like ensemble methods~(\cite{wang2003mining}), neural networks~(\cite{calandra2012learning}), and other non-parametric methods~(\cite{bifet2007learning,frias2015online}), are based on monitoring the classification error or some metrics derived from it~(\cite{ross2012exponentially,gonccalves2014comparative,barros2018large}). The main purpose of this paper is to introduce a new and comprehensive concept drift framework that is based on monitoring the Fisher scores of the individual observations over time, as opposed to metrics derived from their classification errors. The Fisher score, which we will refer to as simply the score, of an observation $(\bm{X}, Y)$ is defined as the gradient of the log-likelihood, $\log(P(Y|\bm{X};\bm{\theta}))$, with respect to the parameters, $\bm{\theta}$. As shown in the following sections, comparisons between the score-based and error-based methods from both theoretical and empirical perspectives provide strong justification for a score-based method. The score-based framework that we develop and demonstrate is intended to detect, monitor, and diagnose concept drift with the following major advantages and attractive properties:

\begin{itemize}
\item
\textit{The approach is based on monitoring for changes in the mean of the score function, which has strong theoretical justification since theory dictates that the mean of the score function changes if and only if $P(Y|\bm{X};\bm{\theta})$ changes, under fairly general conditions. We elaborate on this in Section~\ref{ss:score_func}.}
\item
\textit{In contrast, existing concept drift methods that are based on monitoring the error rate may fail to detect concept drift, because a change in $P(Y|\bm{X};\bm{\theta})$ does not necessarily result in a change in the error rate. Note that a change in $P(Y|\bm{X};\bm{\theta})$ that does not change the error rate still means that the predictive accuracy of the model can be improved if the model is updated accordingly. So it is still desirable to detect such changes. See, e.g., Figure~\ref{fig:logi_err_rate_unch} and Figure~\ref{fig:exp_no_err_ch}.}
\item
\textit{The score-based method is more sensitive to changes in $P(Y|\bm{X}; \bm{\theta})$ and thus more quickly detects concept drifts than error-based methods, as summarized in Section~\ref{ss:real_data} and detailed in Appendix~\ref{ss:simu_MRL} and the empirical examples.}
\item
\textit{The score-based method is applicable to any parametric classification or regression model that can be interpreted probabilistically in terms of $P(Y|\bm{X};\bm{\theta})$, parameterized by a vector of parameters $\bm{\theta}$.}
\item
\textit{The score-based approach provides a convenient means of diagnosing the nature of the concept drift (e.g., which parameters have changed) as derived in Section~\ref{s:decou_cd} and demonstrated in Section~\ref{s:real_data} and Appendix~\ref{ss:cd_diag}.}
\item
\textit{The score-based perspective converts the concept drift problem to the problem of monitoring changes in the mean of a random vector, for which many existing multivariate statistical process control (SPC) methods are available. We focus on a multivariate exponentially weighted moving average (MEWMA) control chart~(\cite{montgomery2007introduction}), which has desirable characteristics for our problem (see Section~\ref{ss:MEWMA}), but as new methods are developed they can be applied directly.}
\item
\textit{The computations involved in the score-based approach are almost the same computations involved in stochastic gradient descent (SGD) algorithms, which are increasingly commonly used to fit parametric supervised learning models. In this sense, the computations come at very little additional cost, resulting in a computationally inexpensive approach. We discuss this in Section~\ref{ss:sgd_score}.}
\item
\textit{It provides a consistent framework for using either retrospectively (e.g., after fitting a supervised learning model to a set of training data, to validate that the training data were stable and, if not, provide diagnostic information as to why) or prospectively (e.g., when using a fitted supervised learning model to predict new cases, to quickly signal when the predictive model has changed, indicating that it is time to update the model).}
\end{itemize}

% The score-based method monitors score vectors (realizations of the score function), using multivariate exponentially weighted moving average (MEWMA) control charts~(\cite{montgomery2007introduction}). In the paper, we use ``the score function" for the gradient of log-likelihood as a function of parameters and use ``score vectors" for evaluation of the score function given specific data observation $(\bm {x}_i,y_i)$ and value of parameters. {We also develop a diagnostic procedure, using (univariate) exponentially weighed moving average (EWMA) control charts to monitor each component of transformed sample score vectors, for diagnosing the nature of detected drifts.}

% The MEWMA control chart has been investigated and comes as the top candidate in monitoring mean changes of a vector of statistics. It has advantages of detecting the mean change in a direction invariant manner and being robust to high noise of monitored vectors. Those properties will be explained in details in Section~\ref{ss:MEWMA}.

% The sample score vectors used in monitoring can be automatically produced in prediction step and stochastic gradient descent as the derivative of the marginal log-likelihood, {$\nabla _{\bm { \theta}} \log(P (y_i|\bm {x}_i, \bm { \theta}))$}. Thus, monitoring score function requires very little extra calculation and can be easily integrated into existing systems as a component of data exploration and model performance monitoring. The discussion of the effect of using SGD on the convergence of parameters and MEWMA control charts is in Section~\ref{ss:sgd_score}.

% Because of those nice properties in methodological and practical perspectives, the score-based method is very general and can be applied onto any models which can obtain derivatives with respect to parameters, like generalized linear models or neural network; it can borrow power of any mean monitoring methods from decades of research in the field of SPC; and integration of this method into existing systems would not be computationally expensive. 

% Practically, the score-based method would be used in following ways. First, it can justify the stationarity assumption of training data set, retrospectively. Then, it can provide quantitative guidance on when the model should be updated or retrained in predicting new observations, on occurrence of a drift. Third, insights obtained from diagnosis with this method offer interpretation to the nature of drifts and starting point for resolutions, which are highly valuable in many industries (e.g. business, insurance, and health care, etc). For example, in financial industry, there are many laws to ensure that some key business processes such as determining who qualifies for lines of credit must comply with fair lending laws~(\cite{chen2018fair}) such as the Equal Credit Opportunity Act (ECOA)~(\cite{hsia1978credit}).

% In order to demonstrate properties of this method, simulation data sets with different models (linear, logistic, multinomial, and Poisson) are tested. Two real data sets (credit default and Capital Bikeshare) of classification and regression are the case studies supporting advantages of the score-based method.


We elaborate on these properties in Sections~\ref{s:theory_analysis_score} and~\ref{s:decou_cd} and provide simulation results demonstrating them in Section~\ref{s:real_data}. Two real examples (credit risk scoring and bike sharing demand prediction) in Section~\ref{s:real_data} serve as case-studies to further illustrate its usage.

\section{Relation to Prior Work}
In this section, we briefly review relevant existing literature on monitoring different types of drift, in terms of goals and methodologies. The concept drift problem is of increasing importance, because data sets in real applications are often generated in dynamic environments such that the distribution of the data changes over time. The impact of such temporally dependent data distributions on model training and prediction depends on the particular type of drift. In the literature on data set drift, the terminology is not always consistent. In this work, we follow what appears to be the most common terminology according to~\cite{moreno2012unifying} and~\cite{vzliobaite2016overview}. In general, any temporal drift in the joint distribution $P(\bm {X}, Y)$ is called ``data set drift". Decomposing the joint distribution as $P(\bm{X}, Y) = P(Y|\bm{X})P(\bm {X})$, ``concept drift" refers to temporal drift in $P(Y|\bm{X})$ (which is what the fitted supervised learning model approximates), while ``covariate drift" refers to temporal drift in $P(\bm{X})$.

In general, the methods in the concept drift literature can be categorized into two classes~(\cite{tsymbal2004problem}): 1) model adaptation methods and 2) concept drift detection methods.

Model adaptation methods mainly focus on maintaining the performance of machine learning models in the presence of concept drift, without formally detecting or diagnosing the drift~(\cite{wang2003mining,tsymbal2008dynamic,gonccalves2014comparative,barros2018large}). To maintain a good prediction metric (classification or regression error), models are automatically updated (i.e., adapted) online continuously as new observations are collected, which is sometimes called online or incremental learning. This class of methods is not particularly relevant to our work, since our goal is concept drift detection and diagnosis, and not model adaptation. In fact, we view our approach as something that can be used in conjunction with model adaptation methods to make them more efficient and interpretable. In particular, the model adaptation could be turned on only when the concept drift detection component has indicated that $P(Y|\bm{X};\bm{\theta})$ has changed. Otherwise, unnecessarily adapting the model when $P(Y|\bm{X}; \bm{\theta})$ is stable is counterproductive, in terms of both predictive performance and computational expense. Or, in some applications in which the model adaptation component needs to be constantly run, the concept drift detection component can be run concurrently with little extra cost to provide diagnostic information.

The concept drift detection methods are more relevant to our work, and examples in this category include DDM~(\cite{gama2004learning}), EDDM~(\cite{baena2006early}), ECDD~(\cite{ross2012exponentially}), and Linear-4-rate~(\cite{wang2015concept}), etc. Although these methods can also be applied in a continuous model adaptation setting, we focus on their use and performance in our setting, in which one has a single model fitted to one set of training data. The Drift Detection Method (DDM) monitors the accumulated classification error rate as it evolves over time. The Early Drift Detection Method (EDDM) instead monitors the intervals between consecutive errors. The EWMA for Concept Drift Detection (ECDD) method monitors the misclassification rate using a univariate EWMA control chart. In Linear-4-rate, four statistics are monitored simultaneously to detect concept drift in binary classification problems. When the particular statistic monitored in each method crosses a specified threshold, an alarm is triggered indicating that concept drift has been detected. Following an alarm, subsequent action such as updating the model or inspecting the data sources can be taken. Most of the methods are designed specifically for binary classification and based on simple error-based metrics, like classification error, precision, recall, and residuals of models~(\cite{wang2015concept,barros2018large}), which are limited in capability compared to our score-based concept drift detection approach.

Although the concept drift community seems to be unaware of the potential of the score-based methods for concept drift (e.g., it is not mentioned in the recent surveys in~\cite{barros2018large} and~\cite{lu2018learning}), there has been some work in the econometrics literature that has used the score function to test for changes in the parameters of regression models~(\cite{kuan1995generalized,zeileis2005unified,zeileis2007generalized,xia2009monitoring}). Our work differs from this prior work in that we develop a comprehensive framework for and investigate issues more relevant in the typical situations to which the so-called concept drift paradigm refers. The econometrics work is developed mainly around the change-point paradigm in which it is assumed there is a single point in time at which $\bm{\theta}$ changes from some before value to some after value, and the goal is to identify the change point with formal hypotheses testing. In contrast, our approach is developed around the much more general and flexible situation in which $\bm{\theta}$ can continuously drift and/or change abruptly at multiple change-points, which is far more common in typical concept drift applications. Ours is more of an exploratory approach to inform the predictive modeling process, as opposed to formal hypotheses testing of well-defined but restrictive hypotheses. We provide strong theoretical and numerical justification for the particular score-based approach that we develop for the general drifting-$\bm{\theta}$ situation. Our approach is also more general in the sense that it applies to any parametric supervised learning model in which $\log{P(Y|\bm{X};\bm{\theta})}$ is differentiable in $\bm{\theta}$ (e.g., any generalized linear model, neural networks, Gaussian process models, etc.). Moreover, we develop and study a number of other aspects that are highly relevant to the concept drift setting, including the computational connection to SGD, a diagnostic framework for understanding the nature of the concept drift that is consistent with the monitoring framework, and use of the framework for both retrospective and prospective analyses of the stability of the predictive relationship. Finally, unlike any formal hypotheses testing approach, our approach is meaningful even in situations in which the parametric supervised learning model is substantially wrong (i.e., its structure differs substantially from the true $P(Y|\bm{X})$, which may be nonparametric), which we discuss in Section~\ref{ss:sgd_score}.

\section{Monitoring the Score Function for Concept Drift}
\label{s:theory_analysis_score}
To monitor supervised learning models for concept drifts, we propose a systematic framework based on the sample score vectors derived from parametric models. In this section, we present theoretical arguments for using the score function as the basis for concept drift monitoring (Section~\ref{ss:score_func}); empirical counterparts to the theoretical arguments, including interpretations when the parametric model structure is only an approximation to the true $P(Y|\bm{X})$ and connections to SGD (Section~\ref{ss:sgd_score}); the MEWMA procedure for monitoring the mean of the score function (Section~\ref{ss:MEWMA}); and various implementation issues and how to handle high-dimensional $\bm{\theta}$ and regularized models (Section~\ref{ss:high_dim_score}).

\subsection{The Score Function as a Basis for Concept Drift Monitoring}
\label{ss:score_func}
Supervised learning is used to approximate an underlying conditional response distribution, which, if parametric, can be denoted as $P(Y|\bm{X};\bm{\theta})$. For example, for a regression neural network, the conditional mean $E[Y|\bm{X};\bm{\theta}]$ is modeled as a neural network, and $Y$ is assumed to be its conditional mean plus (typically) a Gaussian error; or for a classification neural network, $Y$ is multinomial with class probabilities that are modeled as a neural network. Fitting the model then entails estimating the parameters $\bm{\theta}$ by maximizing the log-likelihood, which can be viewed as an empirical approximation to $E_{\bm{\theta}^{(0)}}[\log{P(Y|\bm{X};\bm{\theta})}]$, where $\bm{\theta}^{(0)}$ denotes the true parameters. This is generally valid because of Shannon's Lemma~(\cite{shannon1948mathematical}), which states that if the model is correct and identifiable, the true parameter vector $\bm{\theta}^{(0)}$ \textit{uniquely maximizes} the expected log-likelihood, $E_{\bm{\theta}^{(0)}}[\log{P(Y|\bm{X};\bm{\theta})}]$. 

Given a parametric distribution or {marginal} likelihood function {$P(Y | \bm {X}; \bm{\theta})$} for an individual observation $(\bm{X}, Y)$, and assuming we have an i.i.d. training data set $\{(\bm {x}_i, y_i)\}_{i=1}^n$ drawn from this distribution, the score function for $ (\bm {x}_i, y_i)$ is defined as 
\begin{align}
\bm{s}(\bm { \theta}; (\bm {x}_i, y_i)) = \nabla _{\bm { \theta}} { \log{P(y_i | \bm {x}_i; \bm{\theta})}}
\label{eqn:score_func}
\end{align}
where $\nabla _{\bm { \theta}}$ is the derivative operator with respect to the vector of parameters, $\bm {\theta}$. From fundamental theory (Proposition 3.4.4 from~\cite{bickel2015mathematical}), under certain regularity conditions, if the model is correct and we have a true parameter vector {$\bm { \theta} ^{ (0)}$}, the expected score function evaluated at $\bm { \theta} ^{ (0)}$ is zero
\begin{align}
E_{\bm { \theta} ^{ (0)}}[\bm{s}(\bm { \theta}^{ (0)};(\bm {X}, Y))|\bm {X}] = \int\bm{s}(\bm { \theta}^{ (0)};(\bm {X}, Y=y))P(Y=y|\bm{X};\bm{\theta}^{(0)})dy = \bm{0}
\label{eqn:score_exp_zero}
\end{align}
where the subscript $\bm { \theta} ^{ (0)}$ on the expectation operator indicates that it is with respect to $Y|\bm{X}$ following the distribution $P(Y|\bm{X};\bm{\theta}^{(0)})$. In other words, the conditional expectation of the score function evaluated at $\bm { \theta} ^{ (0)}$ is identically zero.

Concept drift is defined as a change in $ P (Y|\bm {X})$, and in our parametric model setting it equates to a change in the parameters of the conditional distribution $P(Y|\bm {X}; \bm { \theta})$. This suggests that a general approach for monitoring for concept drift is to monitor for changes in the mean of the sample score vector in Equation~(\ref{eqn:score_func}). In particular, with no concept drift, we have $\bm{\theta}=\bm{\theta}^{(0)}$, so that the sample score vector $\bm{s}(\bm { \theta}^{ (0)};(\bm {X}, Y))$ is zero-mean from the above discussion. In contrast, if there is concept drift, this means $\bm{\theta}$ has changed from $\bm{\theta}^{(0)}$ to some other value $\bm{\theta}^{(1)} \neq \bm{\theta}^{(0)}$, in which case the new mean of the score vector $E_{\bm { \theta} ^{ (1)}}[\bm{s}(\bm { \theta}^{ (0)};(\bm {X}, Y))|\bm {X}] \neq \bm{0}$ is no longer zero mean (for small changes in $\bm{\theta}$ and under certain identifiability assumptions). As a preview to how the above concepts will be applied in practice (see Section~\ref{ss:sgd_score} for details), a set of estimated parameters from a training data set will take the place of $\bm { \theta}^{ (0)}$, and to whatever the current values of $\bm{\theta}$ have drifted will take the place of $\bm{\theta}^{(1)}$.

To illustrate the above arguments more concretely, consider the generalized linear model (GLM)~(\cite{nelder1972generalized}), which encompasses many models commonly used in applications. The canonical form of the GLM marginal likelihood and score function is
\begin{align}
\begin{aligned}
P(Y|\bm{X};\bm { \theta}, \phi) =& \exp\{\frac{Y \psi(\bm{X};\bm { \theta})-b( \psi(\bm{X};\bm { \theta}))}{ a ( \phi)} + c(Y; \phi)\} \\
\bm {s}(\bm { \theta};(\bm {X}, Y)) =& \frac{1}{a( \phi)}(Y - b'( \psi (\bm{X};\bm { \theta} )))\nabla _{ \bm { \theta}} \psi(\bm{X};\bm { \theta} )
\end{aligned}
\label{eqn:score_glm}
\end{align}
for functions $b(\cdot)$, $ \psi(\cdot)$, and $a(\cdot)$ and dispersion parameter $\phi$. The conditional mean is $E[Y|\bm{X}]=b'(\psi(\bm{X};\bm{\theta}))$, where the derivative is with respect to $\psi$, and the canonical link function $g(\cdot)$ is defined such that $g(E[Y|\bm{X}])=\psi(\bm{X};\bm{\theta})$($=\bm{X}^T\bm{\theta}$ for the traditional GLM). Using a Taylor expansion, the expected score function with a small parameter change $ \Delta \bm { \theta}= \bm { \theta}^{(1)}-\bm { \theta}^{(0)}$ is
\begin{align}
\begin{aligned}
E _{\bm { \theta} ^{(1)}}[\bm {s}(\bm { \theta} ^{(0)};(\bm {X}, Y))|\bm{X}] \approx& \frac{1}{a ( \phi)}b''( \psi(\bm{X}; \bm { \theta} ^{(0)}))\nabla _{ \bm { \theta}} \psi (\bm{X}; \bm { \theta} ^{ (0)}) \nabla _{\bm { \theta}}^T \psi (\bm{X}; \bm { \theta} ^{ (0)}) \Delta \bm { \theta} \\
=& \frac{1}{a ( \phi)}b''( \psi(\bm{X}; \bm { \theta} ^{(0)}))\bm {X}\bm {X}^T \Delta \bm { \theta}
\end{aligned}
\label{eqn:exp_score_glm}
\end{align}
It is known that $Var[Y|\bm{X}] = a ( \phi)b''(\psi (\bm{X}; \bm { \theta} ))$ so that $b''(\psi (\bm{X}; \bm { \theta} ))>0$ always holds, if $Y$ is not a deterministic function of $\bm{X}$. If we further take the expectation of Equation~(\ref{eqn:exp_score_glm}) with respect to the distribution of $\bm{X}$, then the resulting matrix that premultiplies $\Delta\bm{\theta}$ is always positive definite if the distribution of $\bm{X}$ is not degenerate. Notice that this conclusion does not require $ \psi (\bm{X}; \bm { \theta})$ to be a linear function of $\bm { \theta}$, if certain identifiability conditions on $\bm{\theta}$ are satisfied and the Hessian matrix of the $\psi(\bm{X};\bm{\theta})$ with respect to the parameter vector has eigenvalues with small absolute values (e.g. the Hessian matrix of linear $\psi(\bm{X};\bm{\theta})$ has eigenvalues all as zero). For example, $ \psi (\bm{X}; \bm { \theta})$ can be a neural network, in which case $\bm { \theta}$ is the vector of weights and bias parameters for the neural net. In later studies with simulated and real data sets in Section~\ref{s:real_data}, we show that our score-based method is also effective for neural networks.

In summary, for GLM-type response models with linear or nonlinear $\psi(\cdot)$ that satisfy certain identifiability conditions on $\bm{\theta}$, \textit{the mean of the score function changes if and only if $P(Y|\bm{X};\bm{\theta})$ changes}. This is an important property that provides the theoretical basis for our score-based concept drift monitoring and diagnosis. Note that error-based methods do not enjoy this property, i.e., $P(Y|\bm{X};\bm{\theta})$ can change in ways that do not change the error rate.

\begin{figure}[!htbp]
\centering
 \begin{subfigure}[t]{0.4\linewidth}
         \centering
         \includegraphics[width=\textwidth, trim=.2in .2in .7in .45in, clip]{../figures/v14/demons_fig/2D_logi_orig.png}
         \captionsetup{width=.95\linewidth}
         \caption{The original model $P(Y|{X};\bm{\theta}^{(0)})$ with the decision boundary that minimizes expected error rate in Equation~(\ref{eqn:logi_err_rate}).}
         \label{fig:logi_err_rate_unch_a}
  \end{subfigure}
  \begin{subfigure}[t]{0.4\linewidth}
         \centering
         \includegraphics[width=\textwidth, trim=.2in .2in .7in .45in, clip]{../figures/v14/demons_fig/2D_logi_cd.png}
         \captionsetup{width=.95\linewidth}
         \caption{The drifted model $P(Y|{X};\bm{\theta}^{(1)})$ but with unchanged error rate from the left figure (a).}
         \label{fig:logi_err_rate_unch_b}
  \end{subfigure}
%  \begin{subfigure}[t]{0.4\linewidth}
%          \centering
% 	 \includegraphics[width = \textwidth, trim=.2in .2in .7in .45in, clip]{../figures/v14/demons_fig/2D_logi.png}
% 	    \captionsetup{width=.95\linewidth}
%          \caption{The shaded areas from Figure~\ref{fig:logi_err_rate_unch_a} and~\ref{fig:logi_err_rate_unch_b} are overlapped and it shows unchanged error rate.}
%          \label{fig:logi_err_rate_unch_c}
%   \end{subfigure}
  \begin{subfigure}[t]{0.4\linewidth}
         \centering
	 \includegraphics[width=\textwidth, trim=.2in .2in .7in .45in, clip]{../figures/v14/demons_fig/2D_logi_cd_updated.png}
         \captionsetup{width=.95\linewidth}
         \caption{The drifted model $P(Y|{X};\bm{\theta}^{(1)})$ after detecting concept drift and updating the decision boundary to reduce error rate.}
         \label{fig:logi_err_rate_unch_d}
  \end{subfigure}
  \caption{The demonstration of simple logistic regression that concept drift would result in no change in the error rate. The shaded regions are related to the error in Equation~(\ref{eqn:logi_err_rate}).}
  \label{fig:logi_err_rate_unch}
\end{figure}

To further illustrate the above with a more concrete example, consider the following binary simple logistic regression example in which concept drift occurs but with no change in the error rate. The formulas defining the model and the corresponding score function in this case are
\begin{align}
\begin{aligned}
&P(Y=1|{X};\bm{\theta}) = \frac{\exp\{\bm {X}^{T} \bm {\theta}\}}{1+\exp\{\bm {X}^{T} \bm {\theta}\}} \\
&\bm {s}(\bm { \theta} ; ( {X}, Y)) = (Y-P(Y=1|{X};\bm{\theta}))\bm {X}
\end{aligned}
\label{eqn:logi_mod_score}
\end{align}
where $\bm{X} = [1, X]^T$ and $\bm{\theta}=[\theta_0, \theta_1]^T$. It is straightforward to verify that the (conditional) expectation of the score function is $\bm {0}$, when $\bm{\theta}$ does not change.

Further suppose the $X$ follows a standard normal distribution with density function denoted by $q(x)$ and the conditional distribution $P(Y|X;\bm{\theta})$ has concept drift with $\bm{\theta}$ changing from $\bm{\theta}^{(0)}$ to $\bm{\theta}^{(1)}$. Define 
\begin{align}
p^{(j)}(X)=P(Y=1|X;\bm{\theta}^{(j)}),~j\in\{0,1\},
\label{eqn:simp_nota_p}
\end{align}
where $j=\{0,1\}$ indicates whether the parameters are before or after the change. Figures~\ref{fig:logi_err_rate_unch_a} and~\ref{fig:logi_err_rate_unch_b} plot $p^{(0)}(x)$ and $p^{(1)}(x)$, respectively. Suppose we use the classification rule $\hat{y}(x)=1$ if $p^{(0)}(x)\geq 0.5$ (i.e., if $x\geq 0 $) and $\hat{y}(x)=0$ otherwise. Then the conditional and unconditional error rates are
% \begin{align}
% \begin{aligned}
% err(x;\bm{\theta}^{(j)})\vcentcolon=&P(Y\neq\hat{y}(x)|\bm{X}=[1,x]^T;\bm{\theta}^{(j)}) \\
% =& p^{(j)}(x)(1-I(x\geq x_{opt})) + (1-p^{(j)}(x))I(x\geq x_{opt}) \\
% err(\bm{\theta}^{(j)})\vcentcolon=&E[err(X;\bm{\theta}^{(j)})] = \int_{-b}^{b}err(x;\bm{\theta}^{(j)})q(x)dx
% \end{aligned}
% \label{eqn:logi_err_rate}
% \end{align}
\begin{align}
\begin{aligned}
&P(Y\neq\hat{y}(X)|X;\bm{\theta}^{(j)})
= p^{(j)}(X)(1-\hat{y}(X)) + (1-p^{(j)}(X))\hat{y}(X) \\
&P(Y\neq\hat{y}(X);\bm{\theta}^{(j)}) = E[P(Y\neq\hat{y}(X)|X;\bm{\theta}^{(j)})] \\ &= \int_{-b}^{b}P(Y\neq\hat{y}(x)|X;\bm{\theta}^{(j)})q(x)dx
\end{aligned}
\label{eqn:logi_err_rate}
\end{align}
Note that this classifier $\hat{y}(x)$ minimizes the unconditional error rate. As a numerical example, suppose $\bm{\theta} = \bm{\theta}^{(0)}=[0, 2]^T$ so that the true predictive relationship $P(Y=1|{X})$ is as in Figure~\ref{fig:logi_err_rate_unch_a}, the error rate is $22.2\%$. Now suppose we use the same classifier based on $\bm{\theta}^{(0)}$, but $\bm{\theta}$ changes to $\bm{\theta}^{(1)}=[0.59, 4]^T$ so the true predictive relationship $P(Y=1|{X})$ is as in Figure~\ref{fig:logi_err_rate_unch_b}. Even though $\bm{\theta}$ has changed, the error rate remains unchanged at $22.2\%$. Clearly, any concept drift detection method based on the error rate will fail to detect the change in $\bm{\theta}$. It is important to note that although the error rate does not change when $\bm{\theta}$ changes from $\bm{\theta}^{(0)}$ to $\bm{\theta}^{(1)}$, the change in $\bm{\theta}$ constitutes an opportunity to improve the classification accuracy if we were to detect the change and update the classification rule based on the new model, i.e., use the new classifier $\hat{y}(x)=1$ if $x \geq 0.59$, which corresponds to $\hat{y}(x)=1$ if $p^{(1)}(x)\geq 0.5$ (see Figure~\ref{fig:logi_err_rate_unch_d}).

In contrast to any error rate monitoring approach, as we have shown earlier, the mean of the score function always changes when $P(Y|\bm{X};\bm{\theta})$ changes, under fairly general conditions. In Section~\ref{ss:cd_no_err_change}, we present a numerical example to further support the analysis here.

% Furthermore, as we will show in Section~\ref{ss:MEWMA}, the mean of our score-based MEWMA monitoring statistic also changes, when $P(Y|\bm{X};\bm{\theta})$ changes.

% Error rate of the logistic regression model given predictor $\bm {X}$ can be written as a function (we call it ``penalty function" to help explanation):
% \begin{align}
% C _{err}^{(i)}(\bm {X}) = p ^{(i)}I(\hat{y}=0)+(1-p ^{(i)})I(\hat{y}=1)
% \label{eqn:penal_err}
% \end{align}
% where $p ^{(i)} = P(Y=1|\bm {X}; \bm { \theta} ^{(i)})$, ($i=0,1$), and the superscript, $i$, indicates that this probability function is from original distribution under which the model is trained ($i=0$) or the distribution of new samples during prediction (after a decision model being trained, $i=1$). Thus, if concept drift happens after training, $p ^{(1)} \neq p ^{(0)}$; otherwise, they are equal. Notice that the probability function $p ^{(1)}$ and indentity function $I$ depend on predictor vector $\bm {X}$, but omitted for clean notation. Given the distribution of covariates $\bm {X}$ and decision rule (model) $\hat{y}$, the expectation of this penalty function are those shaded areas in the Figure~\ref{fig:logi_err_rate_unch_a} and \ref{fig:logi_err_rate_unch_b} for the original (blue) and the drifted (yellow) data generating process. In the Figure~\ref{fig:logi_err_rate_unch_c}, Figure~\ref{fig:logi_err_rate_unch_a} and~\ref{fig:logi_err_rate_unch_b} are overlapped together. The drifted data generating process decreases the error by those blue shaded area but adds the red shaded area as new error. Because the two areas are equal, the expected error rate remains the same. If we only monitor the error rate or any metrics derived from it, the concept drift would be missed. More important, this concept drift changes the optimal decision boundary, so that retraining the model can potentially obtain higher accuracy. 



% These plots can be generalized into other penalty functions for metrics like Hotelling $T^2$ of EWMA of the score function as mentioned in the Section~\ref{ss:MEWMA}. For more intuitive comparison, penalties are put close to horizontal line $y=0$, so that the expectation of monitored penalty function equals the area under the curve in Figure~\ref{fig:logi_med_penal}.
 
% The penalty function for score function after simplification is:
% \begin{align}
% C _{score}^{(i)}(\bm {X}) = (p ^{(i)} (1 - p ^{(0)}) + p ^{(0)}(p ^{(0)}-p ^{(i)})) \bm {X}^T\bm { \Sigma}^{-1}\bm {X}
% \label{eqn:penal_score}
% \end{align}
% where $\bm { \Sigma} = E _{\bm {X}}[p ^{(0)}(1-p ^{(0)})\bm {X}\bm {X}^T]$ is the covariance matrix of the score function of the logistic model under training distribution, and the subscript of the expectation means it is over the distribution of covariates $\bm {X}$. As we can see in Figure~\ref{fig:logi_med_penal}, after concept drift, error has the decreased part (blue shaded area) and increased part (red shaded area), which are approximately equal. However, the Hotelling $T^2$ of EWMA of the score function has the increased part larger than the decreased part resulting in net positive change in the penalty function of score function, which indicates that it is more sensitive for monitoring concept drift. The reason is that score function are applied EWMA first and then Hotelling $T^2$. Reversing the order of applying EWMA and Hotelling $T^2$ would void this property, because random noises cannot be averaged out. According to the penalty function~(\ref{eqn:penal_score}) and~(\ref{eqn:penal_err}) and after some derivation, we can see that it makes sense that if two probability functions, $p ^{(0)}$ and $p ^{(1)}$ are different, we have $\int_{\bm{x}}(C _{score}^{(1)}(\bm {x})-C _{score}^{(0)}(\bm {x}))p(\bm{x})d\bm{x}>0$ but the sign of $\int_{\bm{x}}(C _{err}^{(1)}(\bm {x})-C _{err}^{(0)}(\bm {x}))p(\bm{x})d\bm{x}$ is uncertain, which means score-based method directly monitors the deviation of $p ^{(1)}$ from $p ^{(0)}$ while error-based method is not. In other words, score-based method monitors exactly the concept drift. Here the simple logistic regression gives an intuition why score function performs better in monitoring concept drift of parametric models. Of course, in detecting the change of mean, noise level would also affect the sensitivity.

\subsection{Interpretations with Empirical Data and Incorrect Models}
\label{ss:sgd_score}
The zero-mean property $E_{\bm{\theta}^{(0)}}[\log{P(Y|\bm{X}; \bm{\theta}^{(0)})}] = \textbf{0}$ of the score function and the uniqueness of the parameters $\bm{\theta}$ that maximize the expected log-likelihood $E_{\bm{\theta}^{(0)}}[\log P(Y|\bm{X}; \bm{\theta})]$ are guaranteed to hold only when the model is correct; that is, when the supervised learning model $P(Y|\bm{X};\bm{\theta})$ is of the same structure as the true predictive relationship $P(Y|\bm{X})$. Recalling the adage that ``All models are wrong, but some are useful"~(\cite{box1976science})), one might wonder to what extent the results in the previous section are applicable when the structure of the model $P(Y|\bm{X};\bm{\theta})$ differs from the true $P(Y|\bm{X})$. A related question is what should we take to be the empirical counterparts to $E_{\bm{\theta}}[\bm{s}(\bm{\theta}^{(0)};(\bm{X},Y) )]$, when $\bm{\theta}^{(0)}$ is replaced by its estimate from a sample of training data, and the expectation is replaced by a sample average over a set of new data or over the same training data. We address both of these issues in this section and also relate the empirical counterpart to SGD for computational reasons. 

Regardless of whether the model structure is correct, in analogy with Equation~(\ref{eqn:score_exp_zero}), we always have
\begin{align}
\begin{aligned}
&\hat{E}_{(0)}[\bm{s}(\hat{\bm{\theta}}^{(0)};(\bm{X}, Y))] \vcentcolon=\frac{1}{n}\sum_{i=1}^{n}\bm{s}(\hat{\bm{\theta}}^{(0)};(\bm{x}_i, y_i))=\bm{0}, \text{where} \\
&\hat{\bm{\theta}}^{(0)} \vcentcolon =  \argmax_{\bm{\theta}}\hat{E}_{(0)}[\log{P(Y|\bm{X}; \bm{\theta})}] \vcentcolon= \argmax_{\bm{\theta}}\frac{1}{n}\sum_{i=1}^n \log{P(y_i|\bm{x}_i;\bm{\theta})},
\end{aligned}
\label{eqn:score_exp_zero_emp}
\end{align}   
the operator $\hat{E}_{(0)}$ denotes a sample average over the training data $\{(\bm{x}_i, y_i)\}_{i=1}^n$, and $\hat{\bm{\theta}}^{(0)}$ is maximum-likelihood estimator (MLE) of $\bm{\theta}^{(0)}$ for the training data. That is, when we fit a model using MLE, the gradient of the training log-likelihood is identically zero, i.e., $\nabla_{\bm{\theta}}\hat{E}_{(0)}[\log{P(Y|\bm{X}; {\bm{\theta}})}]|_{\bm{\theta}=\hat{\bm{\theta}}^{(0)}} \vcentcolon=\nabla_{\bm{\theta}}\frac{1}{n}\sum_{i=1}^n \log{P(y_i|\bm{x}_i;{\bm{\theta}})|_{\bm{\theta}=\hat{\bm{\theta}}^{(0)}}}=\frac{1}{n}\sum_{i=1}^n\bm{s}(\hat{\bm{\theta}}^{(0)};(\bm{x}_i, y_i)) = \hat{E}_{(0)}[\bm{s}(\hat{\bm{\theta}}^{(0)};(\bm{X}, Y))]=\bm{0}$, even if the model is not the correct structure. Thus, (\ref{eqn:score_exp_zero_emp}) is the empirical counterpart of (\ref{eqn:score_exp_zero}) with the estimated $\hat{\bm{\theta}}^{(0)}$ taking the place of the true $\bm{\theta}^{(0)}$. 

Now suppose the true predictive relationship $\tilde{P}(\tilde{Y}|\tilde{\bm{X}})$ changes from $P(Y|\bm{X})$ over some new set of data $\{(\tilde{\bm{x}}_i, \tilde{y}_i)\}_{i=1}^{\tilde{n}}$. In this case, if the operator $\hat{E}_{(1)}$ denotes a sample average over the new data, a different set of parameters $\hat{\bm{\theta}}^{(1)} \neq \hat{\bm{\theta}}^{(0)}$ for the same supervised learning model structure $P(Y|\bm{X};\bm{\theta})$ will now provide a better fit to the new data than did $\hat{\bm{\theta}}^{(0)}$, where 
\begin{align}
\begin{aligned}
\hat{\bm{\theta}}^{(1)} \vcentcolon= \argmax_{\bm{\theta}}\hat{E}_{(1)}[\log{P(\tilde{Y}|\tilde{\bm{X}}; \bm{\theta})}] \vcentcolon= \argmax_{\bm{\theta}}\frac{1}{\tilde{n}}\sum_{i=1}^{\tilde{n}} \log{P(\tilde{y}_i|\tilde{\bm{x}}_i;\bm{\theta})}
\end{aligned}
\label{eqn:score_exp_nonzero_emp}
\end{align}   
Thus, the gradient $\nabla_{\bm{\theta}}\hat{E}_{(1)}[\log{P(\tilde{Y} | \tilde{\bm{X}}; {\bm{\theta}})}]|_{\bm{\theta}=\hat{\bm{\theta}}^{(0)}} \vcentcolon= \nabla_{\bm{\theta}}\frac{1}{\tilde{n}}\sum_{i=1}^{\tilde{n}} \log{P(\tilde{y}_i | \tilde{\bm{x}}_i; {\bm{\theta}})}|_{\bm{\theta}=\hat{\bm{\theta}}^{(0)}} = \\ \frac{1}{\tilde{n}} \sum_{i=1}^{\tilde{n}} \bm{s}(\hat{\bm{\theta}}^{(0)};(\tilde{\bm{x}}_i, \tilde{y}_i)) = \hat{E}_{(1)}[\bm{s}(\hat{\bm{\theta}}^{(0)};(\tilde{\bm{X}}, \tilde{Y}))]$ of the log-likelihood for the new data (but with the gradient evaluated at the original estimate $\hat{\bm{\theta}}^{(0)}$) will generally differ from zero. The more $\tilde{P}(\tilde{Y}|\tilde{\bm{X}})$ changes from $P(Y|\bm{X})$, the more we expect $\hat{\bm{\theta}}^{(1)}$ to differ from $\hat{\bm{\theta}}^{(0)}$, and the more we expect the new average score vector $\hat{E}_{(1)}[\bm{s}(\hat{\bm{\theta}}^{(0)};(\tilde{\bm{X}}, \tilde{Y}))]$ to differ from $\bm{0}$. This provides the justification for our score-based concept drift monitoring approach, which tracks the mean of the score vector $\bm{s}(\hat{\bm{\theta}}^{(0)};(\bm{X}_i, Y_i)) = \nabla_{\bm{\theta}}\log P(Y_i|\bm{X}_i; \bm{\theta})|_{\bm{\theta}=\hat{\bm{\theta}}^{(0)}}$ to detect and analyze changes in it. 

If the supervised learning model $P(Y|\bm{X};\bm{\theta})$ is of the same structure as the true predictive relationship $P(Y|\bm{X})$, if both $P(Y|\bm{X})$ and $P(\bm{X})$ are constant across the training data $\{(\bm{x}_i, y_i)\}_{i=1}^n$, and if $n \to \infty$, then under some regularity conditions the MLE $\hat{\bm { \theta}} ^{ (0)}$ is consistent and  $\hat{E}_{(0)} [\bm{s}(\hat{\bm { \theta}} ^{ (0)};(\bm {X}, Y))] \to E_{\bm { \theta} ^{ (0)}}[\bm{s}(\hat{\bm { \theta}} ^{ (0)};(\bm {X}, Y))] \to E_{\bm { \theta} ^{ (0)}}[\bm{s}(\bm { \theta} ^{ (0)};(\bm {X}, Y))] = \bm {0}$. In this case, there is no distinction between the theoretical version of the score-based monitoring arguments and their empirical version discussed above. With large $n$, SGD is often used to fit models to the training data, which involves approximating the gradient of the log-likelihood function using individual training observations or mini-batches of training observations at each iteration of the optimization algorithm. The SGD estimator of $\bm { \theta} ^{ (0)}$ converges to the batch MLE under certain conditions involving step size and other considerations (see, e.g., Theorem 4.7 of \cite{bottou2018optimization}). In this case, under the same asymptotic conditions stated above, the SGD estimator $\hat {\bm { \theta}}_{SGD}$ is also consistent and $\hat{E}_{(0)} [\bm{s}(\hat{\bm { \theta}}_{SGD};(\bm {X}, Y))] \to \bm{0}$ as $n \to \infty$. 

The score-based method does not add much extra cost to the current framework of training and using models. For retrospective analysis, the sample score vectors are a byproduct of the SGD, since the mini-batch gradients are of the form ($\nabla _{\bm { \theta}} \sum _{i=1} ^{m} \ln P(y_i|\bm {x}_i;\bm{\theta}) = \sum _{i=1} ^{m} \bm{s}(\bm { \theta};(\bm {x}_i, y_i))$, where $m$ is the batch size). For prospective analysis, prediction of new data usually partially calculates the score vectors. For example, prediction for neural networks requires forward-propagation, and another backward-propagation in memory would generate the score function. Thus, we do not need much extra computation to apply the score-based method.

With finite training data size $n$ and finite new sample sizes $\tilde{n}$ for monitoring (or even $\tilde{n}=1$), noise in $\bm{s}(\bm { \theta};(\bm {x}_i, y_i))$ and its sample averages must be considered. In particular, we need to distinguish by how much $\bm{s}(\hat{\bm { \theta}}^{(0)};(\bm {x}_i, y_i))$ (or its sample average over some moving time window) should differ from $\bm{0}$ before we conclude that $P(Y|\bm{X})$ has changed. The MEWMA monitoring strategy in the next section is designed to distinguish noise from legitimate changes in $P(Y|\bm{X})$. Moreover, for models fitted with regularization, the gradient of the log-likelihood itself is not zero over the training data, because the regularization penalty is included in the optimization objective function. Regardless, the score-based monitoring method can still be applied with the minor modification to the score vectors discussed in Section~\ref{ss:high_dim_score}.

\subsection{An MEWMA Approach for Monitoring the Score Function}
\label{ss:MEWMA}
As discussed in the previous sections, monitoring for concept drift reduces to monitoring for changes over time in the mean of the score function. Among other challenges, this requires distinguishing between noise in the score functions for individual observations vs. an actual mean change. Monitoring for changes in the mean of random vectors (e.g., a set of multivariate quality-related measures) while distinguishing from noise is an old and well-researched problem in the SPC literature. The MEWMA has emerged as one of the most effective techniques for this, and in this section we develop it for monitoring the score function mean vector.

The MEWMA at time $t$, denoted by $\bm{z}_t$, is defined recursively (for $t=1,2,\cdots$) via:
\begin{align}
\bm {z}_t = \lambda \bm {s}_t + (1 - \lambda) \bm {z} _{t-1},
\label{eqn:ewma}
\end{align}
where $\bm {s}_t$ is the to-be-monitored random vector at time $t$, which in our case is the score vector $\bm {s}_t \vcentcolon= \bm{s}(\hat{\bm { \theta}}^{(0)};(\bm {x}_t, y_t))$; $ \lambda$ is a weighting parameter; and $\bm{z}_0$ can be initialized as the sample mean of $\bm {s}_t$ over some small initial batch of observations. An equivalent expression for the recursive relationship~(\ref{eqn:ewma}) is $\bm {z}_t = \lambda\sum _{\tau=1}^t (1-\lambda) ^{t-\tau} \bm{s} _{\tau} + (1-\lambda)^t \bm{z}_0$, which gives exponentially decaying weights to the past $\bm{s}_t$ observations. Smaller $\lambda$ in the MEWMA formula corresponds to more slowly decaying weights and thus longer effective windows over which the exponentially-weighted averages are computed. The effective window length is sometimes viewed as $\sum _{j=0}^\infty (1-\lambda)^j = \frac{1}{\lambda}$. The choice of $\lambda$ should depend on the application of interest, with the following trade-off. Smaller $\lambda$ translates to a larger effective window length, which gives a lower-variance estimate of the mean of $\bm{s}_t$ by smoothing out more noise (which generally results in better detection of small changes), but it also makes the MEWMA more sluggish (which results in longer delays in detecting large changes). If there is no need to detect changes in $P(Y|\bm{X})$ in fewer than some number (say $D$) of observations, then there is no need to have the effective window length $\frac{1}{\lambda}$ smaller than $D$, in which case one should  select $\lambda \leq \frac{1}{D}$.

Since $\bm{s}_t$ and $\bm{z}_t$ are vectors, and we desire to detect changes in the mean of $\bm{s}_t$ in any direction, our MEWMA approach monitors the Hotelling $T^2$ statistic 
\begin{align}
T_t^2 = (\bm {z}_t-\bar { \bm {s}})^T \hat {\bm { \Sigma}} ^{-1}(\bm {z}_t-\bar { \bm {s}})
\label{eqn:hotellingt2}
\end{align}
where $\bar {\bm{s}}$ and $\hat {\bm {\Sigma}}$ are the sample mean vector and covariance matrix of $\bm {s}_t$, respectively, estimated from some set of training data. If $T_t^2$ at some $t$ exceeds a specified upper control limit (UCL) determined as described later, this is taken to be an indication that $P(Y|\bm{X})$ in the time-vicinity of observation $t$ has changed from what it was when $\hat{\theta}^{(0)}$ was estimated.

% After obtaining the initial data set, we can use MEWMA to minimize concept drift in training data, as much as possible through retrospective analysis. With a well-trained and validated predictive model, we execute two phases for concept drift detection. In Phase-I, we {monitor Hotelling $T^2$ of EWMA of the score function for a certain period of time and ensure that those newly incoming data are in-control and} then calculate and set {upper and lower} control limits {(UCL and LCL)} based on a targeted false alarm rate. Here, the in-control data presents random fluctuation in the Phase-I without obvious trend as shown in Figure~\ref{fig:Monitoring}, because sample score vectors would fluctuate around $\bm {0}$ when environment is in stationary and model training becomes stable (as discussed in Section~\ref{ss:sgd_score}). This step ensures that no concept drift happens in Phase-I and the obtained control limits are trustworthy. The false alarm rate is usually chosen small (i.e. $0.1\%$), so that in monitoring the likelihood of encountering false alarms is small. In Phase-II, we continue to monitor sample score vectors of incoming data, while using control limits calculated from Phase-I. If new {monitoring statistics} significantly falls outside of control limits for a long sequence or there are some obvious deviation pattern from the normal ($0$ in this case), the alarm of concept drift is set off.

% \begin{figure}[!htbp]
% \centering
%  \begin{subfigure}[t]{0.6\linewidth}
%          \centering
%          \includegraphics[width = 1\linewidth, trim=.35in .69in .35in .69in, clip]{../figures/v14/flow_chart/Retrospective_1.png}
%          \caption{Retrospective Analysis.}
%          \label{fig:retro_analysis}
%   \end{subfigure}
%   \begin{subfigure}[t]{0.6\linewidth}
%          \centering
%          \includegraphics[width = 1\linewidth, trim=.35in .49in .35in .49in, clip]{../figures/v14/flow_chart/Monitoring_1.png}
%          \caption{Monitoring.}
%          \label{fig:Monitoring}
%   \end{subfigure}
%   \begin{subfigure}[t]{0.6\linewidth}
%          \centering
%          \includegraphics[width = 1\linewidth, trim=.35in .44in .35in .44in, clip]{../figures/v14/flow_chart/Diagnose_1.png}
%          \caption{Diagnosis.}
%          \label{fig:diagnosis}
%   \end{subfigure}
%   \caption{{The framework of monitoring/detecting concept drift based on the score function. (a) Conducting retrospective analysis using MEWMA and/or score function clustering to ensure there is no significant concept drift in the data set used to train and set control limits. The size of data set can be recursively reduced if significant concept drift exists. (b) Monitoring concept drifts using the model and control limits obtained by processing training and Phase-I data sets, which will be demonstrated in Section~\ref{s:demon_cd} and~\ref{s:real_data}. In this subplot, three examples of possible results are given: gradual and abrupt concept drift and in-control cases. (c) Visualization of diagnosing concept drift: The MEWMA for score vectors and the EWMA for individual predictors are visualized to show the origin of concept drift, which will be illustrated in Section~\ref{s:demon_cd} and~\ref{s:real_data}.}}
%   \label{fig:proc_mon_score}
% \end{figure}


\begin{figure}[!htbp]
\centering
\includegraphics[width = 1\linewidth, trim=.35in .69in .35in .69in, clip]{../figures/v14/flow_chart/Retrospective_1.png}
\caption{Step $1$ of the score-based concept drift framework. A retrospective analysis to determine whether the training data to which the model is fit are stable (as in the top example) or concept drift occurred over the training data (as in the bottom plot).}
  \label{fig:proc_mon_score_retro}
\end{figure}

\begin{figure}
\centering
\includegraphics[width = 1\linewidth, trim=.35in .49in .35in .49in, clip]{../figures/v14/flow_chart/Monitoring_1.png}
\caption{Step $2$ of the score-based concept drift framework. A retrospective analysis first splits the stationary data from Step $1$ into a training set (to which the model is fit) and a Phase-I set (which is used to establish the control limit for Phase-II). Then a prospective Phase-II analysis is used to monitor new data for concept drift as each new $(\bm{X}, Y)$ observation is collected.}
\label{fig:proc_mon_score_monitoring}
\end{figure}

\begin{figure}
\centering
\includegraphics[width = 1\linewidth, trim=.35in .20in .35in .20in, clip]{../figures/v14/flow_chart/Diagnose_1.png}
\caption{Step $3$ of the score-based concept drift approach. A diagnostic analysis to help identify the nature of the concept drift once it is detected in either the retrospective Step $1$ stage or the Phase-II monitoring stage in Step $2$. The left and right plots depict two different types of concept drift -- an abrupt change(left plots) and a gradual change (right plots).}
\label{fig:proc_mon_score_diagnosis}
\end{figure}

The score-based concept drift monitoring framework is broken into three steps and depicted in Figures~\ref{fig:proc_mon_score_retro},~\ref{fig:proc_mon_score_monitoring}, and~\ref{fig:proc_mon_score_diagnosis} for Steps $1$, $2$, and $3$, respectively. The three steps will be illustrated in more detail in the later examples. In Step 1, we collect a batch of data in time order, $\{\bm {x}_i, y_i\} _{i=1} ^{m}$, where $m$ denotes the sample size of this batch. Then, after fitting a preliminary supervised learning model to these data, a retrospective analysis is conducted by applying the MEWMA to these same data. If the MEWMA indicates these data are stable and no significant concept drift is detected (as in the top plot in Figure~\ref{fig:proc_mon_score_retro}), we proceed to Step $2$. On the other hand, if the MEWMA detects significant concept drift in these data, then one can discard a portion of the data prior to the concept drift (e.g., the first third of the data in the bottom plot in Figure~\ref{fig:proc_mon_score_retro}) in attempt to ensure that the remaining data are stable. This retrospective analysis would then be repeated (perhaps multiple times) on the remaining data to verify whether it was stable vs. experienced concept drift. When the remaining data is deemed stable, then we proceed to Step $2$, which is depicted in Figure~\ref{fig:proc_mon_score_monitoring}.

Let $n\leq m$ denote the stable data from Step $1$ and denote these data by $\mathcal{D} \vcentcolon=\{\bm {x}_i, y_i\} _{i=1} ^{n}$. The purpose of Step $2$ is to establish the UCL and then to prospectively monitor new data  into two parts: $ \mathcal{D}_1 \vcentcolon= \{\bm {x}_i, y_i\} _{i=1} ^{n_1}$ and $\mathcal{D}_2 \vcentcolon= \{\bm {x}_i, y_i\} _{i=n_1+1} ^{n}$. The first data set, $\mathcal{D}_1$, is used to train a parametric model and the second one, $\mathcal{D}_2$, is used in Phase-I for monitoring statistics and calculating the control limit ($UCL$). Given the control limit from Phase-I, in Phase-II, the control chart is applied to sample score vectors or other metrics of testing data points $\tilde{\mathcal{D}}\vcentcolon=\{\tilde{\bm{x}}_i,\tilde{y}_i\}_{i = 1}^{\tilde{n}}$ to signal the starting position of a concept drift, if it should happen. The Phase-II analysis can be used in data exploration when all the data are available and the existence and starting position of concept drift is interested in; or in prediction when data points come one-by-one and the aim is to monitor concept drifts. Finally, in Step 3, we can diagnose the origin of concept drifts if detected as shown in Figure~\ref{fig:proc_mon_score_diagnosis}, which will be explained mathematically in Section~\ref{s:decou_cd}. In this step, the scores are transformed by Fisher information matrix to decouple different components of score vectors. Then, (univariate) EWMA control charts are used to find out which covariates contribute to the concept drift.
 
Notice that for other scalar metrics or transformed components of the score function (see Section~\ref{s:decou_cd}), we use univariate EWMA~(\cite{roberts1959control}) control chart because the statistics calculated by Equation~(\ref{eqn:ewma}) becomes scalar. The main difference is that in MEWMA control charts the monitored statistics is a summary statistics of vectors and is positive, but in EWMA control charts the statistics can fluctuate to both sides of zero, so that instead of only calculating $UCL$ we also need a lower control limit($LCL$) to detect concept drift. Other monitoring steps are similar.

In this procedure, the EWMA is obtained first, followed by Hotelling $T^2$ calculation. The advantage of this is that the EWMA would reduce the random noise in raw score vectors, so that in Phase-II of detection using Hotelling $T^2$, small drifts would be easier to be detected due to higher signal-to-noise ratio. Because we are interested in applications with a large size of data sets, the usage of empirical control limits of the MEWMA control chart based on the Phase-I data is reasonable.

% \begin{figure}[!htbp]
% \centering
% \begin{subfigure}[t]{0.49\linewidth}
%      \centering
%      \includegraphics[width=\textwidth, trim=.2in .2in .7in .45in, clip]{../figures/v14/demons_fig/2D_err_logi_trunc_norm.png}
%      \captionsetup{width=.95\linewidth}
%      \caption{The error rate function, Equation~(\ref{eqn:logi_err_rate}), before and after concept drift by monitoring error. The drifted data generating process decreases the unconditional error rate by those blue shaded area but adds the red shaded area as new error. Because the two areas are equal, the expected error rate remains the same. If we only monitor the error rate or any metrics derived from it, the concept drift would be missed.}
%      \label{fig:logi_err_rate_penal}
% \end{subfigure}
% \begin{subfigure}[t]{0.49\linewidth}
%      \centering
%     \includegraphics[width = \textwidth, trim=.15in .2in .7in .45in, clip]{../figures/v14/demons_fig/2D_score_logi_modi_trunc_norm_0_7.png}
%      \captionsetup{width=.95\linewidth}
%      \caption{The the expected Hotelling $T^2$, Equation~(\ref{eqn:logi_dev_rate}), before and after concept drift by monitoring Hotelling $T^2$ of EWMA of the score function. The drifted data generating process has the increased area (red) larger than the decreased area (blue) resulting in net positive change in the expected the expected Hotelling $T^2$ of the score function, which indicates that it is more sensitive for monitoring concept drift than error-based methods.}
%      \label{fig:logi_score_rate_penal}
% \end{subfigure}
%   \caption{The comparison of penalty functions by monitoring classification error and Hotelling $T^2$ of EWMA of the score function for the logistic regression model.}
%   \label{fig:logi_med_penal}
% \end{figure}

% Here, we revisit the illustrative example of simple logistic regression introduced in Section~\ref{ss:score_func} to complete the discussion on how we use MEWMA to resolve the limitation error-based methods have. According to Equation~(\ref{eqn:logi_mod_score}), the Hotelling $T^2$ for the logistic regression model can be written as:
% \begin{align}
% T_t^2 = (Y_t-p^{(0)}(X_t))^2 \bm{X}_t^T\hat {\bm { \Sigma}} ^{(0)-1}\bm{X}_t
% \label{eqn:logi_hotellingt2}
% \end{align}
% where the notations follow Equations~(\ref{eqn:logi_mod_score}) and~(\ref{eqn:simp_nota_p}). Similarly to Equation~(\ref{eqn:logi_err_rate}), we can take expectation to the quantity above to evaluate the change of monitoring statistics before and after concept drift, which we refer to as the expected Hotelling $T^2$. Notice that here we substitute $\bm{\Sigma}^{(0)}=E[p^{(0)}(X)(1-p^{(0)}(X))\bm{XX}^T]$ for the estimated covariance $\hat{\bm{\Sigma}}^{(0)}$ in Equation~(\ref{eqn:hotellingt2}) to obtain the following equations to simplify the analysis. 
% % \begin{align}
% % \begin{aligned}
% % dev(x;\bm{\theta}^{(j)})\vcentcolon=&E_{\bm{\theta}^{(j)}}[T^2|\bm{X}=[1,x]^T] \\
% % =& (p^{(j)}(x)-2p^{(j)}(x)p^{(0)}(x)+p^{(0)2}(x)) \bm{x}^T \bm { \Sigma}^{(0)-1}\bm{x} \\
% % dev(\bm{\theta}^{(j)})\vcentcolon=&E[dev(X;\bm{\theta}^{(j)})]
% % \end{aligned}
% % \label{eqn:logi_dev_rate}
% % \end{align}
% \begin{align}
% \begin{aligned}
% &E_{\bm{\theta}^{(j)}}[T^2|\bm{X}=[1,X]^T]
% = (p^{(j)}(X)-2p^{(j)}(X)p^{(0)}(X)+p^{(0)2}(X)) \bm{X}^T \bm { \Sigma}^{(0)-1}\bm{X} \\
% &E_{\bm{\theta}^{(j)}}[T^2] = E[E_{\bm{\theta}^{(j)}}[T^2|\bm{X}=[1,X]^T]] = \int E_{\bm{\theta}^{(j)}}[T^2|\bm{X}=[1,x]^T]q(x)dx
% \end{aligned}
% \label{eqn:logi_dev_rate}
% \end{align}
% After concept drift, assuming that the covariance matrix does not change much before and after concept drift ($\bm{\Sigma}^{(1)}\approx\bm{\Sigma}^{(0)}$), the the expected Hotelling $T^2$ can be decomposed as $E_{\bm{\theta}^{(1)}}[T^2]=E[Tr(p^{(1)}(1-p^{(1)})\bm{XX}^T\bm{\Sigma}^{(0)-1})]+E[(p^{(1)}-p^{(0)})^2\bm{X}^T\bm{\Sigma}^{(0)-1}\bm{X}]=E[Tr(\bm{\Sigma}^{(1)}\bm{\Sigma}^{(0)-1})]+E[(p^{(1)}-p^{(0)})^2\bm{X}^T\bm{\Sigma}^{(0)-1}\bm{X}]\approx E_{\bm{\theta}^{(0)}}[T^2]\\+E[(p^{(1)}-p^{(0)})^2\bm{X}^T\bm{\Sigma}^{(0)-1}\bm{X}]\gtrapprox E_{\bm{\theta}^{(0)}}[T^2]$, where $Tr(\cdot)$ is the trace of a matrix. This can be visualized in Figure~\ref{fig:logi_med_penal}. The shaded areas under curves represent the monitoring statistics. As shown in Figure~\ref{fig:logi_err_rate_penal} of the error rate of the error-based method, it does not change before and after concept drift (the difference between Figures~\ref{fig:logi_err_rate_unch_a}/\ref{fig:logi_err_rate_unch_b} and Figure~\ref{fig:logi_err_rate_penal} is that here we change the vertical axis label from $P(Y=1|X=x;\bm{\theta})$ to $E_{\bm{\theta}}[I(Y\neq \hat{y})|X=x]$ for ease of comparison with the score-based method); while in Figure~\ref{fig:logi_score_rate_penal} of the expected Hotelling $T^2$ of the score-based method, it increases after concept drift, which would be detected using our MEWMA monitoring method. After detection, the model can be updated to improve the performance as in Figure~\ref{fig:logi_err_rate_unch_d}. In Section~\ref{s:demon_cd}, a numerical example would be presented to further support the analysis here.

% Why we want it to be normally distributed?

% \subsection{Implementation of Monitoring the Score Function and Other Metrics}
% \label{ss:MEWMA}

\subsection{Handling High-Dimensional and Regularized Models}
\label{ss:high_dim_score}
One of the advancement of machine learning is that models become increasingly complex. Some state-of-the-art models can have millions of parameters, like convolutional neural network. With such high-dimension of parameters, the sample covariance matrix in Equation~(\ref{eqn:hotellingt2}), $\hat {\bm { \Sigma}}$, is very likely to be close to singular. For example, when the sample size of our training data (denoted as $n_1$ as in Section~\ref{ss:MEWMA}) is smaller than the dimension of parameters, $dim(\bm { \theta})$, the sample covariance would be singular. 

To solve this problem, we can add a nugget parameter on all diagonal entries of $\hat {\bm { \Sigma}}$ or use pseudo-inverse of the sample covariance. For the method of adding a nugget parameter $ \delta$, we substitute $\tilde {\bm { \Sigma}} = \hat {\bm { \Sigma}}+ \delta \bm {I}$ for $\hat {\bm { \Sigma}}$ as the approximated covariance matrix, in Equation~(\ref{eqn:hotellingt2}). To understand the effect of this nugget parameter, denote the eigen-decomposition of the sample covariance matrix as $\hat {\bm { \Sigma}} = \bm {Q}\bm { \Lambda} \bm {Q}^T$ and obtain the eigen-values as $ diag(\bm{\Lambda}) = [ \lambda_1, \lambda_2,\cdots, \lambda_n]$ in a non-increasing order. Then, we can write the approximated sample covariance matrix as $\tilde {\bm { \Sigma}} = \bm {Q}\tilde{\bm { \Lambda}} \bm {Q}^T$, where $\tilde{\bm { \Lambda}} = \bm { \Lambda} + \delta \bm {I}$. This would suppress unimportant directions of variation in Equation~(\ref{eqn:hotellingt2}). The EWMA of the new statistics with this modified covariance matrix would not have the issue of ill-conditioning. 

Setting a condition number achieves a similar purpose. By setting a maximum condition number, $ \gamma$, we set all $ \lambda_i$ equal to $0$, if $ \gamma \lambda_i \leq \lambda_1$. Denote the maximum of the index of $ \lambda_i$ which is not set to $0$ as $k$ and a new diagonal matrix $\bm { \Lambda} ^{-}$ with diagonal entries as $[1/\lambda_1,1/\lambda_2, \cdots, 1/\lambda_k, 0, \cdots, 0]$. Then, a pseudo-inverse of the sample covariance matrix is defined as $\hat {\bm { \Sigma}} ^{-} = \bm {Q}\bm { \Lambda}^{-}\bm {Q}^T$. This is equivalent to applying PCA onto $\bm {z}_t$, so that the most important variations in $\bm {z}_t$ are kept. 

Comparing with adding a nugget parameter, even though both methods give similar results, setting the maximum condition number is more intuitive in terms of controlling the behavior of inverting the covariance matrix, while adding the nugget allows the concept drift to be detected in those directions, that would be otherwise set to $0$ in hard thresholding by setting the maximum condition number. So we choose to add a nugget parameter.

% \subsection{Score function of Regularized Models}
% \label{ss:score_regu}
Another related issue is regularization of complex models, which is almost always required to combat overfitting. The regularization term is used to penalize complex models and large parameters, by minimizing the negative log-likelihood plus a norm of parameters, for example, $L_2$ norm of all parameters: $l(\bm{\theta})=-\frac{1}{n}\sum_{i=1}^n \log P(y_i|\bm{x}_i;\bm{\theta})+\frac{c}{2}||\bm{\theta}||_2^2$, where $c>0$ is a regularization parameter. The regularization term would change the score function of the original model: $\nabla_{\bm{\theta}}l(\bm{\theta}) = -\frac{1}{n}\sum_{i=1}^n\bm{s}(\bm{\theta};(\bm{x}_i,y_i))+c\bm{\theta}$. It can be looked as a prior on parameters from Bayesian perspective, which can be ``distributed" among all data: $\nabla_{\bm{\theta}}l(\bm{\theta})=-\frac{1}{n}\sum_{i=1}^n(\bm{s}(\bm{\theta};(\bm{x}_i,y_i))-c\bm{\theta})=-\frac{1}{n}\nabla_{\bm{\theta}}\sum_{i=1}^n\log(P(y_i|\bm{x}_i;\bm{\theta})\exp\{-\frac{c}{2}||\bm{\theta}||_{2}^2\})$. We can treat the likelihood times prior as the new model: $P(Y|\bm{X};\bm{\theta})\exp\{-\frac{c}{2}||\bm{\theta}||_{2}^2\}$. Then, all methods of calculating score vectors and applying EWMA and Hotelling $T^2$ follows. In some popular form of penalization, this would not change the monitoring statistics. For example, adding a $L_2$ regularization term of all parameters would only add a constant vector to all score vectors: $\bm{s}(\bm{\theta};(\bm{x}_i,y_i))-c\bm{\theta}$, which would be canceled after minus the mean of score vectors in Equation~(\ref{eqn:hotellingt2}). Adding regularization would have another good effect. When $dim ( \bm { \theta})$ is too large, $L_2$ penalization would add a diagonal matrix with positive diagonal entries to the sample covariance matrix: $-\nabla_{\bm{\theta}}(\bm{s}(\bm{\theta};(\bm{x}_i,y_i))-c\bm{\theta})=-\nabla_{\bm{\theta}}\bm{s}(\bm{\theta};(\bm{x}_i,y_i))+c\bm{I}=\mathbf{I}(\bm{\theta})+c\bm{I}$, where $\mathbf{I}$ and $\bm{I}$ are Fishier information and identity matrix respectively, automatically resulting in a well-conditioned matrix. 

\section{Diagnostics and Enhanced Monitoring of Individual Components}
\label{s:decou_cd}
The Hotelling $T^2$ statistic aggregates mean shifts in the components of the score vector into a single scalar statistic. In order to provide diagnostic insight into the nature of the change in $P(Y| \bm {X}, \bm{\theta})$ (e.g., which parameters have changed) and/or to enhance the ability of the procedure to detect some changes, it is helpful to monitor individual components of the score vector $\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y))$ or a transformed version described below. To construct a univariate EWMA chart for the $j$th component ($j=1,2,\cdots,p$) of the score vector, denoted by $s_{j,t} \vcentcolon= [\bm{s}(\hat{\bm { \theta}}^{(0)};(\bm {x}_t, y_t))]_j$, the univariate counterpart of Eq. (\ref{eqn:ewma}) is

\begin{align}
z_{j,t} = \lambda s_{j,t} + (1 - \lambda) z_{j,t-1}.
\label{eqn:uniewma}
\end{align}
In this case, $z_{j,t}$ is plotted directly on the univariate EWMA chart with both a LCL and UCL. The chart signals a change in the mean of the $j$th component at observation $t$ if $z_{j,t}$ falls either below the LCL or above the UCL. If $\lambda$ is small, $z_{j,t}$ is approximately normal by the central limit theorem, and one can set  $\{LCL_j,UCL_j\} = \bar{z}_j \pm z_{\alpha/2}SD[z_j]$, where $\bar{z}_j$ and $SD[z_j]$ denote the sample average and standard deviation of $\{z_{j,t}\}_{t=n_1+1}^n$ over the same Phase-I data depicted in Figure \ref{fig:proc_mon_score_monitoring}, $z_{\alpha/2}$ is the upper $\alpha/2$ quantile of the standard normal distribution, and $\alpha$ is the desired false alarm rate. Alternatively, if the Phase-I sample size $n-n_1$ is sufficiently large, one can choose $LCL_j$ and $UCL_j$ directly as the lower and upper $\alpha/2$ quantiles of the empirical distribution of $\{z_{j,t}\}_{t=n_1+1}^n$. 

If the goal is to diagnose and isolate which parameter(s) have changed, then it is preferable to replace the score components in the univariate EWMA by the components of a transformed version of the score vector discussed below, to effectively decouple the changes in the individual parameters. To illustrate this coupling, consider a linear Gaussian regression model $Y = \bm{X}^T\bm{\theta} + \epsilon$ with $\epsilon$ following a zero-mean Gaussian distribution with variance $\sigma^2$, the score function for which is $\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y)) = \frac{(Y - \bm{X}^T\bm{\theta}^{ (0)})\bm{X}}{\sigma^2}$. With no concept drift (i.e., $\bm { \theta} = \bm { \theta}^{ (0)}$), the mean of the score function is $E_{\bm{ \theta}^{ (0)}}[\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y))] = E_{\bm{ \theta}^{ (0)}}[E_{\bm{ \theta}^{ (0)}}[ \frac{(Y - \bm{X}^T\bm{\theta}^{ (0)})\bm{X}}{\sigma^2}|\bm {X}]] =  E_{\bm{ \theta}^{ (0)}}[\bm{0}] = \bm{0}$. After concept drift, suppose the parameters change to $\bm { \theta} ^{ (1)}$, and denote this change by $ \Delta \bm { \theta} = \bm { \theta} ^{ (1)} - \bm { \theta}^ { (0)}$. The mean of the score function after the concept drift is $E_{\bm{ \theta}^{ (1)}}[\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y))] = E_{\bm{ \theta}^{ (1)}}[E_{\bm{ \theta}^{ (1)}}[\frac{(Y - \bm {X}^T\bm { \theta}^{ (1)} + \bm {X}^T\Delta \bm { \theta}) \bm {X}}{\sigma^2} |\bm {X}]] = E_{\bm{ \theta}^{ (1)}}[E_{\bm{ \theta}^{ (1)}}[\frac{\bm {X}\bm {X}^T\Delta \bm { \theta})}{\sigma^2} |\bm {X}]] = \frac{E [\bm {X}\bm {X}^T] \Delta \bm { \theta}}{\sigma^2}$. Thus, we can decouple the changes in the parameters by premultiplying the score vector mean by the inverse of $\frac{E [\bm {X}\bm {X}^T]}{\sigma^2}$, i.e., via the transformation $ \Delta \bm { \theta} = \sigma^2 \big[E [\bm {X}\bm {X}^T]\big]^{-1} E_{\bm{ \theta}^{ (1)}}[\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y))]$. 

For the more general $P(Y|\bm{X};\bm{\theta})$, we can decouple the parameter changes via a similar transformation. For the general case, the score function mean prior to the concept drift is $\bm{0}$, and the mean after the drift is

\begin{align}
\begin{aligned}
E _{\bm { \theta}^{ (1)}}[\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y))] 
= & E[E _{\bm { \theta}^{ (1)}}[\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y))| \bm {X}] ] \\
= & E[\int \bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, y)) dP(y | \bm {X}, \bm{\theta} ^{ (1)})].
\end{aligned}
\label{eqn:cd_mean_shift}
\end{align}
Using a first-order Taylor expansion of $\bm{s}(\bm { \theta}^{ (0)}; (\bm {x}, y))$ about $\bm { \theta} ^{ (1)}$,
\begin{align}
\begin{aligned}
 &\bm{s}(\bm { \theta}^{ (0)}; (\bm {x}, y)) \cong \bm{s}(\bm { \theta}^{ (1)}; (\bm {x}, y)) 
 - \nabla _{\bm { \theta}}{ \bm{s}(\bm { \theta}^{ (1)}; (\bm {x}, y))} \Delta\bm{ \theta}. 
\end{aligned}
\label{eqn:sc_ty_expa}
\end{align}
Combining Equations~(\ref{eqn:sc_ty_expa})  and~(\ref{eqn:cd_mean_shift}) gives
\begin{align}
\begin{aligned}
E _{\bm { \theta}^{ (1)}}[\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y))] & \cong E \big[ \int \bm{s}(\bm { \theta}^{ (1)}; (\bm {X}, y)) dP(y| \bm {X}, \bm{\theta}^{ (1)}) \big] \\ 
 & - E \big[ \int \nabla _{\bm { \theta}}{ \bm{s}(\bm { \theta}^{ (1)}; (\bm {X}, y))} \Delta\bm{ \theta} dP(y| \bm {X}, \bm{\theta}^{ (1)}) \big] \\ 
& = E \big[ E _{\bm { \theta}^{ (1)}}[\bm{s}(\bm { \theta}^{ (1)}; (\bm {X}, Y))| \bm {X}] \big] \\
& - E \big[ \int \nabla_{\bm { \theta}} \nabla^T _{\bm { \theta}}{ \log P(y|\bm{X};\bm { \theta}^{ (1)})} \Delta\bm{ \theta} dP (y| \bm {X}, \bm{\theta}^{ (1)}) \big] \\
& = \bm{0} -  E \big[ E _{\bm { \theta}^{ (1)}}[\nabla_{\bm { \theta}} \nabla ^T_{\bm { \theta}}{ \log{P}(Y|\bm{X};\bm { \theta}^{ (1)})} | \bm {X}] \big] \Delta\bm{ \theta} \\
& = \mathbf {I}(\bm { \theta}^{ (1)}) \Delta\bm{ \theta} \cong \mathbf {I}(\bm { \theta}^{ (0)}) \Delta\bm{ \theta}, \\
\end{aligned}
\label{eqn:cd_decomp_fisher_approx}
\end{align}
where, for $ j\in\{0,1\}$,
\begin{align}
\begin{aligned}
\mathbf {I}(\bm { \theta}^{ (j)}) &= -E \big[ E _{\bm { \theta}^{ (j)}}[\nabla_{\bm { \theta}} \nabla^T _{\bm { \theta}}{ \log{P}(Y|\bm{X};\bm { \theta}^{ (j)})} | \bm {X}] \big]  \\
&= E \big[ E _{\bm { \theta}^{ (j)}}[\bm{s}(\bm { \theta}^{ (j)}; (\bm {X}, Y)) \bm{s}^T(\bm { \theta}^{ (j)}; (\bm {X}, Y)) | \bm {X}] \big]\\
\end{aligned}
\label{eqn:fisher_approx}
\end{align}
is the (expected) Fisher information matrix at parameter $\bm { \theta} ^{ (j)}$. In the last line of (\ref{eqn:cd_decomp_fisher_approx}) we have approximated the Fisher information matrix at $\bm { \theta} ^{ (1)}$ with that at $\bm { \theta} ^{ (0)}$, since the latter can more easily be estimated as the sample covariance matrix of the score function $\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y))$ over the training and Phase-I data $\{(\bm{x}_t,y_t\}_{t=1}^n$. In the second equality of (\ref{eqn:fisher_approx}) we have used the well-known equivalency of the two expressions for the Fisher information matrix. 

In light of (\ref{eqn:cd_decomp_fisher_approx}), for general $P(Y|\bm{X};\bm{\theta})$, we can decouple the changes in the parameters via the transformation $ \Delta \bm { \theta} = \mathbf {I}^{-1}(\bm { \theta}^{ (0)}) E_{\bm{ \theta}^{ (1)}}[\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y))]$, where $\mathbf {I}(\bm { \theta}^{ (0)})$ is estimated by the sample covariance matrix of $\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y))$ over the training and Phase-I data, and $E_{\bm{ \theta}^{ (1)}}[\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y))]$ is estimated as the sample mean of $\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y))$ over some relevant window of Phase-II data following the concept drift. An alternative to the univariate EWMAs (\ref{eqn:uniewma}) on the score components, we can construct univariate EWMAs on the decoupled score components
\begin{align}
z_{j,t} = \lambda \tilde{s}_{j,t} + (1 - \lambda) z_{j,t-1},
\label{eqn:uniewma-decoupled}
\end{align}
where $\tilde{s}_{j,t}$ is the $j$th component of the decoupled score vector $\tilde{\bm{s}}_t := \mathbf {I}^{-1}(\bm { \theta}^{ (0)})\bm{s}_t$. The LCL and UCL for (\ref{eqn:uniewma-decoupled}) are determined analogously to those for (\ref{eqn:uniewma}), based on the empirical distribution of $\{z_{j,t}\}_{t=n_1+1}^n$.  

In addition to providing diagnostic information on which parameter(s) have changed, and how they have changed, the decoupled univariate EWMAs (\ref{eqn:uniewma-decoupled}) have the following, additional benefit over just using the MEWMA. As discussed earlier, under fairly general conditions, the mean of $\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y))$ changes if and only if $P(Y|\bm{X};\bm{\theta})$ changes. Thus, a change in $P(\bm{X})$ alone with no change in $P(Y|\bm{X};\bm{\theta})$ will not cause the mean of $\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y))$, or the mean of $\bm{z}_t$ in (\ref{eqn:ewma}), to change. However, a change in $P(\bm{X})$ alone can cause the mean of the Hotelling $T^2$ statistic in (\ref{eqn:hotellingt2}) to increase (by changing the covariance of $\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y))$), thus potentially leading to more frequent false alarms if the goal is to signal only when $P(Y|\bm{X};\bm{\theta})$ changes. This is easy to see for the case of the linear Gaussian regression model $Y = \bm{X}^T\bm{\theta} + \epsilon$ with score function $\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y)) = \frac{(Y - \bm{X}^T\bm{\theta}^{ (0)})\bm{X}}{\sigma^2}$. If $P(Y|\bm{X};\bm{\theta})$ does not change (i.e., $\bm{\theta}$ remains unchanged at $\bm{\theta}^{ (0)}$), then we still have that $\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y)) = \frac{\epsilon \bm{X}}{\sigma^2}$ is zero mean regardless of whether $P(\bm{X})$ changes, but the covariance of $\epsilon \bm{X}$ can obviously change.   

The univariate EWMAs are more robust to false alarms that are caused by a change in $P(\bm{X})$ and a resulting covariance change in $\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y))$, because they chart the individual $z_{j,t}$ components directly and not some quadratic form like the $T^2$ statistic, and the mean of $z_{j,t}$ changes if and only if $P(Y|\bm{X};\bm{\theta})$ changes. A variance change in $z_{j,t}$ typically would not increase the false alarm rate as much as if its mean changes. Moreover, by visual inspection of the univariate EWMA charts, it is easier to determine if an alarm was due to a mean change or to a variance change in $z_{j,t}$ than with the aggregated $T^2$ statistic in the MEWMA chart.  Because a single aggregated monitoring statistic has other advantages, in practice we recommend using both the MEWMA and the decoupled univariate EWMAs, which we illustrate with the examples in the subsequent sections. 


\section{Examples and Performance Comparisons}
\label{s:real_data}
In this section we present simulation and real data examples to demonstrate the performance of our score-based concept drift approach and illustrate its usage. In Section~\ref{ss:cd_no_err_change}, we provide a simulation example where concept drift results in no change in expected error rate (and thus, error-based methods cannot detect it), but our score-based approach is able to effectively detect it. We then provide two real data examples, the first being credit default data from a major financial company from $2003$--$2008$, during which time the subprime mortgage crisis happened. The second is Capital Bikeshare rental data from $2010$--$2020$ during which time the ``sharing economy" steadily expanded. The concept drift in these two real data sets corresponds to relatively ``abrupt" and ``gradual" changes, respectively, and so they illustrate different characteristics of our score-based concept drift approach. 

With real data sets, it is generally difficult to know exactly when or in exactly what capacity the concept drift occurred. In order to more quantitatively evaluate and compare the performances of various methods in detecting concept drift, we also use Monte Carlo(MC) simulation examples to compute the median run length (MRL) for various examples and various methods in Phase-II when there is no concept drift (denoted $MRL_0$) and also when there is concept drift (denoted $MRL_1$). The run length is defined as the number of observations between when a change in $\bm{\theta}$ occurs and when the chart first signals a change has occurred, and the MRL is defined as the median run length across a set of Monte Carlo replicates~(\cite{montgomery2007introduction}). One desires a longer $MRL_0$, which corresponds to a smaller false alarm rate, and a shorter $MRL_1$, which corresponds to faster detection of concept drift. 

The details of the $MRL$ performance comparisons from the MC simulations are in Appendix~\ref{ss:simu_MRL}, and in Appendix~\ref{ss:cd_diag} we demonstrate diagnostic capabilities of the decoupling approach when the covariates are correlated, which compounds the coupling phenomenon. Here, we briefly summarize the setting, main results, and conclusions of the MC simulations. We considered the following classification and regression settings in which the data generation processes were linear regression, logistic regression, multinomial regression, and Poisson regression, and the fitted supervised learning models were of the same form, as well as neural network models. On each MC replicate, models were fit as described in Section~\ref{ss:MEWMA} to a set of simulated training data. Then the control limits of all concept drift detection methods were determined from a set of simulated Phase-I data. To have a common basis for comparing the detection delays via the $MRL_1$ values, the control limits of all methods were selected to give a common $MRL_0$, where $MRL_0$ was computed using MC simulation over replicated sets of Phase-I data. An abrupt concept drift (i.e., an abrupt change in $\bm{\theta}$ from $\bm{\theta}^{(0)}$ to $\bm{\theta}^{(1)}$) was then introduced in a set of simulated Phase-II data, and the run length for each concept drift detection method was taken to be the time delay between the change in $\bm{\theta}$ and when the method detected the change. The preceding constitutes one replicate of the MC simulation for an example. The $MRL_1$ values for each method for that example were then taken to be the median of the run lengths across the set of MC replicates.

The concept drift methods to which we compare our score-based MEWMA are an EWMA on the error rate (for classification problems) and an EWMA on the absolute error (for regression problems), which are the most effective of the existing methods, to the best of our knowledge. The EWMA on the error rate is of the form (\ref{eqn:uniewma}), but with $s_{j,t}$ replaced by a binary variable that is $1$ if $y_t$ was misclassified and $0$ if $y_t$ was correctly classified, using the classification model that was fit to the training data. The EWMA on the absolute error is also of the form (\ref{eqn:uniewma}), but with $s_{j,t}$ replaced by the absolute value of the prediction error in predicting $y_t$ using the regression model fit to the training data. We found this to be more effective at detecting concept drift than an EWMA on the prediction errors themselves. This is likely because the concept drift caused an increase in the variance of the prediction errors more than (or in addition to) a change in the mean of the prediction errors. ??Kungang, verify that what I wrote above is correct. Also, can you provide references for any of these existing methods??

The main conclusion from these MC simulations (for details see Appendix~\ref{ss:simu_MRL} and in particular, Tables~\ref{tab:logi_MRL}--\ref{tab:lin_nnet_MRL}) is that our score-based method achieves smaller $MRL_1$ values (much faster detection) than the alternative methods across every example that we considered. The $MRL_1$ values are usually substantially smaller, sometimes as much as four to five times smaller. In addition, as shown in Appendix~\ref{ss:cd_diag}, the decoupling approach effectively indicates which parameters have changed. ??Not sure if we should say more about this. Is there anything else interesting to say? I did not understand what you had written previously??

\subsection{An Example of Concept Drift with No Change in the Error Rate}
\label{ss:cd_no_err_change}
\begin{figure}[!htp]
\centering
\includegraphics[width = \linewidth]{../figures/v14/sim_11/non_nnet_nonunif_ch_f_0_2/1_sim11_logi_1e-08_0_0015_1.png}
  \caption{Illustration of concept drift detection performance for a logistic regression example in which the change in $\bm{\theta}$ results in no change in the error rate. The blue vertical line is the boundary between the Phase-I and Phase-II data, and the green vertical line indicates when $\bm{\theta}$ changed. The top plot is the score-based MEWMA, and the bottom plot is the EWMA on the error rate. The score-based method effectively detects the concept drift, even though there is no change in error rate.}
  \label{fig:exp_no_err_ch}
\end{figure}
In Section~\ref{ss:score_func}, we gave a simple logistic regression example (see Figure~\ref{fig:logi_err_rate_unch}) in which there is concept drift, but the concept drift does not result in a change in error rate. Consequently, methods based on monitoring the error rate will fail to detect the concept drift, whereas our score-based method can still detect the drift. We return to this example and show the monitoring performance of both methods over a set of simulated data generated from the logistic regression model (\ref{eqn:logi_mod_score}) with $\bm{\theta}^{(0)} = ??$ prior to the concept drift and $\bm{\theta}^{(1)} = ??$ following the concept drift. The logistic regression model was fit and the control limits computed via the procedure described in Section~\ref{ss:MEWMA}, using simulated training and Phase-I data, respectively. Figure~\ref{fig:exp_no_err_ch} shows the monitoring results over both the Phase-I and Phase-II data. The latter were also generated from the same logistic regression model with parameters $\bm{\theta}^{(0)}$ up until the parameters changed at observation index $40,000$, after which the remainder of the Phase-II data were generated using parameters $\bm{\theta}^{(1)}$. As seen in Figure~\ref{fig:exp_no_err_ch}, our score-based MEWMA promptly detects the concept drift, whereas the EWMA on the error rate does not detect the concept drift. 

If the concept drift does not cause a change in the error rate, one might wonder whether it is important to detect the concept drift. The answer is ``yes", because it indicates to the user that there is an opportunity to improve the model and further reduce the error rate. In the above example, using the predictive model with parameters $\bm{\theta}^{(1)}$, the mean error rate is $0.26??$ before, as well as after, the parameters of the data generation model change from $\bm{\theta}^{(0)}$ to $\bm{\theta}^{(1)}$. However, after the concept drift is detected, if the parameters of the predictive model are updated to $\bm{\theta}^{(1)}$ by refitting the model to the new data, then the error rate decreases substantially from $0.26??$ to $0.15??$. 


\subsection{Credit Risk Modeling Example}
\label{ss:cr_ds}
In the credit risk data set, each row corresponds to a unique credit card customer of a major financial company, and we have $196587$ rows of data in total. The covariates for row/customer $i$ include various customer information ($\bm {x}_i$) available at the time the customer applies for the credit account, and the binary response ($y_i$) indicates whether the customer defaults within the first $9$-months after opening the account. For the purposes of plotting various quantities over time, we associate each row $i$ with a time stamp that is taken to be the day on which the response $y_i$ first becomes available. Specifically, the set of all customers associated with a particular day, which we refer to as their ``entry day" into the data set, are those who opened their account within nine months of that day and defaulted on that day (in which case they are assigned $y_i=1$), together with those who opened their account exactly nine months prior to that day and did not default (in which case they are assigned $y_i=0$). The data are imbalanced in the sense that there are $3536$ observations with $y_i = 1$, which accounts for around $1.8\%$ of the data. Consequently, when fitting the model, we upsampled the minority class $20$ times each to get a more balanced training data set. These data were originally considered in~\cite{im2012time}, who focused on the same $10$ covariates that we consider in this study: $x_1$ (credit risk score from an earlier model used by the company), $x_2$ (credit bureau risk score), $x_3$ (highest credit limit for open revolving credit accounts), $x_4$ (total balance on all open revolving credit accounts), $x_5$ (balance on the highest-utilization open revolving credit account), $x_6$ (credit limit on the highest-utilization open revolving credit account), $x_7$ (number of inquiries in the last 24 months), $x_8$ (balance on open mortgages), $x_9$ (categorical variable involving status of savings accounts), and $x_{10}$ (number of inquiries in the last 24 months, excluding the last two weeks). 

We applied concept drift detection to two supervised learning models:  A logistic regression model and a neural network (with one hidden layer having $50$ activation nodes). The main reason the company fit models of this nature was to score credit card applicants for risk (via their predicted probability of defaulting) at the time they apply. For the purpose of evaluating the concept drift detection, we trained and validated both models on the data from $2003$-Jan to $2005$-Dec (after upsampling the minority class). The data from $2006$-Jan to $2006$-Dec are used as the Phase-I data to calculate control limits. The remainder of the data, from $2007$-Jan to $2008$-Aug are used as the Phase-II data to test the capability of detecting the concept drift due to the subprime mortgage crisis. Recall that the S\&P 500 declined by more than $50\%$ over a $15$-month period between the end of $2007$-Dec and the end of $2009$-Mar (from 1478 to 683). The prevailing view is that the major root cause of the stock market decline was the subprime crisis, which had been gradually developing prior to that.

We applied our score-based MEWMA with the goal of detecting concept drift as far in advance of the beginning of the crash ($2007$-Dec) as possible. For comparison, we also applied an EWMA on the binary classification error, which is considered state-of-the-art among existing concept drift detection methods in terms of fast detection~(\cite{barros2018large}). The predicted label, used to calculate the binary classification error, is $1$ if the predicted probability exceeds a specified threshold, and $0$ otherwise). We chose a threshold of $0.2490$ to match the true positive rate and the true negative rate as closely as possible (alternatively, if costs of false positives and false negatives were known in advance, we could have chosen the threshold to minimize the total misclassification costs). For both methods, we selected $\lambda = 0.001$, so that the effective window is $\frac{1}{\lambda} = 1,000$ customers, which corresponds roughly to one week. Figure~\ref{fig:credit_default} shows the results of both concept drift monitoring methods for both models (logistic regression and neural network) over the Phase-I and Phase-II data. 

As seen in Figure~\ref{fig:credit_default}, for the logistic regression model, the score-based MEWMA consistently signals (i.e., the $T^2$ statistic consistently falls above the UCL) beginning around $2007$-Jun, which is a full $6$-months prior to the beginning of the economic crash in $2007$-Dec. In this sense, the score-based method provides advanced warning that something has changed substantially in the predictive relationship between $Y$ and $\bm{X}$, which could have been indicative that serious economic changes were evolving. At the very least, it would have been an indication that the fitted credit risk scoring model was becoming obsolete and that more effective scoring could be achieved by updating the model. In contrast, the EWMA on the error rate does not consistently signal until much later, $2008$-Feb, which is $8$ months after the score-based MEWMA began to consistently signal and $2$ months after the economic crash began. The results for the neural network model, which  are shown in the right panels of Figure~\ref{fig:credit_default}, are very similar. 

\begin{figure}[!htbp]
\centering
\includegraphics[width = 0.49\linewidth]{../figures/v14/credit_default/logi_scal_train_PI/credit_logi_1e-08_0_0001_0_001_99_0.png}
\includegraphics[width = 0.49\linewidth]{../figures/v14/credit_default/logi_nnet_scal_train_PI/credit_logi_0_002_0_0001_0_001_99_0.png}
  \caption{
For the credit risk example, comparison of concept drift monitoring performance for our score-based MEWMA (top plots) versus an EWMA on the classification error (bottom plots). The left and right plots are for the logistic regression and neural network models, respectively. The blue vertical line is the boundary between the Phase-I data (which were used to establish the control limits) and the Phase-II data. The tilted numbers along the horizontal axes at below each plot are the year-month indices (e.g., $6$-$1$ stands for 2006-Jan). The score-based MEWMA consistently signals beginning $2007$-Jun, which is six months prior to the beginning of the stock market crash in $2007$-Dec. In contrast, the EWMA on the classification error does not consistently signal until $2008$-Feb. 
}
\label{fig:credit_default}
\end{figure}

Figure~\ref{fig:credit_default_diag} shows several representative univariate component EWMAs (for $\theta_1$, $\theta_2$, $\theta_3$, $\theta_8$, and the intercept $\theta_0$) for the logistic regression model over the Phase-I and Phase-II data. The MEWMA is also shown in the top row as a reference. The component EWMAs in the left column are for the original (coupled) score components (\ref{eqn:uniewma}), and those in the right column are for the decoupled score components (\ref{eqn:uniewma-decoupled}). Decoupling the score components reveals different patterns of concept drift than are not seen in the original score components. In particular, the decoupled component EWMA for the intercept parameter (bottom right plot) shows a clear upwards drift over the entire range of data, whereas this drift is not apparent in the corresponding original component EWMA (bottom left plot). The correlation between the constant intercept term and some of the other covariates has evidently conflated the drift in their corresponding parameters in the left column plots. The upwards drift in the decoupled intercept parameter is telling, as the intercept can be viewed as a regression-adjusted indicator of the overall default rate. Since the drift was upwards, this means that the intercept parameter increased over time, which means that the regression-adjusted likelihood of default (i.e., the default for applicants having the same covariate values) increased substantially over time. In fact, the concept drift is even more evident, and evident earlier, in the decoupled component EWMA for $\theta_0$ than in the MEWMA. For example, the decoupled component EWMA for $\theta_0$ falls consistently above the center line ($0$) beginning back in the Phase-I data, around $2006$-Jun. Even though it is not consistently above the UCL at this point, the fact that it is consistently above the centerline is an indication that $\theta_0$ has increased. In SPC control charting in general, users typically look for these types of patterns in the charts to signal changes in the mean of what is being charted (\cite{montgomery2007introduction}). 

The decoupled component EWMA chart for $\theta_1$ (right column, second from top) also shows clear upwards drift in $\theta_1$, which means that $x_1$ (credit risk score from an earlier model) should be given more weight in predicting credit risk as time evolves. The decoupled component EWMA chart for $\theta_2$ (right column, third from top) is less clear, because it wanders up ($\sim 2007$-Aug) then down ($\sim 2008$-Jan), then up again ($\sim 2008$-April). Rather than a change in the mean, which indicates a change in $P(Y|\bm{X};\bm{\theta})$, it could indicate a change in variance that results from a change in $P(\bm{X})$. Notice also that the corresponding plot in the left column is quite different than the decoupled version in the right column. Again, this is because the covariates are correlated, in which case the original score components conflate the drift in the parameters.  

% \begin{figure}[!htbp]
\begin{figure}[H]
\centering
 \includegraphics[width = 0.48\linewidth]{../figures/v14/credit_default/logi_scal_train_PI/pos_single_credit_mlines_with_regu_1e-08_0_0001_0_001_99_0.png}
  \includegraphics[width = 0.48\linewidth]{../figures/v14/credit_default/logi_scal_train_PI/pos_single_credit_fisher_mlines_with_regu_1e-08_0_0001_0_001_99_0.png}
  \caption{
  MEWMA (top row) and various univariate component EWMA (other rows, for $\theta_1$, $\theta_2$, $\theta_3$, $\theta_8$, and the intercept $\theta_0$) control charts for the logistic regression model in the credit risk example over the Phase-I and Phase-II data. The left column are the original score component EWMAs (\ref{eqn:uniewma}), and the right column are the decoupled component EWMAs (\ref{eqn:uniewma-decoupled}). The horizontal lines are the control limits and also a line indicating the zero value for the EWMAs. The vertical line is the boundary between the Phase-I and Phase-II data. The decoupled component EWMAs show clear drift in a number of parameters, especially the intercept (bottom right).
}
\label{fig:credit_default_diag}
\end{figure}

\subsection{Bike Sharing data set}
\label{ss:bs_ds}

===========================================================
New figures for bike sharing data set
\begin{enumerate}
    \item  Step A:
    
\begin{figure}[H]
\centering
    \begin{subfigure}[t]{0.4\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_A/PII_neg_single_retro_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Retrospective Diagnosis plots for a model trained using raw bike rental counts. The training data is from $2010$-Sep to $2017$-Dec.}
     \label{fig:bs_raw_cnt}
\end{subfigure}
\caption{
}
\label{fig:bike_sharing_diag}
\end{figure}

\item Step B-1:

\begin{figure}[H]
\centering
    \begin{subfigure}[t]{0.4\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_B_1/PII_neg_single_retro_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Retrospective Diagnosis plots for a model trained using raw bike rental counts. The training data is from $2012$-Jun to $2015$-Jun, and the control limit is set based on training data, but applied on data from $2010$-Sep to $2017$-Dec.}
     \label{fig:bs_raw_cnt}
\end{subfigure}
\caption{
}
\label{fig:bike_sharing_diag}
\end{figure}

\item Step B-2:

\begin{figure}[H]
\centering
    \begin{subfigure}[t]{0.4\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_B_2/PII_neg_single_retro_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Retrospective Diagnosis plots for a model trained using raw bike rental counts. The training data is from $2010$-Sep to $2017$-Dec, and the control limit is set based on data from $2012$-Jun to $2015$-Jun, but applied on the training data.}
     \label{fig:bs_raw_cnt}
\end{subfigure}
\caption{
}
\label{fig:bike_sharing_diag}
\end{figure}

\item Step C:

\begin{figure}[H]
\centering
    \begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_C/PII_pos_single_retro_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Retrospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2010$-Sep to $2017$-Dec, and the control limit is set based on the training data.}
     \label{fig:bs_raw_cnt}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_C_1/PII_pos_single_retro_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Retrospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2012$-Jan to $2017$-Dec, and the control limit is set based on the training data.}
     \label{fig:bs_raw_cnt_1}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_C_2/PII_pos_single_retro_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Retrospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2013$-Jan to $2017$-Dec, and the control limit is set based on the training data.}
     \label{fig:bs_raw_cnt_2}
\end{subfigure}
\caption{
}
\label{fig:bike_sharing_diag}
\end{figure}

Following are using covariance matrix for Fisher information matrix.

\begin{figure}[H]
\centering
    \begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cov_C/PII_pos_single_retro_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Retrospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2010$-Sep to $2017$-Dec, and the control limit is set based on the training data.}
     \label{fig:bs_raw_cnt}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cov_C_1/PII_pos_single_retro_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Retrospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2012$-Jan to $2017$-Dec, and the control limit is set based on the training data.}
     \label{fig:bs_raw_cnt_1}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cov_C_2/PII_pos_single_retro_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Retrospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2013$-Jan to $2017$-Dec, and the control limit is set based on the training data.}
     \label{fig:bs_raw_cnt_2}
\end{subfigure}
\caption{
}
\label{fig:bike_sharing_diag}
\end{figure}

\item Step D:

\begin{figure}[H]
\centering
    \begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_D/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2010$-Sep to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_D_1/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2012$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_1}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_D_2/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2013$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_2}
\end{subfigure}
\caption{
}
\label{fig:bike_sharing_diag}
\end{figure}

\begin{figure}[H]
\centering
    \begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_D/PII_neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2010$-Sep to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_D_1/PII_neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2012$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_1}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_D_2/PII_neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2013$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_2}
\end{subfigure}
\caption{
}
\label{fig:bike_sharing_diag}
\end{figure}

Following are using Phase-I data to estimate control limits.

\begin{figure}[H]
\centering
    \begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_PI_D/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2010$-Sep to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_PI_D_1/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2012$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_1}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_PI_D_2/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2013$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_2}
\end{subfigure}
\caption{
}
\label{fig:bike_sharing_diag}
\end{figure}

\begin{figure}[H]
\centering
    \begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_PI_D/PII_neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2010$-Sep to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_PI_D_1/PII_neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2012$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_1}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_PI_D_2/PII_neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2013$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_2}
\end{subfigure}
\caption{
}
\label{fig:bike_sharing_diag}
\end{figure}

Following are using covariance matrix for Fisher information matrix.

\begin{figure}[H]
\centering
    \begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cov_D/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2010$-Sep to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cov_D_1/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2012$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_1}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cov_D_2/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2013$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_2}
\end{subfigure}
\caption{
}
\label{fig:bike_sharing_diag}
\end{figure}

\begin{figure}[H]
\centering
    \begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cov_D/PII_neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2010$-Sep to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cov_D_1/PII_neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2012$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_1}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cov_D_2/PII_neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using normalized bike rental counts (using yearly mean). The training data is from $2013$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_2}
\end{subfigure}
\caption{
}
\label{fig:bike_sharing_diag}
\end{figure}

\item Step E:

\begin{figure}[H]
\centering
    \begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_E/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using raw bike rental counts (using yearly mean). The training data is from from $2010$-Sep to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_E_1/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using raw bike rental counts (using yearly mean). The training data is from from $2012$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_1}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_E_2/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using raw bike rental counts (using yearly mean). The training data is from from $2013$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_2}
\end{subfigure}
\caption{
}
\label{fig:bike_sharing_diag}
\end{figure}


\begin{figure}[H]
\centering
    \begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_E/PII_neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using raw bike rental counts (using yearly mean). The training data is from from $2010$-Sep to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_E_1/PII_neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using raw bike rental counts (using yearly mean). The training data is from from $2012$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_1}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_E_2/PII_neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using raw bike rental counts (using yearly mean). The training data is from from $2013$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_2}
\end{subfigure}
\caption{
}
\label{fig:bike_sharing_diag}
\end{figure}

Following are using Phase-I data to estimate control limits.

\begin{figure}[H]
\centering
    \begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_PI_E/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using raw bike rental counts (using yearly mean). The training data is from from $2010$-Sep to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_PI_E_1/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using raw bike rental counts (using yearly mean). The training data is from from $2012$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_1}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_PI_E_2/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using raw bike rental counts (using yearly mean). The training data is from from $2013$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_2}
\end{subfigure}
\caption{
}
\label{fig:bike_sharing_diag}
\end{figure}


\begin{figure}[H]
\centering
    \begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_PI_E/PII_neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using raw bike rental counts (using yearly mean). The training data is from from $2010$-Sep to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_PI_E_1/PII_neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using raw bike rental counts (using yearly mean). The training data is from from $2012$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_1}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_PI_E_2/PII_neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using raw bike rental counts (using yearly mean). The training data is from from $2013$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_2}
\end{subfigure}
\caption{
}
\label{fig:bike_sharing_diag}
\end{figure}

Following are using covariance matrix for Fisher information matrix.

\begin{figure}[H]
\centering
    \begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cov_E/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using raw bike rental counts (using yearly mean). The training data is from from $2010$-Sep to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cov_E_1/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using raw bike rental counts (using yearly mean). The training data is from from $2012$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_1}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cov_E_2/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using raw bike rental counts (using yearly mean). The training data is from from $2013$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_2}
\end{subfigure}
\caption{
}
\label{fig:bike_sharing_diag}
\end{figure}


\begin{figure}[H]
\centering
    \begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cov_E/PII_neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using raw bike rental counts (using yearly mean). The training data is from from $2010$-Sep to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cov_E_1/PII_neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using raw bike rental counts (using yearly mean). The training data is from from $2012$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_1}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cov_E_2/PII_neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Prospective Diagnosis plots for a model trained using raw bike rental counts (using yearly mean). The training data is from from $2013$-Jan to $2016$-Dec; Phase-I data is from the whole year of $2017$; Phase-II data is after $2018$-Jan.}
     \label{fig:bs_raw_cnt_2}
\end{subfigure}
\caption{
}
\label{fig:bike_sharing_diag}
\end{figure}

\end{enumerate}


===========================================================

The bike sharing data set\footnote{https://www.capitalbikeshare.com/system-data} comes from hourly-aggregated loggings of the Captial Bikeshare system in $7$ jurisdictions of the Washington metropolitan area from $2010$--$2020$, integrated with hourly weather data\footnote{https://www.freemeteo.com} over the same time period. The total sample size is $n=82093$, and each row corresponds to one hour, with the ($y$) being the number of bike rentals that hour, and the covariates ($\bm {x}$) being time- and environmental-related associated with that hour. We use the same definitions and preprocessing procedures described for the UCI data set\footnote{https://archive.ics.uci.edu/
ml/datasets/Bike+Sharing+Dataset}~(\cite{fanaee2014event}). The $d=11$ covariates are year ($x_1$, numerical with $11$ integer values from 2010 to 2020), month ($x_2$, categorical with $12$ categories: $1=Jan, 2=Feb, \cdots, 12=Dec$), hour ($x_3$, categorical with $24$ categories: $\{0,1,\cdots,23\}$), holiday ($x_4$, binary: $0=non-holiday,1=holiday$), weekday ($x_5$, categorical with $7$ categories: 
$0=Sun,1=Mon,\cdots,6=Sat$), workingday ($x_6$, binary: $0=weekend~or~holiday,1=otherwise$), weather situation ($x_7$, categorical with $3$ categories: $1 = (clear|few~clouds|partly~cloudy), 2=(cloudy|mist), 3=(rain|thunder|snow|freezing~fog)$), temp ($x_8$, numerical: temperature in Celsius), atemp ($x_9$, numerical: feeling temperature in Celsius), hum ($x_{10}$, numerical: humidity), and windspeed ($x_{11}$, numerical: wind speed). Several of the original covariates were dependent on others (e.g., season is a deterministic function of month) and so were excluded from the model. The covariates temp and atemp are very highly correlated, but we kept both in the model because we are using regularization. See Apley and Zhu (2016,2020) for an analysis of these data and graphical illustrations of the main effects and interaction effects of the various covariates via accumulated local effects plots. ??see my comments in the pdf about getting rid of atemp. Also, add a reference to the 2020 paper Apley, D. W. and Zhu, J., "Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models", Journal of the Royal Statistical Society, Series B, to appear; and also to the 2016 arxiv version??. 

We trained and validated various regression models on windows of data with various lengths. In each case, we used the data for the one year following the training data as the Phase-I data for establishing the control limits, and we used the remainder of the data following the Phase-I data as our Phase-II for the prospective concept drift detection. In all cases, we chose the EWMA parameter $ \lambda = 0.01$, which corresponds to an effective window length of $\frac{1}{\lambda}=100$ or a little over half a week. We standardized all numerical covariates and responses to have range close to $[0,1]$. In linear regression, since the model cannot capture the interactions between covariates, we add several interaction terms (interactions between month and hour, weather and month, weather and hour and so on) into the model and obtain a $5$-fold cross-validation $r^2\approx 0.87$, which is consistent between \texttt{R} package \texttt{glmnet} and \texttt{python} package \texttt{sklearn}. In the MLP model, since it inherently captures interaction, we only use those $11$ covariates and treated all categorical ones as numerical, because it would apply some smoothness constraint on the relationship between responses and those covariates, make the neural network simpler, and reduce overfitting. The $5$-fold cross-validation $r^2\approx 0.95$ is consistent between \texttt{R} package \texttt{nnet} and \texttt{python} package \texttt{tensorflow}.


Within the past decade, many bike-sharing businesses were under continuous and fast-paced expansion~(\cite{shaheen2012public}). This kind of gradual drift in the context can be a good example for testing the capability of detecting and diagnosing ``gradual concept drift" of our method. We apply both retrospective and prospective analysis to detect and understand the origin of concept drifts, and in turn develop a better model with those insights. Here, the models we used are the linear regression and MLP, with two hidden layers of $10$ and $5$ nodes, respectively.

From Figure~\ref{fig:bike_sharing}, the upper left plot shows that the training data from year $2012$ has strong non-stationarity, from both the MEWMA of the score function and EWMA of absolute residual, in year $2012-2013$ and $2015-2016$. The high metrics near the beginning and the end of training phase shows an ``U-shape", in contrast to a case of stationary training phase where metrics should randomly fluctuate around a constant value. This is because that the data gradually drifted during the time of training data and minimizing the loss function of training data would result in high error at both ends of the training period. According to the plot in Figure~\ref{fig:bs_tr_2012}, because the data in year $2012-2013$ introduce too much non-stationarity, after deleting them and training a model using data after year $2013$, we should expect the plot of metrics more stationary, which is indeed shown in Figure~\ref{fig:bs_tr_2013}. This better model should further give better testing performance. Indeed, we can see that the Hotelling $T^2$ and absolute residual are lower with the model trained using data after $2013$, even though the size of training data becomes smaller. More quantitatively, we found the testing $r^2$ (Phase-II) for models trained with data from $2012$ and $2013$ are $\approx0.74$ and $\approx0.81$, respectively. Even though, according to the analysis and plots above, the data has concept drift inherently and both models succeed in capturing that, the model trained on the data set with less non-stationarity would still has higher predicting power. This result is also observed in neural network model, but omitted here for space limitation. The reason behind this non-stationarity in the data will be discussed in details later.

\begin{figure}[!htbp]
\centering
\begin{subfigure}[t]{0.48\linewidth}
     \centering
     \begin{subfigure}[t]{\linewidth}
     \centering
         \includegraphics[width=\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_12_tr_5_new/train_bike_reg_1e-08_0_0001_0_01_99_99.png}
         \caption{Training phase starting from year $2012$.}
         \label{fig:bs_tr_2012}
     \end{subfigure}
     \begin{subfigure}[t]{\linewidth}
     \centering
         \includegraphics[width=\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_12_tr_5_new/bike_reg_1e-08_0_0001_0_01_99_99.png}
         \caption{Phase-I and Phase-II up to year $2018$.}
         \label{fig:bs_PIPII_tr_2012}
     \end{subfigure}
\end{subfigure}
\begin{subfigure}[t]{0.48\linewidth}
     \centering
     \begin{subfigure}[t]{\linewidth}
     \centering
        \includegraphics[width=\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_13_tr_5_new/train_bike_reg_1e-08_0_0001_0_01_99_99.png}
        \caption{Training phase starting from year $2013$.}
        \label{fig:bs_tr_2013}
     \end{subfigure}
     \begin{subfigure}[t]{\linewidth}
     \centering
        \includegraphics[width=\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_13_tr_5_new/bike_reg_1e-08_0_0001_0_01_99_99.png}
        \caption{Phase-I and Phase-II up to year $2018$.}
        \label{fig:bs_PIPII_tr_2013}
     \end{subfigure}
\end{subfigure}
\caption{
Control charts monitoring Hotelling $T^2$ of EWMA of the score function and EWMA of the absolute residual are compared using the bike sharing data set. The left column is from a model trained with data starting from year $2012$, while the right from year $2013$. The upper row is from training phase, and the lower from Phase-I and Phase-II. The training data starting from $2012$ show stronger non-stationarity than that starting from $2013$, likely because in year $2012-2013$($2012$ Aug.) the Captial Bikeshare program expanded to Alexandria City and the population affected by this expansion increases by $\approx45\%$(from $\approx550000$ to $\approx800000$). This strong movement of expansion very likely results in the change in how people participate in bike sharing everyday. The model trained on more stationary data has Phase-II testing $r^2\approx0.81$ which is much higher than $r^2\approx0.74$ calculated from less stationary data set, as the less degree of concept drift shown in the right column. Even though the data has concept drift inherently and both models succeed in capturing that, the model trained on data with less non-stationarity can still have higher predictive power. Also, by comparing the plots of the score function and absolute residual, we can see that the score-based method is more sensitive to concept drifts.
}
\label{fig:bike_sharing}
\end{figure}

% \begin{figure}[!htbp]
% \centering
% % \captionsetup[subfigure]{justification=top}
% \begin{subfigure}[c]{0.266\linewidth}
%      \centering
%      \begin{subfigure}[t]{\linewidth}
%      \centering
%          \includegraphics[width=0.8\textwidth, trim=.0in .4in .5in 1.2in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_10_tr_exp/PII_pos_single_retro_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
%      \end{subfigure}
%      % \hspace{3.0cm}
%      \captionsetup{width=.95\linewidth}
%      \caption{Retrospective analysis on the entire data set by training on middle part of data set.}
%      \label{fig:bs_retro_mid}
%      \end{subfigure}
% \begin{subfigure}[c]{0.357\linewidth}
%      \centering
%      \begin{subfigure}[t]{\linewidth}
%      \centering
%          \includegraphics[width=0.8\textwidth, trim=.0in .0in .0in 1.2in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_10_tr_3/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
%          \includegraphics[width=.8\textwidth, trim=12.5in 0.5in 3.0in 1.0in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_10_tr_3/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99_72.png}
%      \end{subfigure}
%      \captionsetup{width=.95\linewidth}
%      \caption{Diagnosis plots for a model trained using raw bike rental counts.}
%      \label{fig:bs_raw_cnt}
% \end{subfigure}
% \begin{subfigure}[c]{0.357\linewidth}
%      \centering
%      \begin{subfigure}[t]{\linewidth}
%      \centering
%         \includegraphics[width=0.8\textwidth, trim=.0in .0in .0in 1.2in, clip]{../figures/v14/bike_sharing/reg_lin_cat_norm_syr_10_pow_tr_3/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
%         \includegraphics[width=.8\textwidth, trim=12.5in 0.5in 3.0in 1.0in, clip]{../figures/v14/bike_sharing/reg_lin_cat_norm_syr_10_pow_tr_3/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99_72.png}
%      \end{subfigure}
%      \captionsetup{width=.95\linewidth}
%      \caption{Diagnosis plots for a model trained using normalized rental counts, by trailing yearly mean.}
%      \label{fig:bs_norm_cnt}
% \end{subfigure}
%   \caption{
%   (a) Retrospective analysis using a model trained upon data from around $2014$ to the end of $2016$. These plots show a very strong non-stationarity, and are more clear than Figure~\ref{fig:bs_retro}. The EWMA is much more stable in the middle period and shows stronger concept drift at both ends of period. (b)(c) MEWMA and EWMA control charts for linear regression of bike sharing data set (lines are in different colors in electronic version) for detecting and diagnosing concept drifts. The left column (b) is from a model trained on the raw bike rental count, while the right column (c) the normalized bike rental count. The upper row shows some representative lines of components, while the lower row includes more lines to overview the pattern of concept drifts. The results from raw bike rental count show strong concept drifts, while the concept drift is largely reduced in results from normalized bike rental count, because the change of hidden context is learned by the model. The Phase-II testing $r^2$ for the left and right column are $\approx0.26$ and $\approx0.85$, respectively.
% }
% \label{fig:bike_sharing_diag}
% \end{figure}


Clearly, the retrospective analysis above provides a deeper understanding in the data set, which likely helps us build a better model. Both Hotelling $T^2$ and absolute residuals can achieve this purpose. To further illustrate the retrospective analysis and demonstrate real usage of our method in applications, this time we use the entire data set. As in Figure~\ref{fig:bs_retro}, the MEWMA control chart for the score function shows strong non-stationarity as the data in the beginning and the end of the period has high monitoring statistics, presenting an ``U-shape". This is because the data set has a gradual concept drift over the entire period, and the trained model converges to a solution best fitting data from the middle period of time. To better show the pattern of non-stationarity for interpretation, we can choose the training data set based on the preliminary retrospective analysis in Figure~\ref{fig:bs_retro}. This is because when we train our model on the entire data set, there is no single model that can fit all the data very well and the solution may not be very meaningful. Instead, if we can restrict our data set within a period where data are approximately stationary, the model would be more meaningful and the pattern of concept drift would also be more obvious. As shown in the top plot of Figure~\ref{fig:bs_retro_mid}, we train a model using data from the beginning of year $2014$ to the end of year $2016$, during which data are more or less stationary (the choice of period is at users' discretion, as long as the data chosen is more stationary than the entire data set and the level of stationarity is acceptable), and use that model to calculate score vectors over the entire data set. The ``U-shape" we observed in Figure~\ref{fig:bs_retro} also shows up and is clearer. The cross-validation $r^2$ of the plot in Figure~\ref{fig:bs_retro} is $0.7928$ and the cross-validation $r^2$ for training data and $r^2$ for the entire data set in Figure~\ref{fig:bs_retro_mid} are $0.8776$ and $0.7590$, meaning the model trained on more stationary data can fit data better and be more sensitive to the concept drift in the entire data set. 

\begin{figure}[!htbp]
\centering
\begin{subfigure}[c]{0.54\linewidth}
     \centering
     \begin{subfigure}[t]{\linewidth}
     \centering
         \includegraphics[width=\textwidth, trim=0.0in 4.6in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_10_tr_10/train_bike_reg_1e-08_0_0001_0_01_99_99.png}
     \end{subfigure}
     % \hspace{3.0cm}
     \caption{MEWMA control chart.}
     \label{fig:bs_retro}
     \end{subfigure}
\begin{subfigure}[c]{0.45\linewidth}
     \centering
     \begin{subfigure}[t]{\linewidth}
     \centering
         \includegraphics[width=\textwidth, trim=0.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/plot_cnt.png}
     \end{subfigure}
     \caption{Hourly bike rental counts over time.}
     \label{fig:bs_cnt}
\end{subfigure}
  \caption{
(a) The MEWMA control charts of the score function of bike sharing data from retrospective analysis. The entire data set is used to train a model and the Hotelling $T^2$ of the score vectors are calculated and visualized. The plot shows non-stationarity of the data set. (b) The hourly bike rental counts over time.
}
\label{fig:bike_sharing_retro}
\end{figure}

\begin{figure}[!htbp]
    \centering
         \includegraphics[width=0.5\textwidth, trim=.0in .4in .4in 1.2in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_10_tr_exp/PII_pos_single_retro_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     % \hspace{3.0cm}
    %  \captionsetup{width=.95\linewidth}
     \caption{Retrospective analysis using a model trained upon data from around $2014$ to the end of $2016$. These plots show a very strong non-stationarity, and are more clear than Figure~\ref{fig:bs_retro}. The EWMA is much more stable in the middle period and shows stronger concept drift at both ends of period. This means training on a subset of data set which is more stationary would build a better model which in turn is more sensitive to concept drifts in the retrospective analysis over the entire data set.}
     \label{fig:bs_retro_mid}
\end{figure}

The control charts of components of score function show the directions in which corresponding coefficients should change to better fit the data in the local time window, providing more interpretation. As shown in the second row of Figure~\ref{fig:bs_retro_mid} for the predictor year, the deviation is strong in the beginning and the end, but with different signs. The model captures the slope of year in the middle period but slopes before and after that are larger and smaller, according to the bike rental count over time in Figure~\ref{fig:bs_cnt}. This observation is consistent with that in the last row of control chart for the intercept, where it is close to zero in the middle and negative at both ends. Because we centered the predictor year and all other covariates, the intercept should closely represent the mean of bike rental count in the middle period. By extrapolating from the middle to both ends in Figure~\ref{fig:bs_cnt} using the learned coefficient of year (slope), we can imagine that the data can be better fitted to with a smaller intercept at the both ends of training period to compensate for overestimate due to the linear extrapolation from the middle. Also in the third and the fourth rows of Figure~\ref{fig:bs_retro_mid}, the effects of month May and September show similar trend as that of year, but because month is treated as factor, during the time outside those months, the monitored statistics in control charts are close to zero.

As mentioned, the period of data chosen to train a model in retrospective analysis is not unique. We can also choose the beginning period from year $2010$ to the end of year $2013$. As shown in Figure~\ref{fig:bike_sharing_diag}, the Figure~\ref{fig:bs_raw_cnt} has plots of EWMA of components of the score function after transformation with Fisher information matrix as described in Section~\ref{s:decou_cd}. We see that all components have similar pattern of drifts, from which we can naturally infer that the concept drift probably results from overall increase of the popularity of bike sharing. We select several plots in Figure~\ref{fig:bike_sharing_diag} and put more EWMA of components together in Figure~\ref{fig:bike_sharing_diag_more} to give a bird's-eye view on this pattern. With this understanding in mind, we consider normalizing the raw bike rental count, $y$, with the trailing moving average of the hourly rental count over one year, $y_{MA}$. More specifically, we generate a normalized response, denoted as $\tilde{y}=y/y_{MA}^\beta$, where $\beta=0.78$ is a tuning parameter\footnote{This idea comes from the following observation. Assume a smooth version of response (bike rental count) over time as $f(t)$. A proper normalization should satisfy a constraint $\frac{f(t)}{\int_{t-1}^{t}f(\tau)d\tau}\approx const$ or $f'(t)\approx const*(f(t)-f(t-1))$. Obviously, a function of time oscillating around a constant value satisfies this. In order to obtain a solution with increasing trend without blowing up, we can see that a $f(t)$ with
monotonically decreasing $f'(t)$ may satisfy the constraint.} and found by maximizing testing $r^2$. The model trained on this normalized response has diagnosis plots as in Figure~\ref{fig:bs_norm_cnt}. And by comparing them with those in Figure~\ref{fig:bs_raw_cnt}, we see that the concept drift is largely reduced because the change of hidden context is learned after normalizing response. We obtained the Phase-II testing $r^2$ as $\approx0.26$ and $\approx0.85$ without and with this normalization, respectively.

\begin{figure}[!htbp]
\centering
    \begin{subfigure}[t]{0.49\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in 1.2in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_10_tr_3/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Diagnosis plots for a model trained using raw bike rental counts.}
     \label{fig:bs_raw_cnt}
\end{subfigure}
\begin{subfigure}[t]{0.49\linewidth}
     \centering
        \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in 1.2in, clip]{../figures/v14/bike_sharing/reg_lin_cat_norm_syr_10_pow_tr_3/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Diagnosis plots for a model trained using normalized rental counts, by trailing yearly mean.}
     \label{fig:bs_norm_cnt}
\end{subfigure}
\caption{
MEWMA and EWMA control charts for linear regression of bike sharing data set (lines are in different colors in electronic version) for detecting and diagnosing concept drifts. The left column (a) is from a model trained on the raw bike rental count, while the right column (b) the normalized bike rental count. Here we show some representative lines of components. The results from raw bike rental count show strong concept drifts increasing over time, while the concept drift is largely reduced in results from normalized bike rental count, because the change of hidden context is learned by the model. The Phase-II testing $r^2$ for the left and right column are $\approx0.26$ and $\approx0.85$, respectively.
}
\label{fig:bike_sharing_diag}
\end{figure}


\begin{figure}[!htbp]
    \begin{subfigure}[t]{1.0\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=12.5in 0.5in 3.0in 1.0in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_10_tr_3/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99_72.png}
    %  \captionsetup{width=.95\linewidth}
     \caption{Diagnosis plots for a model trained using raw bike rental counts.}
     \label{fig:bs_raw_cnt_more}
\end{subfigure}
\begin{subfigure}[t]{1.0\linewidth}
     \centering
        \includegraphics[width=1.0\textwidth, trim=12.5in 0.5in 3.0in 1.0in, clip]{../figures/v14/bike_sharing/reg_lin_cat_norm_syr_10_pow_tr_3/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99_72.png}
    %  \captionsetup{width=.95\linewidth}
     \caption{Diagnosis plots for a model trained using normalized rental counts, by trailing yearly mean.}
     \label{fig:bs_norm_cnt_more}
\end{subfigure}
  \caption{
  MEWMA and EWMA control charts for linear regression of bike sharing data set (lines are in different colors in electronic version) for detecting and diagnosing concept drifts. The upper figure (a) is from a model trained on the raw bike rental count, while the lower figure (b) the normalized bike rental count. Here, more plots of Figure~\ref{fig:bs_raw_cnt} are shown to give a overview of the pattern in diagnostic plots.
}
\label{fig:bike_sharing_diag_more}
\end{figure}


After normalizing response, the global concept drift has been learned by the model and is no longer significant in the top plot in Figure~\ref{fig:bs_norm_cnt}. However, as shown in the first plot in Figure~\ref{fig:bike_sharing_interp}, there are still some local concept drift (here, we use smaller EWMA parameter $\lambda=0.0005$, to average out more noise and make the local concept drift clearer).
To interpret it, we look deeper into the plots of training phase (retrospective analysis), the data set, and how those data are processed from original loggings of bike rentals. In the first plot of Figure~\ref{fig:bike_sharing_interp}, we plot the Hotelling $T^2$ of EWMA of the score function from the model trained using the normalized responses of the entire data set. There are three major peaks in the metrics, happening in periods $2012-2013$, $2016-2017$, and $2018-2020$. In the second plot, we include the intercept of the model, which shows some similar peaks. In the third plot, we show the number of jurisdictions nearby Washington Metropolitan Area in the Capital Bikeshare program over time, and in the bottom plot, the number of bikes available in the program. We see that those three peaks correspond to three periods when the program of Capital Bikeshare expanded to new jurisdictions and increased the number of bikes dramatically. Such fast expansion in a short period of time can temporarily introduce non-stationarity into the data set, probably because of factors of media exposures, psychology effects, and adaptation of habits of commuting.

\begin{figure}[!htbp]
\centering
\includegraphics[width=.5\textwidth, trim=.0in 4.58in .0in .0in, clip]{../figures/v14/bike_sharing/reg_nnet_numer_norm_syr_10_pow_tr_10/marked_train_bike_reg_1e-06_0_0001_0_0005_99_99.png}
\includegraphics[width = 0.53\linewidth, trim=.0in .in .0in 0.0in, clip]{../figures/v14/bike_sharing/reg_lin_cat_norm_syr_10_pow_tr_10/mark_training_pos_single_train_bike_fisher_mlines_with_regu_1e-08_0_0001_0_0005_99_99.png}
\includegraphics[width = 0.47\linewidth]{../figures/v14/bike_sharing/marked_plot_num_counties.png}\linebreak
\includegraphics[width = 0.5\linewidth]{../figures/v14/bike_sharing/marked_plot_num_bikes.png}
  \caption{
The MEWMA control charts of bike sharing data set with normalized response shows some local non-stationarity in training phase. The three major peaks are marked in the upper plot. In the second plot, the diagnostic plot of intercept of the model is drawn. In the third plot, the number of counties in the program of Capital Bikeshare over time is drawn, and the name of those counties are marked. In the bottom plot, the three periods where the number of bikes increases fast due to expansion of the program are also marked, corresponding to the three major peaks in the top plot.
}
\label{fig:bike_sharing_interp}
\end{figure}

\section{Conclusion}
Predictive models are often trained on historical data sets, but due to potential change of the conditional distribution of $P (Y|\bm {X})$ (a.k.a concept drift) the performance of those models may degrade. Here, we propose to monitor the score function of models using MEWMA control charts. The score-based method has several advantages, comparing against the error-based methods. First, concept drifts can result in no change in error, but it must change the mean of score function for parametric models, under fairly general conditions. Failing to detect those concept drifts using the error-based methods would miss potential opportunities to improve model performance, which can be resolved using the score-based methods. Second, the score-based method are shown to have smaller $MRL_1$ (out-of-control median run-length), given the same $MRL_0$ (in-control median run-length), comparing with other metrics, indicating that the score-based method is more sensitive to concept drifts. Third, the score-based method, applicable to any parametric model, provides a comprehensive framework to detect and diagnose concept drift, without incurring much extra computation. 

The advantages of this method manifest in perspectives of theory, computation, and practicality in real applications. The insights obtained can in turn help improve the model performance and thus mitigate the effect of concept drifts. For future works, we plan to extend this method to other predictive models with biased estimators and deal with interpretability for the MLP or deep neural network models.

\acks{}
%\acks{We would like to acknowledge support for this project
%from the National Science Foundation (NSF grant IIS-9988642)
%and the Multidisciplinary Research Program of the Department
%of Defense (MURI N00014-00-1-0637). }

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

%\newpage

%
%\vskip 0.2in
%\bibliography{sample} % For PC
\bibliography{sample.bib} % For mac

\vspace{1in}

\begin{appendix}
% \textbf{Appendix A}
% \label{app:sgd_ewma}
% The EWMA of the score function in definition (\ref{eqn:ewma}) should have the same mean as the mean of the score function, $E[\bm {z}_t]=E[\bm {s}_t]$, if $\bm {s}_t$ follows its stationary distribution. However, because here the score function $\bm {s}_t$ comes from SGD, so that it takes some time before the parameters of the score function converging to the true value. To rigorously argue that the $\lim _{t\to +\infty}\bm {z}_t=\bm{0}$, we need to incorporate the dynamics of SGD. We can expand the EWMA as following:
% \begin{align}
% \bm {z}_t = \alpha \sum _{i=1}^t (1- \alpha)^{t-i}\bm {s}_i + (1- \alpha)^t \bm {z}_0 
% \label{eqn:ewma_expa}
% \end{align}
% Take total expectation on both sides, we have:
% \begin{align}
% E[\bm {z}_t] = \alpha \sum	_{i=1}^t (1- \alpha) ^{t-i} E[\bm {s}_i] + (1- \alpha)^t E[\bm {z}_0]
% \label{eqn:exp_ewma_expa}
% \end{align}
% To argue that $\lim _{t\to +\infty}E[\bm {z}_t]=\bm{0}$, we need to argue that $\lim _{t\to +\infty} E[\bm {s}_i] =\bm {0}$. According to the assumptions in paper (\cite{bottou2018optimization}), with strong convexity on the expectation of log-likelihood function, $E[\ln f ( \bm { \theta}| (\bm {X}, Y))]$, we have:
% \begin{align}
% \frac{1}{2}c||\bm { \theta}_t - \bm { \theta}^*||_2^2 \leq E[\ln f ( \bm { \theta}_t| (\bm {X}, Y))]-E[\ln f ( \bm { \theta}^*| (\bm {X}, Y))] \leq \frac{ \nu}{ \gamma+t} 
% \end{align}
% where the constants, $ \nu$ and $ \gamma$, are according to Theorem 4.7. So we have $\lim _{t\to+\infty} \bm { \theta}_t = \bm { \theta}^*$, where the $ \bm { \theta}^*$ is the value when $E[\ln f ( \bm { \theta}| (\bm {X}, Y))]$ achieve maximum, that is, when $\bm { \theta}$ takes the true parameter value (according to KL-divergence). So we have the parameter sequence of $\{\bm { \theta}_t\}_t$ converge to the true value of $\bm { \theta}$. With continuity conditions on the score function, we have $\lim _{t\to +\infty} E[\bm {s}_i] =\bm {0}$, and thus $\lim _{t\to +\infty}E[\bm {z}_t]=\bm{0}$.


% \textbf{Appendix A}
\section{Simulations on Median Run-Length ($MRL$)}
\label{ss:simu_MRL}
To quantitatively show higher sensitivity of the score-based method, the Monte Carlo simulations are conducted for different models to calculate $MRL$. Data sets are generated according to linear, logistic, multinomial, and poisson regression model as data generating processes. Then, corresponding training models are fitted to those simulated data sets.

For a fair comparison, error rate, residual, and score-based metrics are used in the control charts with the same EWMA parameter $\lambda$, as defined in Section~\ref{ss:MEWMA}. In SPC, different sizes of mean drift have different optimal $ \lambda$'s, because of the trade-off of sensitivity in drift size and detection delay, as mentioned in Section~\ref{ss:MEWMA}. Here, we choose three drift sizes and $ \lambda$'s so that $MRL_0$'s (in-control $MRL$) equal to a given value. The shorter $MRL_1$ (out-of-control $MRL$) for a given $ \lambda$, the concept drift is detected earlier. Here, examples of the linear(Table~\ref{tab:lin_MRL}), logistic(Table~\ref{tab:logi_MRL}), multinomial(Table~\ref{tab:multi_logi_MRL}), and poisson(Table~\ref{tab:pois_MRL}) regressions are tested. The neural network models (Table~\ref{tab:lin_nnet_MRL},~\ref{tab:logi_nnet_MRL},~\ref{tab:multi_logi_nnet_MRL}) are also applied on those data sets to see if the score-based method can also be used in highly nonlinear models. In those tables, $ \alpha$ is the changing factor that represents the size of concept drift or changes of parameters (larger $ \alpha$ means larger concept drift). As shown in results, the score-based method has smaller $MLR_1$, which indicates higher sensitivity. The gap between $MRL_1$ of the score-based metric and other metrics is larger when concept drift size is smaller, meaning that the detection becomes more challenging. This means the score-based method shows stronger advantage in detecting smaller drift size. Overall, those simulations prove the generality and superior capability of the score-based method.

\begin{table}[H]
\centering
\begin{tabular}{ccccc}
\toprule
\multicolumn{2}{c}{($ \alpha$, $ \lambda$)} & {$  \lambda_1$} & {$ \lambda_2$} & {$ \lambda_3$} \\
\midrule
\multirow{3}{*}{$\alpha = 0$} & score &$3500.0$ & $3499.0$ & $3502.5$ \\
& err &$3499.5$ & $3501.0$ & $3500.0$ \\
%& dev &$3499.5$ & $3499.5$ & $3501.0$ \\
\midrule
\multirow{3}{*}{$\alpha = 0.3$} & score &$\bm{391.0}$ & $\bm{377.5}$ & $\bm{464.0}$ \\
& err &$835.0$ & $1050.0$ & $1278.0$ \\
%& dev &$575.0$ & $633.0$ & $814.0$ \\
\midrule
\multirow{3}{*}{$\alpha = 0.5$} & score &$\bm{184.0}$ & $\bm{148.0}$ & $\bm{153.0}$ \\
& err &$359.0$ & $360.5$ & $442.0$ \\
%& dev &$244.0$ & $218.0$ & $239.0$ \\
\midrule
\multirow{3}{*}{$\alpha = 0.7$} & score &$\bm{111.0}$ & $\bm{82.0}$ & $\bm{75.0}$ \\
& err &$205.0$ & $177.0$ & $186.0$ \\
%& dev &$134.0$ & $108.0$ & $105.0$ \\
\midrule
\end{tabular}
\caption{Comparison of Phase-II $MRL$'s of the logistic regression using the score and classification error, with $10000$ simulations. The $MRL_0$'s (in-control median run lengths) are matched as close as possible. $ \lambda$ is the EWMA parameter ({$ \lambda_1 = 0.0028369$}, {$ \lambda_2 = 0.0087330$}, {$ \lambda_3 = 0.018546$}), and $ \alpha$ is the changing factor ($ \alpha=0$ corresponds to the in-control case).}
\label{tab:logi_MRL}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{ccccc}
\toprule
\multicolumn{2}{c}{($ \alpha$, $ \lambda$)} & $ \lambda_1$ & $ \lambda_2$ & $ \lambda_3$ \\
\midrule
\multirow{3}{*}{$\alpha=0$} & score &$5500.0$ & $5498.5$ & $5499.5$ \\
& err &$5499.0$ & $5499.5$ & $5498.5$ \\
%& dev &$5502.5$ & $5498.5$ & $5498.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.5$} & score &$\bm{246.0}$ & $\bm{260.0}$ & $\bm{302.0}$ \\
& err &$929.0$ & $1164.0$ & $1444.5$ \\
%& dev &$557.0$ & $649.0$ & $807.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.7$} & score &$\bm{149.0}$ & $\bm{141.0}$ & $\bm{148.0}$ \\
& err &$482.0$ & $567.0$ & $694.0$ \\
%& dev &$285.0$ & $300.0$ & $342.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.9$} & score &$\bm{110.0}$ & $\bm{98.0}$ & $\bm{97.0}$ \\
& err &$316.0$ & $345.0$ & $406.0$ \\
%& dev &$182.0$ & $176.0$ & $184.0$ \\
\midrule
\end{tabular}
\caption{Comparison of Phase-II $MRL$'s of multinomial regression using the score and classification error, with $10000$ simulations. The $MRL_0$'s (in-control median run lengths) are matched as close as possible. $ \lambda$ is the EWMA parameter ({$ \lambda_1 = 0.0060052$}, {$ \lambda_2 = 0.010923$}, {$ \lambda_3 = 0.016641$}), and $ \alpha$ is the changing factor ($ \alpha=0$ corresponds to the in-control case).}
\label{tab:multi_logi_MRL}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{ccccc}
\toprule
\multicolumn{2}{c}{($ \alpha$, $ \lambda$)} & {$ \lambda_1$} & {$ \lambda_2$} & {$ \lambda_3$} \\
\midrule
\multirow{3}{*}{$ \alpha=0$} & score &$1199.5$ & $1200.5$ & $1200.0$ \\
& abs\_resi &$1200.0$ & $1199.5$ & $1200.0$ \\
%& dev &$1199.5$ & $1199.5$ & $1199.5$ \\
\midrule
\multirow{3}{*}{$\alpha=0.1$} & score &$\bm{247.0}$ & $\bm{339.0}$ & $\bm{516.0}$ \\
& abs\_resi &$898.5$ & $1003.5$ & $1023.5$ \\
%& dev &$866.5$ & $986.0$ & $989.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.3$} & score &$\bm{69.0}$ & $\bm{46.0}$ & $\bm{52.0}$ \\
& abs\_resi &$127.0$ & $111.0$ & $135.0$ \\
%& dev &$113.0$ & $100.0$ & $127.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.5$} & score &$\bm{38.0}$ & $\bm{20.5}$ & $\bm{19.0}$ \\
& abs\_resi &$48.0$ & $30.0$ & $27.0$ \\
%& dev &$38.0$ & $26.0$ & $25.0$ \\
\midrule
\end{tabular}
\caption{Comparison of Phase-II $MRL$'s of the linear regression using the score and absolute residual, with $10000$ simulations. The $MRL_0$'s (in-control median run lengths) are matched as close as possible. $ \lambda$ is the EWMA parameter ( {$ \lambda_1 = 0.0038876$}, {$ \lambda_2 = 0.028477$}, {$ \lambda_3 =0.065955$}), and $ \alpha$ is the changing factor ($ \alpha=0$ corresponds to the in-control case).}
\label{tab:lin_MRL}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{ccccc}
\toprule
\multicolumn{2}{c}{($ \alpha$, $ \lambda$)} & {$ \lambda_1$} & {$ \lambda_2$} & {$ \lambda_3$} \\
\midrule
\multirow{3}{*}{$\alpha=0$} & score &$6495.5$ & $6506.0$ & $6506.5$ \\
& abs\_resi &$6501.0$ & $6500.0$ & $6502.0$ \\
%& dev &$6501.0$ & $6498.0$ & $6507.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.3$} & score &$\bm{321.0}$ & $\bm{381.0}$ & $\bm{460.0}$ \\
& abs\_resi &$1661.5$ & $1772.0$ & $1807.5$ \\
%& dev &$3146.0$ & $3474.5$ & $3659.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.5$} & score &$\bm{171.0}$ & $\bm{176.5}$ & $\bm{195.0}$ \\
& abs\_resi &$453.0$ & $475.5$ & $508.0$ \\
%& dev &$718.0$ & $794.0$ & $875.5$ \\
\midrule
\multirow{3}{*}{$\alpha=0.7$} & score &$\bm{119.0}$ & $\bm{116.0}$ & $\bm{123.0}$ \\
& abs\_resi &$233.0$ & $228.0$ & $232.0$ \\
%& dev &$290.0$ & $286.0$ & $295.0$ \\
\midrule
\end{tabular}
\caption{Comparison of Phase-II $MRL$'s of poisson regression using the score and absolute residual, with $10000$ simulations. The $MRL_0$'s (in-control median run lengths) are matched as close as possible. $ \lambda$ is the EWMA parameter ({$ \lambda_1=0.0050701$} , {$ \lambda_2=0.0090466$}, {$ \lambda_3=0.013069$}), and $\alpha$ is the changing factor ($ \alpha=0$ corresponds to the in-control case).}
\label{tab:pois_MRL}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ccccc}
\toprule
\multicolumn{2}{c}{($ \alpha$, $ \lambda$)} & {$ \lambda_1$} & {$ \lambda_2$} & {$ \lambda_3$} \\
\midrule
\multirow{3}{*}{$\alpha=0$} & score &$3494.0$ & $3505.5$ & $3494.5$ \\
& err &$3503.5$ & $3508.0$ & $3521.5$ \\
%& dev &$3500.5$ & $3508.5$ & $3507.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.3$} & score &$\bm{476.0}$ & $\bm{390.0}$ & $\bm{385.0}$ \\
& err &$800.5$ & $834.5$ & $1064.0$ \\
%& dev &$616.5$ & $576.5$ & $645.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.5$} & score &$\bm{252.5}$ & $\bm{177.0}$ & $\bm{153.5}$ \\
& err &$403.0$ & $346.5$ & $349.5$ \\
%& dev &$288.0$ & $230.0$ & $218.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.7$} & score &$\bm{159.0}$ & $\bm{110.0}$ & $\bm{84.0}$ \\
& err &$251.0$ & $198.0$ & $177.0$ \\
%& dev &$173.0$ & $136.0$ & $112.0$ \\
\midrule
\end{tabular}
\caption{Comparison of Phase-II $MRL$'s of neural network ($1$ hidden layer with $10$ nodes) for logistic binary classification data using the score and classification error, with $1000$ simulations. The $MRL_0$'s (in-control median) run lengths are matched as close as possible. $ \lambda$ is the EWMA parameter ({$ \lambda_1=0.0010545$}, {$ \lambda_2=0.0035807$}, {$ \lambda_3=0.0082376$}), and $ \alpha$ is the changing factor ($ \alpha=0$ corresponds to the in-control case).}
\label{tab:logi_nnet_MRL}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{ccccc}
\toprule
\multicolumn{2}{c}{($ \alpha$, $ \lambda$)} & {$ \lambda_1$} & {$ \lambda_2$} & {$ \lambda_3$} \\
\midrule
\multirow{3}{*}{$\alpha=0$} & score &$5519.5$ & $5470.5$ & $5517.0$ \\
& err &$5503.5$ & $5491.5$ & $5509.5$ \\
%& dev &$5495.0$ & $5494.0$ & $5474.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.5$} & score &$ \bm{341.0}$ & $\bm{434.0}$ & $\bm{553.0}$ \\
& err &$955.0$ & $1164.5$ & $1432.5$ \\
%& dev &$626.5$ & $721.0$ & $880.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.7$} & score &$\bm{202.0}$ & $\bm{208.0}$ & $\bm{249.0}$ \\
& err &$507.0$ & $569.5$ & $711.0$ \\
%& dev &$336.5$ & $347.0$ & $375.5$ \\
\midrule
\multirow{3}{*}{$\alpha=0.9$} & score &$\bm{136.0}$ & $\bm{132.0}$ & $\bm{140.0}$ \\
& err &$313.0$ & $325.5$ & $376.5$ \\
%& dev &$205.5$ & $197.0$ & $202.5$ \\
\midrule
\end{tabular}
\caption{Comparison of Phase-II $MRL$'s (median run lengths) of neural network ($1$ hidden layer with $10$ nodes) for multinomial regression data using the score and classification error, with $1000$ simulations. The $MRL_0$'s (in-control median run lengths) are matched as close as possible. $ \lambda$ is the EWMA parameter ({$ \lambda_1 =0.0052546$}, {$ \lambda_2=0.0092416$}, {$ \lambda_3 =0.014028$}), and $ \alpha$ is the changing factor ($ \alpha=0$ corresponds to the in-control case).}
\label{tab:multi_logi_nnet_MRL}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{ccccc}
\toprule
\multicolumn{2}{c}{($ \alpha$, $ \lambda$)} & {$ \lambda_1$} & {$ \lambda_2$} & {$ \lambda_3$} \\
\midrule
\multirow{3}{*}{$\alpha=0$} & score &$1192.5$ & $1202.0$ & $1199.5$ \\
& abs\_resi &$1200.0$ & $1198.5$ & $1201.5$ \\
%& dev &$1199.0$ & $1200.0$ & $1199.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.1$} & score &$\bm {518.5}$ & $\bm{1031.5}$ & $1429.5$ \\
& abs\_resi &$1313.5$ & $1431.5$ & $\bm{1412.0}$ \\
%& dev &$1635.0$ & $1911.5$ & $1998.5$ \\
\midrule
\multirow{3}{*}{$\alpha=0.3$} & score &$\bm{134.0}$ & $\bm{122.5}$ & $\bm{225.0}$ \\
& abs\_resi &$223.0$ & $218.5$ & $274.0$ \\
%& dev &$297.0$ & $487.0$ & $1655.5$ \\
\midrule
\multirow{3}{*}{$\alpha=0.5$} & score &$\bm{66.0}$ & $\bm{46.0}$ & ${52.0}$ \\
& abs\_resi &$76.0$ & $56.0$ & $52.0$ \\
%& dev &$84.0$ & $72.0$ & $92.5$ \\
\midrule
\end{tabular}
\caption{Comparison of Phase-II $MRL$'s (median run lengths) of neural network ($1$ hidden layer with $10$ nodes) for linear regression data using the score and absolute residual, with $1000$ simulations. The $MRL_0$' (in-control median run lengths) are matched as close as possible. $ \lambda$ is the EWMA parameter ({$ \lambda_1=0.0021882$}, {$ \lambda_2=0.012062$}, {$ \lambda_3=0.027374$}), and $ \alpha$ is the changing factor ($ \alpha=0$ corresponds to the in-control case).}
\label{tab:lin_nnet_MRL}
\end{table}


\section{Concept Drift Diagnoses}
\label{ss:cd_diag}
To demonstrate diagnosis capability of the score-based method, we simulate various kinds of data sets for regression and classification. We also generate data sets with abrupt and gradual concept drifts to mimic more cases in real applications. Examples are presented from simple to complex ones, to help build up intuitions for the score-based method.

\subsection{Simulated data set for Linear Regression}
\label{sss:lin_exp}
% Change nugget parameter to psudo-inverse.
\begin{enumerate}[(I)]
\item
\textbf{Abrupt Concept Drifts with Independent Covariates}
\label{ssss:lin_ind_pred}

\begin{figure}[!hpt]
\centering
  \includegraphics[width = 0.6\linewidth]{../figures/v14/sim_2/reg/neg_single_1_sim2_mlines_with_regu_1e-08_0_005.png}
  \caption{Abrupt concept drift of the linear model with independent covariates (lines are in different colors in electronic version). For conciseness, here only show MEWMA control chart of Hotelling $T^2$ (black) and EWMA control charts for the $1$st (blue), $5$th (red), $7$th (green), intercept (cyan), from the top to the bottom. The blue (first) and green (second) vertical lines mark the boundaries of Phase-I/II and before/after concept drift. The lines of monitored statistics in control charts have the same style and color with lines of their control limits correspondingly.}
  \label{fig:lin_reg_ind_X}
\end{figure}
In this simulation, the data set is generated by a {linear} model $y = \bm {x}^T\bm { \theta} + \epsilon$. The $10$ {covariates} {$\bm {x}=[x_1, x_2, \cdots, x _{10}]$} are independently distributed as Gaussian with mean $0$ and variance $1$; the variance of random error $ \epsilon$ is $1$; the EWMA parameter is $\lambda=0.027562$ chosen by ensuring around $50\%$ signal ratio of absolute residual after concept drift; there is no regularization in training the model because the number of parameters is small comparing to the sample size; the alarm rate for calculating the control limits is $99.9\%$; the sizes of the data set used in training, validation, Phase-I, and Phase-II are $10000$, $2000$, $20000$, and $40000$. The responses in training, validation, and Phase-I are generated with coefficients all equal to $1$. In the first half of Phase-II, the coefficients are unchanged, but in the second half, the first four coefficients are multiplied by $1- \alpha=0.7$, where $ \alpha=0.3$ is defined as the changing factor. Other coefficients are unchanged.
\begin{figure}[!htp]
\centering
\includegraphics[width = 0.6\linewidth]{../figures/v14/sim_2/reg/1_sim2_lin_1e-08_0_005_1.png}
  \caption{Abrupt concept drift of the linear model with independent covariates. Hotelling $T^2$ of EWMA of the score function and EWMA of the absolute residual are compared.}
  \label{fig:lin_reg_ind_X_comp}
\end{figure}

Figure~\ref{fig:lin_reg_ind_X} shows Phase-I and Phase-II of monitoring Hotelling $T^2$ and individual components of the score function. Before the blue (first) vertical lines (Phase-I), all lines of monitored statistics are well mixed in their range indicating no concept drift in training and Phase-I. After blue vertical lines (Phase-II), the EWMA lines corresponding to the first four coefficients have significant drifts (only show the first one here) and the change of mean is $0.3$ corresponding to the mean drift of coefficients, contributing to the entire line of MEWMA Hotelling $T^2$ (the black line in the top plot). In this example, all covariates are independent with others, so that there is not coupling, meaning only coefficients with concept drifts would have mean drifts. For brevity, only representative lines are shown. Lines of the rest of coefficients (including intercept) do not have mean drift but have larger deviation (or variance) after the concept drift, for the reason mentioned in Section~\ref{ss:var_infla}. The increase of deviation in control charts of those unchanged parameters results from the drifts ($\Delta \bm { \theta}$) coupling with variation of covariates ($X_k\bm {X}^T$).

To compare Hotelling $T^2$ of EWMA of the score function and EWMA of the absolute residual, we plot it with the score function respectively as shown in Figure~\ref{fig:lin_reg_ind_X_comp}. Both methods can capture the abrupt concept drift pretty well: in Phase-I they are all mixed well and in Phase-II mean drifts are close to each other. Here, we also see inflated variance of lines after the concept drift, even though they plateau very quickly. Both metrics capture the concept drift quite well, but the score-based method has higher signal ratio after concept drift, indicating that its superior performance in detecting small size changes, which is consistent with the results from Appendix~\ref{ss:simu_MRL}. Later, we will show results of gradual concept drift.

\item
\textbf{Abrupt Concept Drifts with Correlated Covariates}

\label{ssss:lin_not_ind_pred}
\begin{figure}[!htbp]
\centering
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_4/reg/PII_neg_single_1_sim4_mlines_with_regu_1e-08_0_005.png}
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_4/reg/PII_neg_single_1_sim4_fisher_mlines_with_regu_1e-08_0_005.png}
  \caption{Abrupt concept drift of the linear model with correlated {covariates} (lines are in different colors in electronic version). Comparison are made between before (left) and after (right) being transformed by {the inverse of the estimated} Fisher Information Matrix, {$\mathbf {I} ( {\bm{\theta}} ^{(0)})$.} For legibility, here only show MEWMA control chart of Hotelling $T^2$ (black) and EWMA control charts for the $1$st (blue), $5$th (red), $7$th (green), intercept (cyan), from the top to the bottom. The lines of monitored statistics in control charts have the same style and color with lines of their control limits correspondingly.}
  \label{fig:lin_reg_not_ind_X}
\end{figure}
Then, we manually introduce correlation between {covariates} by substituting $0.5 x_1 + 0.5 x_5$ and $ 0.3 x_1 + 0.3 x_2 + 0.4 x_6$ for $x_5$ and $x_6$ respectively, with others remaining the same. The EWMA and MEWMA control charts before (left) and after (right) transformed by {the inverse of the estimated} ${\mathbf {I}}(\bm { \theta}^{(0)})$ is as shown in Figure~\ref{fig:lin_reg_not_ind_X}. The blue (the $2$nd row) and green lines (the $4$th row) are for $x_1$ and $x_7$: transforming or not has no effect on those lines, where the coefficient for $x_1$ has mean {drift} but $x_7$ does not. However, according to the red line (the $3$rd row) for $x_5$, transformation changes it from having mean {drift} to not, indicating that this transformation successfully decouples concept drift from correlated covariates. This is not surprising, because ${\mathbf {I}} ^{-1}(\bm { \theta}^{(0)})$ equals to $E ^{-1} [\bm {X}\bm {X}^T]$, which does not depend on $ \bm { \theta} ^{(0)}$ or $ \bm { \theta} ^{(1)}$, meaning high-ordered term in Equation~(\ref{eqn:cd_decomp_fisher_approx}) is zero. However, in the next experiment for logistic regression, the concept drift is harder to decouple because multiplying ${\mathbf {I}} ^{-1}(\bm { \theta}^{(0)})$ can only approximately decouple concept drifts.

\item
\textbf{Gradual Concept Drifts with Correlated Covariates}
\label{ssss:lin_not_ind_pred_grad_cd}
\begin{figure}[!htbp]
\centering
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_6/reg/PII_neg_single_1_sim6_mlines_with_regu_1e-08_0_005.png}
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_6/reg/PII_neg_single_1_sim6_fisher_mlines_with_regu_1e-08_0_005.png}
 \caption{Gradual concept drift of the linear model with correlated {covariates} (lines are in different colors in electronic version). Comparison are made between before (left) and after (right) being transformed by {the inverse of the estimated} Fisher Information Matrix, {$\mathbf {I} ( {\bm{\theta}} ^{(0)})$.} For legibility, here only show MEWMA control chart of Hotelling $T^2$ (black) and EWMA control charts for the $1$st (blue), $5$th (red), $7$th (green), intercept (cyan), from the top to the bottom. The lines of monitored statistics in control charts have the same style and color with lines of their control limits correspondingly.}
  \label{fig:lin_reg_not_ind_X_grad_cd}
\end{figure}

In real applications, many concept drifts happen in a gradual way. In this data set, all conditions are the same with that in Experiment~\ref{ssss:lin_not_ind_pred} above except that the concept drifts happen linearly in the second half time of Phase-II. The starting and ending coefficients of the linear concept drift period correspond to those before and after concept drifts in the abrupt case. 
\begin{figure}[!htbp]
\centering
\includegraphics[width = 0.6\linewidth]{../figures/v14/sim_6/reg/1_sim6_lin_1e-08_0_005_1.png}
  \caption{Gradual concept drift of the linear model with correlated covariates. Hotelling $T^2$ of EWMA of the score function and EWMA of the absolute residual are compared.}
  \label{fig:lin_reg_ind_X_grad_cd_comp}
\end{figure}

Shown in Figure~\ref{fig:lin_reg_not_ind_X_grad_cd} are the EWMA and MEWMA control charts before (left) and after (right) transformed by {the inverse of the estimated} ${\mathbf {I}}(\bm { \theta}^{(0)})$. Similarly to that in Experiment~\ref{ssss:lin_not_ind_pred} above, lines corresponding to covariates with true concept drifts or independent with others are not affected by transformation in terms whether concept drifts show up in control charts. In Figure~\ref{fig:lin_reg_ind_X_grad_cd_comp}, we see that monitoring the score vectors gives an earlier detection of the starting position of the concept drift than EWMA of the absolute residual.
\end{enumerate}

\subsection{Simulated data set for Logistic Regression}
\label{sss:logi_exp}
\begin{enumerate}[(I)]
\item
\textbf{Abrupt Concept Drifts with Independent Covariates}
\label{ssss:log_ind_pred}
\begin{figure}[!htbp]
\centering
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_5/logi_no_muco/PII_pos_single_1_sim5_mlines_with_regu_1e-08_0_005.png}
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_5/logi_no_muco/PII_pos_single_1_sim5_fisher_mlines_with_regu_1e-08_0_005.png}
  \caption{Abrupt concept drift of the logistic regression with independent {covariates} (lines are in different colors in electronic version). Comparison are made between before (left) and after (right) being transformed by {the inverse of the estimated} Fisher Information Matrix, {$\mathbf {I} ( {\bm{\theta}} ^{(0)})$.} For legibility, here only show MEWMA control chart of Hotelling $T^2$ (black) and EWMA control charts for the $1$st (blue), $5$th (red), $7$th (green), intercept (cyan), from the top to the bottom. The lines of monitored statistics in control charts have the same style and color with lines of their control limits correspondingly.}
  \label{fig:log_reg_ind_X}
\end{figure}

This simulated data set has the same parameter as that in the corresponding Experiment~\ref{ssss:lin_ind_pred} in Appendix~\ref{sss:lin_exp}, except that here the model becomes the {logistic model} {$p(y=1|\bm {x})= \sigma (\bm {x}^T\bm { \theta})$,} where $\sigma (\cdot)$ is the sigmoid function.
\begin{figure}[!htbp]
\centering
\includegraphics[width = 0.6\linewidth]{../figures/v14/sim_5/logi_no_muco/1_sim5_logi_1e-08_0_005_1.png}
  \caption{Abrupt concept drift of the logistic regression with independent covariates. Hotelling $T^2$ of EWMA of the score function and EWMA of the error rate are compared.}
  \label{fig:log_reg_ind_X_comp}
\end{figure}

As shown in Figure~\ref{fig:log_reg_ind_X}, even though components of $\bm {x}$ are independent, the concept drifts of the first four components still propagate to the rest (with a smaller magnitude) which do not have drifts, except the intercept (because of the symmetry of distribution of our simulated $\bm {x}$). This is because that $E [ (\sigma ( \bm {X}^T\bm { \theta}^{(1)} ) - \sigma ( \bm {X}^T\bm { \theta}^{(0)} )) \bm {X}] $ is nonlinear and depends on parameters. This is supported by the comparison of the left and right column in Figure~\ref{fig:log_reg_ind_X}: applying transformation matrix cannot recover zero mean drift for those covariates without concept drift (the red line in the $3$rd row for $x_5$) meaning this coupling is due to the nonlinearity rather than correlation between covariates in Experiment~\ref{ssss:lin_not_ind_pred} in Appendix~\ref{sss:lin_exp}. During experiments, we find an interesting special case that if the concept drift is restricted to flipping sign of coefficients, the covariates without concept drift will not have mean {drift} (no coupling effect) in those control charts, due to the same reason that the intercept has no mean drift, mentioned above.

As the comparison in Experiment~\ref{ssss:lin_ind_pred} in Appendix~\ref{sss:lin_exp}, in Figure~\ref{fig:log_reg_ind_X_comp}, Hotelling $T^2$ of EWMA of the score function and EWMA of the prediction error, both methods can capture the abrupt concept drift, but the EWMA of the prediction error has much less signal ratio. This is because in Phase-I the prediction error has a wider range between control limits than that of the score function, so that in Phase-II the control chart is less sensitive to the deviation.

\item
\textbf{Abrupt Concept Drifts with Correlated Covariates}
\label{ssss:log_not_ind_pred}

\begin{figure}[!htbp]
\centering
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_5/logi/PII_pos_single_1_sim5_mlines_with_regu_1e-08_0_005.png}
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_5/logi/PII_pos_single_1_sim5_fisher_mlines_with_regu_1e-08_0_005.png}
  \caption{Abrupt concept drift of the logistic regression with correlated {covariates} (lines are in different colors in electronic version). Comparison are made between before (left) and after (right) being transformed by {the inverse of the estimated} Fisher Information Matrix, {$\mathbf {I} ( {\bm{\theta}} ^{(0)})$.} For legibility, here only show MEWMA control chart of Hotelling $T^2$ (black) and EWMA control charts for the $1$st (blue), $5$th (red), $7$th (green), intercept (cyan), from the top to the bottom. The lines of monitored statistics in control charts have the same style and color with lines of their control limits correspondingly.
}
  \label{fig:log_reg_not_ind_X}
\end{figure}

As in Experiment~\ref{ssss:lin_not_ind_pred} above, we introduce correlation between {covariates.} The comparison between before and after transformation with {the inverse of the estimated} {$\mathbf {I} ( {\bm{\theta}} ^{ (0)}) = E [{p} (\bm {X},\bm { \theta} ^{ (0)}) (1-{p}(\bm {X},\bm { \theta} ^{ (0)})) \bm {X} \bm {X}^T] \Delta \bm{ \theta}$}, where {$p (\bm {X},\bm { \theta} ^{ (0)}) = \sigma ( \bm {X}^T\bm { \theta} ^{ (0)})$}, is shown in Figure~\ref{fig:log_reg_not_ind_X}. The transformation makes the mean {drift} uniformly significant on control charts over all {covariates,} no matter whether they have concept drift (here only show representative lines). For the $x_7$ component (green in the $4$th row), previously mild concept drift due to nonlinearity becomes even larger (worse) after transformation. This indicates that the approximation in Equation (\ref{eqn:cd_decomp_fisher_approx}) is not accurate. This makes sense, because the concept drift is large (coefficients from $1$ to $-1$), violating the assumption of ``small higher-ordered terms of concept drift".

\item
\textbf{Concept Drifts with Correlated Covariates and an Assumption}
\label{ssss:log_not_ind_pred_assum}
\begin{figure}[!htbp]
\centering
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_5/logi_small/PII_pos_single_1_sim5_mlines_with_regu_1e-08_0_005.png}
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_5/logi_small/PII_pos_single_1_sim5_fisher_mlines_with_regu_1e-08_0_005.png}
  \caption{
 Abrupt concept drift of the logistic regression with correlated covariates (lines are in different colors in electronic version). Simulated data are modified to reduce the magnitude of $ \bm {x}_i^T\bm { \theta}^{(1)}$ and $  \bm {x}_i^T\bm { \theta}^{(0)}$. Comparison are made between before (left) and after (right) being transformed by the inverse of the estimated Fisher Information Matrix, $\mathbf {I} ( {\bm{\theta}} ^{(0)})$. For legibility, here only show MEWMA control chart of Hotelling $T^2$ (black) and EWMA control charts for the $1$st (blue), $5$th (red), $7$th (green), intercept (cyan), from the top to the bottom. The lines of monitored statistics in control charts have the same style and color with lines of their control limits correspondingly.
}
  \label{fig:log_reg_not_ind_X_1}
\end{figure}

To resolve the failure of diagnosing true concept drifts in Experiment~\ref{ssss:log_not_ind_pred}, we observe that, according to the mean drift of the logistic regression $E [ (\sigma (\bm {X}^T\bm { \theta}^{ (1)}) - \sigma ( \bm {X}^T\bm { \theta}^{ (0)})) \bm {X}] $, if $ \bm {x}^T\bm { \theta}^{ (1)}$ and $ \bm {x}^T\bm { \theta}^{ (0)}$ are all relative small (say $\ll 4$ in absolute value), the sigmoid function $ \sigma (\cdot)$ are in the linear region, so that we can approximate the mean drift by $E [ (\sigma ( \bm {X}^T\bm { \theta}^{ (1)} ) - \sigma ( \bm {X}^T\bm { \theta}^{ (0)} )) \bm {X}] \approx E [ \eta( \bm {X}^T\bm { \theta}^{ (1)} -  \bm {X}^T\bm { \theta}^{ (0)} ) \bm {X}] = \eta E[\bm{XX}^T] \Delta \bm { \theta} $, where $ \eta$ is just a scalar constant factor, reducing to the case of the linear model, as in Appendix~\ref{sss:lin_exp}.  

To test this argument, we follow the data set generating process in Experiment~\ref{ssss:log_not_ind_pred} in Appendix~\ref{sss:lin_exp} but make modifications as below: the coefficients of all covariates in training, validation, Phase-I, and the first half period of Phase-II changes to $0.2$; in the second half period of Phase-II, the coefficients of the first four covariates are multiplied by $-1$. In Figure~\ref{fig:log_reg_not_ind_X_1}, we observe that, transforming or not does not change the lines corresponding to covariates which are independent with others or truly have concept drifts in terms of whether to show concept drifts in control charts (the coefficient for $x_1$ (blue in the $2$nd row) has mean drift but $x_7$ (green in the $4$th row) and intercept (cyan in the last row) does not); but transformation changes the line $x_5$ (red in the $3$rd row) in the control charts from having mean drift to not. In other words, the concept drifts are successfully decoupled. Due to the reduced variance of covariates and smaller concept drifts, the variance inflation after concept drifts is also mitigated, as shown in the second half of Phase-II. Even though this requires some assumption, this property of the logistic regression is still practically valuable, because when arguments of the sigmoid function have a large absolute value, large concept drifts may not have much impact on the performance of predictive models (the sigmoid function is in the plateau regions), while when the sigmoid function is in the linear region, small drifts in coefficients can significantly change predictions, so that we might want to understand which covariates contribute to such change.

This property of the logistic regression can be extended to gradual concept drifts. Modifying the above data set as done in Experiment~\ref{ssss:lin_not_ind_pred_grad_cd} in Appendix~\ref{sss:lin_exp}, meaning change the abrupt concept drift to the linear one.
\begin{figure}[!htbp]
\centering
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_7/logi_small/PII_pos_single_1_sim7_mlines_with_regu_1e-08_0_005.png}
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_7/logi_small/PII_pos_single_1_sim7_fisher_mlines_with_regu_1e-08_0_005.png}
 \caption{Gradual concept drift of the logistic regression with correlated {covariates} (lines are in different colors in electronic version). Comparison are made between before (left) and after (right) being transformed by {the inverse of the estimated} Fisher Information Matrix, {$\mathbf {I} ( {\bm{\theta}} ^{(0)})$.} For legibility, here only show MEWMA control chart of Hotelling $T^2$ (black) and EWMA control charts for the $1$st (blue), $5$th (red), $7$th (green), intercept (cyan), from the top to the bottom. The lines of monitored statistics in control charts have the same style and color with lines of their control limits correspondingly.}
  \label{fig:log_reg_not_ind_X_grad_cd}
\end{figure}
\begin{figure}[!htbp]
\centering
\includegraphics[width = 0.6\linewidth]{../figures/v14/sim_7/logi_small/1_sim7_logi_1e-08_0_005_1.png}
  \caption{Gradual concept drift of the logistic regression with correlated covariates. Hotelling $T^2$ of EWMA of the score function and EWMA of the absolute residual are compared.}
  \label{fig:log_reg_ind_X_grad_cd_comp}
\end{figure}

The MEWMA and EWMA control charts before (left) and after (right) transformed by {the inverse of the estimated} ${\mathbf {I}}(\bm { \theta}^{(0)})$ is as shown in Figure~\ref{fig:log_reg_not_ind_X_grad_cd}. Similarly with that in Figure \ref{fig:log_reg_not_ind_X_1}, lines corresponding to covariates with true concept drifts or independent with others are not affected by transformation in terms whether to show concept drifts in figures, confirming the validity of the assumption in this section. In Figure~\ref{fig:log_reg_ind_X_grad_cd_comp}, we see that monitoring the score vectors renders an even much earlier detection of the starting position of the concept drift than EWMA of the prediction error rate.

In summary, we demonstrated the efficacy of using MEWMA control charts to monitor the score function in detecting concept drifts. And we also show how to diagnose the concept drifts by applying proper transformation and EWMA control charts. Also, we discussed the coupling due to multicollinearity and nonlinearity and how to resolve those issues. The practicability of the assumption of small argument for decoupling concept drifts in the logistic regression was also justified.

???I moved the following from Section 5 to here. You will have to integrate it with the rest of this appendix.??? We also use simulation to demonstrate the diagnosis capability of the score-based method. We focus on the linear regression and logistic regression for proof-of-concept here, to show the challenges and resolutions. For the linear regression, EWMA control charts of components of transformed score vectors as described in Section~\ref{s:decou_cd} would correctly show whether or not covariates have concept drift, even when multicolinearity exists among covariates, as shown in Figure~\ref{fig:lin_reg_not_ind_X}. For the logistic regression, the nonlinearity of the sigmoid function would further complicate the diagnosis problem. Even for uncorrelated covariates, concept drifts of some covariates would falsely show up in EWMA control charts of those without concept drift, as shown in Figure~\ref{fig:log_reg_ind_X}. However, if we assume the argument of the sigmoid function $\bm{x}^T\bm{\theta}$ is small enough (i.e. $\ll 4$), Equation~(\ref{eqn:cd_decomp_fisher_approx}) approximately holds and we can use the Fisher information matrix to decouple the concept drift as done in the linear regression, as shown in Figure~\ref{fig:log_reg_not_ind_X_1}. We argue that this assumption about the argument $\bm{x}^T\bm{\theta}$ is practical valuable. When $\bm{x}^T\bm{\theta}$ is large the model is very confident with predictions (the sigmoid function is in plateau regions) and a small concept drift would not make much difference in the overall prediction accuracy. However, when $\bm{x}^T\bm{\theta}$ is small the data are most confusing for the model and small concept drift can have a big impact on prediction error, so that people may want to understand the origins of concept drift. One thing worth highlighting is that we obtained consistent results across experimental data sets with abrupt and gradual concept drifts, which further justify the capability of the score-based method in different parametric models (Figures~\ref{fig:lin_reg_not_ind_X_grad_cd},~\ref{fig:lin_reg_ind_X_grad_cd_comp},~\ref{fig:log_reg_not_ind_X_grad_cd},~\ref{fig:log_reg_ind_X_grad_cd_comp}).



\end{enumerate}

\end{appendix}

%\listofchanges
\end{document}