\documentclass[twoside,11pt]{article}
\usepackage{jmlr2e}
\usepackage{setspace}
%\singlespacing
\onehalfspacing % Line space
\usepackage{mathtools} % vertical centered colon
\usepackage{amsmath} % For \argmax
\DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays
\usepackage{lineno}
\linenumbers
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\setlength{\marginparwidth}{2.5cm} % For notes in margin, can be delete for final version.

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\DeclareMathOperator{\Tr}{Tr}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\jmlrheading{1}{2018}{1-48}{4/00}{10/00}{meila00a}{Kungang Zhang, Anh T. Bui, and Daniel W. Apley}

\definechangesauthor[name={Per cusse}, color=brown]{Kungang}
\definechangesauthor[name={Per cusse}, color=orange]{Anh}
\definechangesauthor[name={Per cusse}, color=red]{Apley}
% \setremarkmarkup{(#2)}

% Short headings should be running head and authors last names

\ShortHeadings{Zhang, Bui, and Apley}{}
\firstpageno{1}

\begin{document}

\title{Concept Drift Monitoring and Diagnostics of Supervised Learning Models via Score Functions}

\author{\name Kungang Zhang \email zkg@u.northwestern.edu \\
	\name Anh T.\ Bui \email buiat2@vcu.edu \\
	\name Daniel W.\ Apley \email apley@northwestern.edu \\
       \addr Department of Industrial Engineering and Management Sciences\\
       Northwestern University\\
       Evanston, IL 60208-3119, USA\\
       Department of Statistical Sciences and Operations Research\\ 
       Virginia Commonwealth University\\
       Richmond, VA 23284-3083, USA}

\editor{EDITOR NAMES}
% Author guide: http://www.jmlr.org/format/authors-guide.html
\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
Supervised learning models are widely adopted in many applications to automate business processes or decision-making. They are also one of the most fundamental classes of models used to achieve other machine learning tasks, such as unsupervised learning and reinforcement learning. Viewing supervised learning from a probabilistic perspective, the set of training data to which the model is fitted is assumed to follow a stationary distribution, regardless of when the observations are collected. However, this stationarity assumption is often violated in a phenomenon as known as concept drift, which refers to changes over time in the predictive relationship between covariates $\bm{X}$ and a response variable $Y$ and can render trained models suboptimal or obsolete. In this study, we develop a comprehensive and computationally efficient framework for detecting, monitoring, and diagnosing concept drift. Specifically, we monitor the Fisher score function, defined as the gradient of the log-likelihood for the fitted model, using a form of multivariate EWMA, which was originally designed to monitor for general changes in the mean of a random vector. In spite of the substantial performance advantages that we demonstrate over popular error-based methods, a score-based approach appears to have not been previously considered for concept drift monitoring. Advantages of the proposed score-based framework include versatility for use with any parametric supervised learning model, faster detection of changes in the predictive relationship, better sensitivity as shown in theory and experiments, and inherent diagnostic capabilities for helping identify the nature of the changes, which we also develop in this work.
\end{abstract}

\begin{keywords} 	
  Concept Drift, Score Function, Control Chart, Streaming Data, Predictive Model, Multivariate EWMA
\end{keywords}

\section{Introduction}
Supervised learning is widely used in many applications, in which models are trained to predict a response variable $Y$, given an observed set of covariates $\bm{X} \in \mathbb{R}^p$. The modeling goal is usually to make test prediction accuracy metrics (i.e. $R^2$, classification accuracy, F1-score, etc.) as high as possible. The supervised learning model can also act as an intermediate module in other machine learning tasks, such as dimension reduction in unsupervised learning or Q-Learning in reinforcement learning. Training the supervised learning model can be viewed as an optimization problem. From a probabilistic perspective, a training sample of observations, $\{\bm {x}_i, y_i\}_{i=1}^n$, is assumed to be drawn from some joint distribution, $P (\bm {X}, Y)$, such that the conditional distribution, $P(Y|\bm{X};\bm{\theta})$, is stationary, no matter when those samples are collected~(\cite{hulten2001mining}). Here, {$\bm {x}_i \in \mathbb{R}^{p}$ and $y_i \in \mathbb{R}$ are the covariates and response variable in the $i$th observation respectively}\footnote{The case with scalar response can easily be extended to vector response variable $\bm {y}_i$.}, and we have parametrized the conditional distribution via the vector $\bm{\theta}$. However, the stationarity assumption is often violated in real applications, a phenomenon as known as concept drift~(\cite{moreno2012unifying,vzliobaite2016overview}). For example, models used to predict customers' probabilities of defaulting by credit-scoring agencies are usually fitted to training data collected over a three to five years period, so that changing financial environments may result in an outdated predictive relationship by the time the trained model is used to score new customers~(\cite{crook1992degradation,vzliobaite2016overview}). 

Concept drift poses many challenges in constructing trustworthy supervised learning models. Concept drift during training (historical) and testing (future) data both can degrade performance of supervised learning models: during training, concept drift results in there being no single predictive model, since the model is changing over time; while concept drift during testing degrades the accuracy of the predictive model, relative to what it could be with an updated model for $P(Y|\bm{X};\bm{\theta})$. In order to obtain and maintain the highest possible predictive performance of supervised learning models, \textit{retrospective} concept drift analysis of the training data and \textit{prospective} concept drift monitoring of the test data are important and should be a standard component of the predictive modeling process.

Concept drift is fairly common in practice and often originates from changes of some hidden effects in generating the response outcomes of interests~(\cite{tsymbal2008dynamic,vzliobaite2012beating,widmer1996learning,kukar2003drifting,donoho2004early,carmona2010gnusmail,fong2015change,vzliobaite2016overview}). For example, in the problem of antibiotic resistance in nosocomial infections, supervised learning models are trained and validated using a data set, with response labels indicating whether the level of sensitivity of a pathogen to an antibiotic is sensitive, resistant, or intermediate, and the covariates being patients' demographical data and conditions of hospitalization~(\cite{pechenizkiy2005knowledge}). After properly training and validating such a model, it can with high accuracy determine whether or not a pathogen is sensitive to an antibiotic or not. However, according to medical experts~(\cite{kukar2003drifting}), hidden effects due to failures and/or replacements of some medical equipment, changes in personnel, and seasonal bacterial outbreaks could result in unexpected changes over time in resistance of new pathogen strains to antibiotics. In other words, there is likely a significant level of concept drift in nosocomial infections. Consequently, bacteria that are predicted to be sensitive to a particular antibiotic may now be resistant, and the errors of the predictive model becomes unacceptable. 

The problem of concept drift is gaining increasing attention, because increasingly data are organized in the form of data streams, and it is often unrealistic to expect that data distributions remain stable for a long period of time. Most state-of-the-art concept drift detection and adaptation methods, like ensemble methods~(\cite{wang2003mining}), neural networks~(\cite{calandra2012learning}), and other non-parametric methods~(\cite{frias2015online,dos2016fast}), are based on monitoring the classification error or some metrics derived from it~(\cite{ross2012exponentially,gonccalves2014comparative,barros2018large}). The main purpose of this paper is to introduce a new and comprehensive concept drift framework that is based on monitoring the Fisher scores of the individual observations over time, as opposed to metrics derived from their classification errors. The Fisher score, which we will refer to as simply the score, of an observation $(\bm{X}, Y)$ is defined as the gradient of the log-likelihood, $\log(P(Y|\bm{X};\bm{\theta}))$, with respect to the parameters, $\bm{\theta}$. As shown in the following sections, comparisons between the score-based and error-based methods from both theoretical and empirical perspectives provide strong justification for a score-based method. The score-based framework that we develop and demonstrate is intended to detect, monitor, and diagnose concept drift with the following major advantages and attractive properties:

\begin{itemize}
\item
\textit{The approach is based on monitoring for changes in the mean of the score function, which has strong theoretical justification since theory dictates that the mean of the score function changes if and only if $P(Y|\bm{X};\bm{\theta})$ changes, under fairly general conditions. We elaborate on this in Section~\ref{ss:score_func}.}
\item
\textit{In contrast, existing concept drift methods that are based on monitoring the error rate may fail to detect concept drift, because a change in $P(Y|\bm{X};\bm{\theta})$ does not necessarily result in a change in the error rate. Note that a change in $P(Y|\bm{X};\bm{\theta})$ that does not change the error rate still means that the predictive accuracy of the model can be improved if the model is updated accordingly. So it is still desirable to detect such changes. See, e.g., Figure~\ref{fig:logi_err_rate_unch} and Figure~\ref{fig:exp_no_err_ch}.}
\item
\textit{The score-based method is more sensitive to changes in $P(Y|\bm{X}; \bm{\theta})$ and thus more quickly detects concept drifts than error-based methods, as summarized in Section~\ref{ss:summ_simu} and detailed in Appendix~\ref{ss:simu_MRL} and the empirical examples.}
\item
\textit{The score-based method is applicable to any parametric classification or regression model that can be interpreted probabilistically in terms of $P(Y|\bm{X};\bm{\theta})$, parameterized by a vector of parameters $\bm{\theta}$.}
\item
\textit{The score-based approach provides a convenient means of diagnosing the nature of the concept drift (e.g., which parameters have changed) as derived in Section~\ref{s:decou_cd} and demonstrated in Section~\ref{s:demon_cd} and Appendix~\ref{ss:cd_diag}.}
\item
\textit{The score-based perspective converts the concept drift problem to the problem of monitoring changes in the mean of a random vector, for which many existing multivariate statistical process control (SPC) methods are available. We focus on a multivariate exponentially weighted moving average (MEWMA) control chart~(\cite{montgomery2007introduction}), which has desirable characteristics for our problem (see Section~\ref{ss:MEWMA}), but as new methods are developed they can be applied directly.}
\item
\textit{The computations involved in the score-based approach are almost the same computations involved in stochastic gradient descent (SGD) algorithms, which are increasingly commonly used to fit parametric supervised learning models. In this sense, the computations come at very little additional cost, resulting in a computationally inexpensive approach. We discuss this in Section~\ref{ss:sgd_score}.}
\item
\textit{It provides a consistent framework for using either retrospectively (e.g., after fitting a supervised learning model to a set of training data, to validate that the training data were stable and, if not, provide diagnostic information as to why) or prospectively (e.g., when using a fitted supervised learning model to predict new cases, to quickly signal when the predictive model has changed, indicating that it is time to update the model).}
\end{itemize}

% The score-based method monitors score vectors (realizations of the score function), using multivariate exponentially weighted moving average (MEWMA) control charts~(\cite{montgomery2007introduction}). In the paper, we use ``the score function" for the gradient of log-likelihood as a function of parameters and use ``score vectors" for evaluation of the score function given specific data observation $(\bm {x}_i,y_i)$ and value of parameters. {We also develop a diagnostic procedure, using (univariate) exponentially weighed moving average (EWMA) control charts to monitor each component of transformed sample score vectors, for diagnosing the nature of detected drifts.}

% The MEWMA control chart has been investigated and comes as the top candidate in monitoring mean changes of a vector of statistics. It has advantages of detecting the mean change in a direction invariant manner and being robust to high noise of monitored vectors. Those properties will be explained in details in Section~\ref{ss:MEWMA}.

% The sample score vectors used in monitoring can be automatically produced in prediction step and stochastic gradient descent as the derivative of the marginal log-likelihood, {$\nabla _{\bm { \theta}} \log(P (y_i|\bm {x}_i, \bm { \theta}))$}. Thus, monitoring score function requires very little extra calculation and can be easily integrated into existing systems as a component of data exploration and model performance monitoring. The discussion of the effect of using SGD on the convergence of parameters and MEWMA control charts is in Section~\ref{ss:sgd_score}.

% Because of those nice properties in methodological and practical perspectives, the score-based method is very general and can be applied onto any models which can obtain derivatives with respect to parameters, like generalized linear models or neural network; it can borrow power of any mean monitoring methods from decades of research in the field of SPC; and integration of this method into existing systems would not be computationally expensive. 

% Practically, the score-based method would be used in following ways. First, it can justify the stationarity assumption of training data set, retrospectively. Then, it can provide quantitative guidance on when the model should be updated or retrained in predicting new observations, on occurrence of a drift. Third, insights obtained from diagnosis with this method offer interpretation to the nature of drifts and starting point for resolutions, which are highly valuable in many industries (e.g. business, insurance, and health care, etc). For example, in financial industry, there are many laws to ensure that some key business processes such as determining who qualifies for lines of credit must comply with fair lending laws~(\cite{chen2018fair}) such as the Equal Credit Opportunity Act (ECOA)~(\cite{hsia1978credit}).

% In order to demonstrate properties of this method, simulation data sets with different models (linear, logistic, multinomial, and Poisson) are tested. Two real data sets (credit default and Capital Bikeshare) of classification and regression are the case studies supporting advantages of the score-based method.


We elaborate on these properties in Sections~\ref{s:theory_analysis_score} and~\ref{s:decou_cd} and summarize simulation results demonstrating them in Section~\ref{s:demon_cd}. Two real examples (credit risk scoring and bike sharing demand prediction) in Section~\ref{s:real_data} serve as case-studies to further illustrate its usage.

\section{Relation to Prior Work}
In this section, we briefly review relevant existing literature on monitoring different types of drift, in terms of goals and methodologies. The concept drift problem is of increasing importance, because data sets in real applications are often generated in dynamic environments such that the distribution of the data changes over time. The impact of such temporally dependent data distributions on model training and prediction depends on the particular type of drift. In the literature on data set drift, the terminology is not always consistent. In this work, we follow what appears to be the most common terminology according to~\cite{moreno2012unifying} and~\cite{vzliobaite2016overview}. In general, any temporal drift in the joint distribution $P(\bm {X}, Y)$ is called ``data set drift". Decomposing the joint distribution as $P(\bm{X}, Y) = P(Y|\bm{X})P(\bm {X})$, ``concept drift" refers to temporal drift in $P(Y|\bm{X})$ (which is what the fitted supervised learning model approximates), while ``covariate drift" refers to temporal drift in $P(\bm{X})$.

In general, the methods in the concept drift literature can be categorized into two classes~(\cite{tsymbal2004problem}): 1) model adaptation methods and 2) concept drift detection methods.

Model adaptation methods mainly focus on maintaining the performance of machine learning models in the presence of concept drift, without formally detecting or diagnosing the drift~(\cite{wang2003mining,tsymbal2008dynamic,gonccalves2014comparative,barros2018large}). To maintain a good prediction metric (classification or regression error), models are automatically updated (i.e., adapted) online continuously as new observations are collected, which is sometimes called online or incremental learning. This class of methods is not particularly relevant to our work, since our goal is concept drift detection and diagnosis, and not model adaptation. In fact, we view our approach as something that can be used in conjunction with model adaptation methods to make them more efficient and interpretable. In particular, the model adaptation could be turned on only when the concept drift detection component has indicated that $P(Y|\bm{X};\bm{\theta})$ has changed. Otherwise, unnecessarily adapting the model when $P(Y|\bm{X}; \bm{\theta})$ is stable is counterproductive, in terms of both predictive performance and computational expense. Or, in some applications in which the model adaptation component needs to be constantly run, the concept drift detection component can be run concurrently with little extra cost to provide diagnostic information.

The concept drift detection methods are more relevant to our work, and examples in this category include DDM~(\cite{gama2004learning}), EDDM~(\cite{baena2006early}), ECDD~(\cite{ross2012exponentially}), and Linear-4-rate~(\cite{wang2015concept}), etc. Although these methods can also be applied in a continuous model adaptation setting, we focus on their use and performance in our setting, in which one has a single model fitted to one set of training data. The Drift Detection Method (DDM) monitors the accumulated classification error rate as it evolves over time. The Early Drift Detection Method (EDDM) instead monitors the intervals between consecutive errors. The EWMA for Concept Drift Detection (ECDD) method monitors the misclassification rate using a univariate EWMA control chart. In Linear-4-rate, four statistics are monitored simultaneously to detect concept drift in binary classification problems. When the particular statistic monitored in each method crosses a specified threshold, an alarm is triggered indicating that concept drift has been detected. Following an alarm, subsequent action such as updating the model or inspecting the data sources can be taken. Most of the methods are designed specifically for binary classification and based on simple error-based metrics, like classification error, precision, recall, and residuals of models~(\cite{wang2015concept,barros2018large}), which are limited in capability compared to our score-based concept drift detection approach.

Although the concept drift community seems to be unaware of the potential of the score-based methods for concept drift (e.g., it is not mentioned in the recent surveys in~\cite{barros2018large} and~\cite{lu2018learning}), there has been some work in the econometrics literature that has used the score function to test for changes in the parameters of regression models~(\cite{kuan1995generalized,zeileis2005unified,zeileis2007generalized,xia2009monitoring}). Our work differs from this prior work in that we develop a comprehensive framework for and investigate issues more relevant in the typical situations to which the so-called concept drift paradigm refers. The econometrics work is developed mainly around the change-point paradigm in which it is assumed there is a single point in time at which $\bm{\theta}$ changes from some before value to some after value, and the goal is to identify the change point with formal hypotheses testing. In contrast, our approach is developed around the much more general and flexible situation in which $\bm{\theta}$ can continuously drift and/or change abruptly at multiple change-points, which is far more common in typical concept drift applications. Ours is more of an exploratory approach to inform the predictive modeling process, as opposed to formal hypotheses testing of well-defined but restrictive hypotheses. We provide strong theoretical and numerical justification for the particular score-based approach that we develop for the general drifting-$\bm{\theta}$ situation. Our approach is also more general in the sense that it applies to any parametric supervised learning model in which $\log{P(Y|\bm{X};\bm{\theta})}$ is differentiable in $\bm{\theta}$ (e.g., any generalized linear model, neural networks, Gaussian process models, etc.). Moreover, we develop and study a number of other aspects that are highly relevant to the concept drift setting, including the computational connection to SGD, a diagnostic framework for understanding the nature of the concept drift that is consistent with the monitoring framework, and use of the framework for both retrospective and prospective analyses of the stability of the predictive relationship. Finally, unlike any formal hypotheses testing approach, our approach is meaningful even in situations in which the parametric supervised learning model is substantially wrong (i.e., its structure differs substantially from the true $P(Y|\bm{X})$, which may be nonparametric), which we discuss in Section~\ref{ss:sgd_score}.

\section{Monitoring the Score Function for Concept Drift}
\label{s:theory_analysis_score}
To monitor supervised learning models for concept drifts, we propose a systematic framework based on the sample score vectors derived from parametric models. In this section, we present theoretical arguments for using the score function as the basis for concept drift monitoring (Section~\ref{ss:score_func}); empirical counterparts to the theoretical arguments, including interpretations when the parametric model structure is only an approximation to the true $P(Y|\bm{X})$ and connections to SGD (Section~\ref{ss:sgd_score}); the MEWMA procedure for monitoring the mean of the score function (Section~\ref{ss:MEWMA}); and various implementation issues and how to handle high-dimensional $\bm{\theta}$ and regularized models (Section~\ref{ss:high_dim_score}).

\subsection{The Score Function as a Basis for Concept Drift Monitoring}
\label{ss:score_func}
Supervised learning is used to approximate an underlying conditional response distribution, which, if parametric, can be denoted as $P(Y|\bm{X};\bm{\theta})$. For example, for a regression neural network, the conditional mean $E[Y|\bm{X};\bm{\theta}]$ is modeled as a neural network, and $Y$ is assumed to be its conditional mean plus (typically) a Gaussian error; or for a classification neural network, $Y$ is multinomial with class probabilities that are modeled as a neural network. Fitting the model then entails estimating the parameters $\bm{\theta}$ by maximizing the log-likelihood, which can be viewed as an empirical approximation to $E_{\bm{\theta}^{(0)}}[\log{P(Y|\bm{X};\bm{\theta})}]$, where $\bm{\theta}^{(0)}$ denotes the true parameters. This is generally valid because of Shannon's Lemma~(\cite{shannon1948mathematical}), which states that if the model is correct and identifiable, the true parameter vector $\bm{\theta}^{(0)}$ \textit{uniquely maximizes} the expected log-likelihood, $E_{\bm{\theta}^{(0)}}[\log{P(Y|\bm{X};\bm{\theta})}]$. 

Given a parametric distribution or {marginal} likelihood function {$P(Y | \bm {X}; \bm{\theta})$} for an individual observation $(\bm{X}, Y)$, and assuming we have an i.i.d. training data set $\{(\bm {x}_i, y_i)\}_{i=1}^n$ drawn from this distribution, the score function for $ (\bm {x}_i, y_i)$ is defined as 
\begin{align}
\bm{s}(\bm { \theta}; (\bm {x}_i, y_i)) = \nabla _{\bm { \theta}} { \log{P(y_i | \bm {x}_i; \bm{\theta})}}
\label{eqn:score_func}
\end{align}
where $\nabla _{\bm { \theta}}$ is the derivative operator with respect to the vector of parameters, $\bm {\theta}$. From fundamental theory (Proposition 3.4.4 from~\cite{bickel2015mathematical}), under certain regularity conditions, if the model is correct and we have a true parameter vector {$\bm { \theta} ^{ (0)}$}, the expected score function evaluated at $\bm { \theta} ^{ (0)}$ is zero:
\begin{align}
E_{\bm { \theta} ^{ (0)}}[\bm{s}(\bm { \theta}^{ (0)};(\bm {X}, Y))|\bm {X}] = \int\bm{s}(\bm { \theta}^{ (0)};(\bm {X}, Y=y))P(Y=y|\bm{X};\bm{\theta}^{(0)})dy = \bm{0}
\label{eqn:score_exp_zero}
\end{align}
where the subscript $\bm { \theta} ^{ (0)}$ on the expectation operator indicates that it is with respect to $Y|\bm{X}$ following the distribution $P(Y|\bm{X};\bm{\theta}^{(0)})$. In other words, the conditional expectation of the score function evaluated at $\bm { \theta} ^{ (0)}$ is identically zero.

Concept drift is defined as a change in $ P (Y|\bm {X})$, and in our parametric model setting it equates to a change in the parameters of the conditional distribution $P(Y|\bm {X}; \bm { \theta})$. This suggests that a general approach for monitoring for concept drift is to monitor for changes in the mean of the sample score vector in Equation~(\ref{eqn:score_func}). In particular, with no concept drift, we have $\bm{\theta}=\bm{\theta}^{(0)}$, so that the sample score vector $\bm{s}(\bm { \theta}^{ (0)};(\bm {X}, Y))$ is zero-mean from the above discussion. In contrast, if there is concept drift, this means $\bm{\theta}$ has changed from $\bm{\theta}^{(0)}$ to some other value $\bm{\theta}^{(1)} \neq \bm{\theta}^{(0)}$, in which case the new mean of the score vector $E_{\bm { \theta} ^{ (1)}}[\bm{s}(\bm { \theta}^{ (0)};(\bm {X}, Y))|\bm {X}] \neq \bm{0}$ is no longer zero mean (for small changes in $\bm{\theta}$ and under certain identifiability assumptions). As a preview to how the above concepts will be applied in practice (see Section~\ref{ss:sgd_score} for details), a set of estimated parameters from a training data set will take the place of $\bm { \theta}^{ (0)}$, and to whatever the current values of $\bm{\theta}$ have drifted will take the place of $\bm{\theta}^{(1)}$.

To illustrate the above arguments more concretely, consider the generalized linear model (GLM)~(\cite{nelder1972generalized}), which encompasses many models commonly used in applications. The canonical form of the GLM marginal likelihood and score function is
\begin{align}
\begin{aligned}
P(Y|\bm{X};\bm { \theta} ^{(0)}, \phi) =& \exp\{\frac{Y \psi(\bm{X};\bm { \theta} ^{(0)})-b( \psi(\bm{X};\bm { \theta} ^{(0)}))}{ a ( \phi)} + c(Y; \phi)\} \\
\bm {s}(\bm { \theta} ^{(0)};(\bm {X}, Y)) =& \frac{1}{a( \phi)}(Y - b'( \psi (\bm{X};\bm { \theta} ^{(0)})))\nabla _{ \bm { \theta}} \psi(\bm{X};\bm { \theta} ^{(0)})
\end{aligned}
\label{eqn:score_glm}
\end{align}
where $b(\cdot)$ and $ \psi(\cdot)$ are two functions, the derivative $b'({\psi(\bm{X};\bm{\theta} ^{(0)})})$ is with respect to $ \psi(\cdot)$, and $ a ( \phi)$ is a positive scaling factor. In GLMs, the function $g(\cdot)$ such that $g(E[Y|\bm{X}]) = \psi ( \bm{X};\bm { \theta})=\bm {X}^T\bm { \theta}$ is called the canonical link function. Using a Taylor expansion, the expected score function with a small parameter change $ \Delta \bm { \theta}= \bm { \theta}^{(1)}-\bm { \theta}^{(0)}$ is
\begin{align}
\begin{aligned}
E _{\bm { \theta} ^{(1)}}[\bm {s}(\bm { \theta} ^{(0)};(\bm {X}, Y))|\bm{X}] \approx& \frac{1}{a ( \phi)}b''( \psi(\bm{X}; \bm { \theta} ^{(0)}))\nabla _{ \bm { \theta}} \psi (\bm{X}; \bm { \theta} ^{ (0)}) \nabla _{\bm { \theta}}^T \psi (\bm{X}; \bm { \theta} ^{ (0)}) \Delta \bm { \theta} \\
=& \frac{1}{a ( \phi)}b''( \psi(\bm{X}; \bm { \theta} ^{(0)}))\bm {X}\bm {X}^T \Delta \bm { \theta}
\end{aligned}
\label{eqn:exp_score_glm}
\end{align}
It is known that $Var[Y|\bm{X}] = a ( \phi)b''(\psi (\bm{X}; \bm { \theta} ^{ (0)}))$ so that $b''(\psi (\bm{X}; \bm { \theta} ^{ (0)}))>0$ always holds, if $Y$ is not a deterministic function of $\bm{X}$. If we further take the expectation of Equation~(\ref{eqn:exp_score_glm}) with respect to the distribution of $\bm{X}$, then the resulting matrix that premultiplies $\Delta\bm{\theta}$ is always positive definite if the distribution of $\bm{X}$ is not degenerate. Notice that this conclusion does not require $ \psi (\bm{X}; \bm { \theta})$ to be a linear function of $\bm { \theta}$, if certain identifiability conditions on $\bm{\theta}$ are satisfied and the Hessian matrix of the $\psi(\bm{X};\bm{\theta})$ with respect to the parameter vector has eigenvalues with small absolute values (e.g. the Hessian matrix of linear $\psi(\bm{X};\bm{\theta})$ has eigenvalues all as zero). For example, $ \psi (\bm{X}; \bm { \theta})$ can be a neural network, in which case $\bm { \theta}$ is the vector of weights and biases for the neural net. In later studies with simulated and real data sets in Sections~\ref{s:demon_cd} and ~\ref{s:real_data}, we show that our score-based method is also effective for neural networks.

In summary, for GLM-type response models with linear or nonlinear $\psi(\cdot)$ that satisfies certain identifiability conditions on $\bm{\theta}$, \textit{the mean of the score function changes if and only if $P(Y|\bm{X};\bm{\theta})$ changes}. This is an important property that provides the theoretical basis for our score-based concept drift monitoring and diagnosis. Note that error-based methods do not enjoy this property, i.e., $P(Y|\bm{X};\bm{\theta})$ can change in ways that do not change the error rate.

\begin{figure}[!htbp]
\centering
 \begin{subfigure}[t]{0.4\linewidth}
         \centering
         \includegraphics[width=\textwidth, trim=.2in .2in .7in .45in, clip]{../figures/v14/demons_fig/2D_logi_orig.png}
         \captionsetup{width=.95\linewidth}
         \caption{The original data generating process and a model with the optimal decision boundary.}
         \label{fig:logi_err_rate_unch_a}
  \end{subfigure}
  \begin{subfigure}[t]{0.4\linewidth}
         \centering
         \includegraphics[width=\textwidth, trim=.2in .2in .7in .45in, clip]{../figures/v14/demons_fig/2D_logi_cd.png}
         \captionsetup{width=.95\linewidth}
         \caption{The drifted data generating process without updating the optimal decision boundary.}
         \label{fig:logi_err_rate_unch_b}
  \end{subfigure}
%  \begin{subfigure}[t]{0.4\linewidth}
%          \centering
% 	 \includegraphics[width = \textwidth, trim=.2in .2in .7in .45in, clip]{../figures/v14/demons_fig/2D_logi.png}
% 	    \captionsetup{width=.95\linewidth}
%          \caption{The shaded areas from Figure~\ref{fig:logi_err_rate_unch_a} and~\ref{fig:logi_err_rate_unch_b} are overlapped and it shows unchanged error rate.}
%          \label{fig:logi_err_rate_unch_c}
%   \end{subfigure}
  \begin{subfigure}[t]{0.4\linewidth}
         \centering
	 \includegraphics[width=\textwidth, trim=.2in .2in .7in .45in, clip]{../figures/v14/demons_fig/2D_logi_cd_updated.png}
         \captionsetup{width=.95\linewidth}
         \caption{The drifted data generating process after updating the optimal decision boundary.}
         \label{fig:logi_err_rate_unch_d}
  \end{subfigure}
  \caption{The demonstration of simple logistic regression that concept drift would result in no change in the error rate.}
  \label{fig:logi_err_rate_unch}
\end{figure}

To further illustrate the above with a more concrete example, consider the following binary simple logistic regression example in which concept drift occurs but with no change in the error rate. The formulas defining the model and the corresponding score function in this case are
\begin{align}
\begin{aligned}
&P(Y=1|\bm{X};\bm{\theta}) = \frac{\exp\{\bm {X}^{T} \bm {\theta}\}}{1+\exp\{\bm {X}^{T} \bm {\theta}\}} \\
&\bm {s}(\bm { \theta} ; (\bm {X}, Y)) = (Y-P(Y=1|\bm{X};\bm{\theta}))\bm {X}
\end{aligned}
\label{eqn:logi_mod_score}
\end{align}
where $\bm{X} = [1, X]^T$ and $\bm{\theta}=[\theta_0, \theta_1]^T$. It is straightforward to verify that the (conditional) expectation of the score function is $\bm {0}$, when $\bm{\theta}$ does not change.

Further suppose the $\bm{X}$ follows a standard normal distribution with density function denoted by $q(x)$ and the conditional distribution $P(Y|\bm{X};\bm{\theta})$ has concept drift with $\bm{\theta}$ changing from $\bm{\theta}^{(0)}$ to $\bm{\theta}^{(1)}$. Define 
\begin{align}
p^{(j)}(X)=P(Y=1|\bm{X}=[1,X]^T;\bm{\theta}^{(j)}),~j\in\{0,1\},
\label{eqn:simp_nota_p}
\end{align}
where $j=\{0,1\}$ indicates whether the parameters are before or after the change. Figures~\ref{fig:logi_err_rate_unch_a} and~\ref{fig:logi_err_rate_unch_b} plot $p^{(0)}(x)$ and $p^{(1)}(x)$, respectively. Suppose we use the classification rule $\hat{y}(x)=1$ if $p^{(0)}(x)\geq 0.5$ (i.e., if $x\geq 0 $) and $\hat{y}(x)=0$ otherwise. Then the conditional and unconditional error rates are
% \begin{align}
% \begin{aligned}
% err(x;\bm{\theta}^{(j)})\vcentcolon=&P(Y\neq\hat{y}(x)|\bm{X}=[1,x]^T;\bm{\theta}^{(j)}) \\
% =& p^{(j)}(x)(1-I(x\geq x_{opt})) + (1-p^{(j)}(x))I(x\geq x_{opt}) \\
% err(\bm{\theta}^{(j)})\vcentcolon=&E[err(X;\bm{\theta}^{(j)})] = \int_{-b}^{b}err(x;\bm{\theta}^{(j)})q(x)dx
% \end{aligned}
% \label{eqn:logi_err_rate}
% \end{align}
\begin{align}
\begin{aligned}
&P(Y\neq\hat{y}(X)|\bm{X}=[1,X]^T;\bm{\theta}^{(j)})
= p^{(j)}(X)(1-\hat{y}(X)) + (1-p^{(j)}(X))\hat{y}(X) \\
&P(Y\neq\hat{y}(X);\bm{\theta}^{(j)}) = E[P(Y\neq\hat{y}(X)|\bm{X}=[1,X]^T;\bm{\theta}^{(j)})] \\ &= \int_{-b}^{b}P(Y\neq\hat{y}(x)|\bm{X}=[1,x]^T;\bm{\theta}^{(j)})q(x)dx
\end{aligned}
\label{eqn:logi_err_rate}
\end{align}
Note that this classifier $\hat{y}(x)$ minimizes the unconditional error rate. When $\bm{\theta} = \bm{\theta}^{(0)}$ so the true predictive relationship $P(Y=1|\bm{X})$ is as in Figure~\ref{fig:logi_err_rate_unch_a}, the error rate is $22.2\%$. Now suppose we use the same classifier based on $\bm{\theta}^{(0)}$, but $\bm{\theta}$ changes to $\bm{\theta}^{(1)}$ so the true predictive relationship $P(Y=1|\bm{X})$ is as in Figure~\ref{fig:logi_err_rate_unch_b}. Even though $\bm{\theta}$ has changed, the error rate remains unchanged at $22.2\%$. Clearly, any concept drift detection method based on the error rate will fail to detect the change in $\bm{\theta}$. It is important to note that although the error rate does not change when $\bm{\theta}$ changes from $\bm{\theta}^{(0)}$ to $\bm{\theta}^{(1)}$, the change in $\bm{\theta}$ constitutes an opportunity to improve the classification accuracy if we were to detect the change and update the classification rule based on the new model, i.e., use the new classifier $\hat{y}(x)=1$ if $x \geq 0.59$, which corresponds to $\hat{y}(x)=1$ if $p^{(1)}(x)\geq 0.5$ (see Figure~\ref{fig:logi_err_rate_unch_d}).

In contrast to any error rate monitoring approach, as we have shown earlier, the mean of the score function always changes when $P(Y|\bm{X};\bm{\theta})$ changes, under fairly general conditions. Furthermore, as we will show in Section\ref{ss:MEWMA}, the mean of our score-based MEWMA monitoring statistic also changes, when $P(Y|\bm{X};\bm{\theta})$ changes.

% Error rate of the logistic regression model given predictor $\bm {X}$ can be written as a function (we call it ``penalty function" to help explanation):
% \begin{align}
% C _{err}^{(i)}(\bm {X}) = p ^{(i)}I(\hat{y}=0)+(1-p ^{(i)})I(\hat{y}=1)
% \label{eqn:penal_err}
% \end{align}
% where $p ^{(i)} = P(Y=1|\bm {X}; \bm { \theta} ^{(i)})$, ($i=0,1$), and the superscript, $i$, indicates that this probability function is from original distribution under which the model is trained ($i=0$) or the distribution of new samples during prediction (after a decision model being trained, $i=1$). Thus, if concept drift happens after training, $p ^{(1)} \neq p ^{(0)}$; otherwise, they are equal. Notice that the probability function $p ^{(1)}$ and indentity function $I$ depend on predictor vector $\bm {X}$, but omitted for clean notation. Given the distribution of covariates $\bm {X}$ and decision rule (model) $\hat{y}$, the expectation of this penalty function are those shaded areas in the Figure~\ref{fig:logi_err_rate_unch_a} and \ref{fig:logi_err_rate_unch_b} for the original (blue) and the drifted (yellow) data generating process. In the Figure~\ref{fig:logi_err_rate_unch_c}, Figure~\ref{fig:logi_err_rate_unch_a} and~\ref{fig:logi_err_rate_unch_b} are overlapped together. The drifted data generating process decreases the error by those blue shaded area but adds the red shaded area as new error. Because the two areas are equal, the expected error rate remains the same. If we only monitor the error rate or any metrics derived from it, the concept drift would be missed. More important, this concept drift changes the optimal decision boundary, so that retraining the model can potentially obtain higher accuracy. 



% These plots can be generalized into other penalty functions for metrics like Hotelling $T^2$ of EWMA of the score function as mentioned in the Section~\ref{ss:MEWMA}. For more intuitive comparison, penalties are put close to horizontal line $y=0$, so that the expectation of monitored penalty function equals the area under the curve in Figure~\ref{fig:logi_med_penal}.
 
% The penalty function for score function after simplification is:
% \begin{align}
% C _{score}^{(i)}(\bm {X}) = (p ^{(i)} (1 - p ^{(0)}) + p ^{(0)}(p ^{(0)}-p ^{(i)})) \bm {X}^T\bm { \Sigma}^{-1}\bm {X}
% \label{eqn:penal_score}
% \end{align}
% where $\bm { \Sigma} = E _{\bm {X}}[p ^{(0)}(1-p ^{(0)})\bm {X}\bm {X}^T]$ is the covariance matrix of the score function of the logistic model under training distribution, and the subscript of the expectation means it is over the distribution of covariates $\bm {X}$. As we can see in Figure~\ref{fig:logi_med_penal}, after concept drift, error has the decreased part (blue shaded area) and increased part (red shaded area), which are approximately equal. However, the Hotelling $T^2$ of EWMA of the score function has the increased part larger than the decreased part resulting in net positive change in the penalty function of score function, which indicates that it is more sensitive for monitoring concept drift. The reason is that score function are applied EWMA first and then Hotelling $T^2$. Reversing the order of applying EWMA and Hotelling $T^2$ would void this property, because random noises cannot be averaged out. According to the penalty function~(\ref{eqn:penal_score}) and~(\ref{eqn:penal_err}) and after some derivation, we can see that it makes sense that if two probability functions, $p ^{(0)}$ and $p ^{(1)}$ are different, we have $\int_{\bm{x}}(C _{score}^{(1)}(\bm {x})-C _{score}^{(0)}(\bm {x}))p(\bm{x})d\bm{x}>0$ but the sign of $\int_{\bm{x}}(C _{err}^{(1)}(\bm {x})-C _{err}^{(0)}(\bm {x}))p(\bm{x})d\bm{x}$ is uncertain, which means score-based method directly monitors the deviation of $p ^{(1)}$ from $p ^{(0)}$ while error-based method is not. In other words, score-based method monitors exactly the concept drift. Here the simple logistic regression gives an intuition why score function performs better in monitoring concept drift of parametric models. Of course, in detecting the change of mean, noise level would also affect the sensitivity.

\subsection{Interpretations with Empirical Data and Incorrect Models}
\label{ss:sgd_score}
The zero-mean property $E_{\bm{\theta}^{(0)}}[\log{P(Y|\bm{X}; \bm{\theta}^{(0)})}] = \textbf{0}$ of the score function and the uniqueness of the parameters $\bm{\theta}$ that maximize the expected log-likelihood $E_{\bm{\theta}^{(0)}}[\log P(Y|\bm{X}; \bm{\theta})]$ are guaranteed to hold only when the model is correct; that is, when the supervised learning model $P(Y|\bm{X};\bm{\theta})$ is of the same structure as the true predictive relationship $P(Y|\bm{X})$. Recalling the adage that ``All models are wrong, but some are useful"~(\cite{box1976science})), one might wonder to what extent the results in the previous section are applicable when the structure of the model $P(Y|\bm{X};\bm{\theta})$ differs from the true $P(Y|\bm{X})$. A related question is what should we take to be the empirical counterparts to $E_{\bm{\theta}^{(0)}}[\bm{s}(\bm{\theta}^{(0)};(\bm{X},Y) )]$, when $\bm{\theta}^{(0)}$ is replaced by its estimate from a sample of training data, and the expectation is replaced by a sample average over a set of new data or over the same training data. We address both of these issues in this section and also relate the empirical counterpart to SGD for computational reasons. 

Regardless of whether the model structure is correct, in analogy with Equation~(\ref{eqn:score_exp_zero}), we always have
\begin{align}
\begin{aligned}
&\hat{E}_{(0)}[\bm{s}(\hat{\bm{\theta}}^{(0)};(\bm{X}, Y))] \vcentcolon=\frac{1}{n}\sum_{i=1}^{n}\bm{s}(\hat{\bm{\theta}}^{(0)};(\bm{x}_i, y_i))=\bm{0}, \text{where} \\
&\hat{\bm{\theta}}^{(0)} \vcentcolon =  \argmax_{\bm{\theta}}\hat{E}_{(0)}[\log{P(Y|\bm{X}; \bm{\theta})}] \vcentcolon= \argmax_{\bm{\theta}}\frac{1}{n}\sum_{i=1}^n \log{P(y_i|\bm{x}_i;\bm{\theta})},
\end{aligned}
\label{eqn:score_exp_zero_emp}
\end{align}   
the operator $\hat{E}_{(0)}$ denotes a sample average over the training data $\{(\bm{x}_i, y_i)\}_{i=1}^n$, and $\hat{\bm{\theta}}^{(0)}$ is maximum-likelihood estimator (MLE) of $\bm{\theta}^{(0)}$ for the training data. That is, when we fit a model using MLE, we have $\nabla_{\bm{\theta}}\hat{E}_{(0)}[\log{P(Y|\bm{X}; {\bm{\theta}})}]|_{\bm{\theta}=\hat{\bm{\theta}}^{(0)}} \vcentcolon=\nabla_{\bm{\theta}}\frac{1}{n}\sum_{i=1}^n \log{P(y_i|\bm{x}_i;{\bm{\theta}})|_{\bm{\theta}=\hat{\bm{\theta}}^{(0)}}}\\=\frac{1}{n}\sum_{i=1}^n\bm{s}(\hat{\bm{\theta}}^{(0)};(\bm{x}_i, y_i)) = \hat{E}_{(0)}[\bm{s}(\hat{\bm{\theta}}^{(0)};(\bm{X}, Y))]$ as the gradient of the training log-likelihood identically zero, even if the model is not the correct structure. Thus, (\ref{eqn:score_exp_zero_emp}) is the empirical counterpart of (\ref{eqn:score_exp_zero}) with the estimated $\hat{\bm{\theta}}^{(0)}$ taking the place of the true $\bm{\theta}^{(0)}$. 

Now suppose the true predictive relationship $\tilde{P}(\tilde{Y}|\tilde{\bm{X}})$ changes from $P(Y|\bm{X})$ over some new set of data $\{(\tilde{\bm{x}}_i, \tilde{y}_i)\}_{i=1}^{\tilde{n}}$. In this case, if the operator $\hat{E}_{(1)}$ denotes a sample average over the new data, a different set of parameters $\hat{\bm{\theta}}^{(1)} \neq \hat{\bm{\theta}}^{(0)}$ for the same supervised learning model $P(Y|\bm{X};\bm{\theta})$ will now provide a better fit to the new data than did $\hat{\bm{\theta}}^{(0)}$, where 
\begin{align}
\begin{aligned}
\hat{\bm{\theta}}^{(1)} \vcentcolon= \argmax_{\bm{\theta}}\hat{E}_{(1)}[\log{P(\tilde{Y}|\tilde{\bm{X}}; \bm{\theta})}] \vcentcolon= \argmax_{\bm{\theta}}\frac{1}{\tilde{n}}\sum_{i=1}^{\tilde{n}} \log{P(\tilde{y}_i|\tilde{\bm{x}}_i;\bm{\theta})},
\end{aligned}
\label{eqn:score_exp_nonzero_emp}
\end{align}   
Thus, the gradient $\nabla_{\bm{\theta}}\hat{E}_{(1)}[\log{P(\tilde{Y} | \tilde{\bm{X}}; {\bm{\theta}})}]|_{\bm{\theta}=\hat{\bm{\theta}}^{(0)}} \vcentcolon= \nabla_{\bm{\theta}}\frac{1}{\tilde{n}}\sum_{i=1}^{\tilde{n}} \log{P(\tilde{y}_i | \tilde{\bm{x}}_i; {\bm{\theta}})}|_{\bm{\theta}=\hat{\bm{\theta}}^{(0)}} = \\ \frac{1}{\tilde{n}} \sum_{i=1}^{\tilde{n}} \bm{s}(\hat{\bm{\theta}}^{(0)};(\tilde{\bm{x}}_i, \tilde{y}_i)) = \hat{E}_{(1)}[\bm{s}(\hat{\bm{\theta}}^{(0)};(\tilde{\bm{X}}, \tilde{Y}))]$ of the log-likelihood for the new data (but evaluated at the original estimate $\hat{\bm{\theta}}^{(0)}$) will generally differ from zero. The more $\tilde{P}(\tilde{Y}|\tilde{\bm{X}})$ changes from $P(Y|\bm{X})$, the more we expect $\hat{\bm{\theta}}^{(1)}$ to differ from $\hat{\bm{\theta}}^{(0)}$, and the more we expect the new average score vector $\hat{E}_{(1)}[\bm{s}(\hat{\bm{\theta}}^{(0)};(\tilde{\bm{X}}, \tilde{Y}))]$ to differ from $\bm{0}$. This provides the justification for our score-based concept drift monitoring approach, which tracks the mean of the score vector $\bm{s}(\hat{\bm{\theta}}^{(0)};(\bm{X}_i, Y_i)) = \nabla_{\bm{\theta}}\log P(Y_i|\bm{X}_i; \bm{\theta})|_{\bm{\theta}=\hat{\bm{\theta}}^{(0)}}$ to detect and analyze changes in it. 

If the supervised learning model $P(Y|\bm{X};\bm{\theta})$ is of the same structure as the true predictive relationship $P(Y|\bm{X})$, both $P(Y|\bm{X})$ and $P(\bm{X})$ are constant across the training data $\{(\bm{x}_i, y_i)\}_{i=1}^n$, and $n \to \infty$, then under some regularity conditions the MLE $\hat{\bm { \theta}} ^{ (0)}$ is consistent, and  $\hat{E}_{(0)} [\bm{s}(\hat{\bm { \theta}} ^{ (0)};(\bm {X}, Y))] \to E_{\bm { \theta} ^{ (0)}}[\bm{s}(\hat{\bm { \theta}} ^{ (0)};(\bm {X}, Y))] \to E_{\bm { \theta} ^{ (0)}}[\bm{s}(\bm { \theta} ^{ (0)};(\bm {X}, Y))] = \bm {0}$. In this case, there is no distinction between the theoretical version of the score-based monitoring arguments and their empirical version discussed above. With large $n$, SGD is often used to fit models to the training data, which involves approximating the gradient of the log-likelihood function using individual training observations or mini-batches of training observations at each iteration of the optimization algorithm. The SGD estimator of $\bm { \theta} ^{ (0)}$ converges to the batch MLE under certain conditions involving step size and other considerations (see, e.g., Theorem 4.7 of \cite{bottou2018optimization}). In this case, under the same asymptotic conditions stated above, the SGD estimator $\hat {\bm { \theta}}_{SGD}$ is also consistent and $\hat{E}_{(0)} [\bm{s}(\hat{\bm { \theta}}_{SGD};(\bm {X}, Y))] \to \bm{0}$ as $n \to \infty$. 

We argue that the score-based method does not add much extra cost to the current framework of training and using models. For retrospective analysis, the sample score vectors are a byproduct of the SGD, since the mini-batch gradients are of the form ($\nabla _{\bm { \theta}} \sum _{i=1} ^{m} \ln P(y_i|\bm {x}_i;\bm{\theta}) = \sum _{i=1} ^{m} \bm{s}(\bm { \theta};(\bm {x}_i, y_i))$, where $m$ is the batch size). For prospective analysis, prediction of new data usually partially calculates the score vectors. For example, prediction for neural networks requires forward-propagation, and another backward-propagation in memory would generate the score function. Thus, we do not need much extra computation to apply the score-based method.

With finite training data size $n$ and finite new sample sizes $\tilde{n}$ for monitoring (or even $\tilde{n}=1$), noise in $\bm{s}(\bm { \theta};(\bm {x}_i, y_i))$ and its sample averages must be considered. In particular, we need to distinguish by how much $\bm{s}(\hat{\bm { \theta}}^{(0)};(\bm {x}_i, y_i))$ should differ from $\bm{0}$ before we conclude that $P(Y|\bm{X})$ has changed. The MEWMA monitoring strategy in the next section is designed to distinguish noise from legitimate changes in $P(Y|\bm{X})$. Moreover, for models fitted with regularization, the gradient of the log-likelihood itself is not zero over the training data, because the regularization penalty is included in the optimization objective function. Regardless, the score-based monitoring method can still be applied with the minor modification to the score vectors discussed in Section~\ref{ss:high_dim_score}.

\subsection{An MEWMA Approach for Monitoring the Score Function}
\label{ss:MEWMA}
As discussed in the previous sections, monitoring for concept drift reduces to monitoring for changes over time in the mean of the score function. Among other challenges, this requires distinguishing between noise in the individual score functions vs. an actual mean change. Monitoring for changes in the mean of random vectors (e.g., a set of multivariate quality-related measures) while distinguishing from noise is an old and well-researched problem in the SPC literature. The MEWMA has emerged as one of the most effective techniques for this, and in this section we develop it for monitoring the score function mean.

The MEWMA at time $t$, denoted by $\bm{z}_t$, is defined recursively (for $t=1,2,\cdots$) via:
\begin{align}
\bm {z}_t = \lambda \bm {s}_t + (1 - \lambda) \bm {z} _{t-1},
\label{eqn:ewma}
\end{align}
where $\bm {s}_t$ is the to-be-monitored random vector at time $t$, which in our case is the score vector $\bm {s}_t \vcentcolon= \bm{s}(\hat{\bm { \theta}}^{(0)};(\bm {x}_t, y_t))$; $ \lambda$ is a weighting parameter; and $\bm{z}_0$ can be initialized as the sample mean of $\bm {s}_t$ over some small initial batch of training data. An equivalent expression for the recursive relationship~(\ref{eqn:ewma}) is $\bm {z}_t = \lambda\sum _{\tau=1}^t (1-\lambda) ^{t-\tau} \bm{s} _{\tau} + (1-\lambda)^t \bm{z}_0$, which gives exponentially decaying weights to the past $\bm{s}_t$ observations. It follows that smaller $\lambda$ in the MEWMA formula corresponds to more slowly decaying weights and thus longer effective windows over which the exponentially-weighted averages are computed. The effective window length is sometimes viewed as $\sum _{j=0}^\infty (1-\lambda)^j = \frac{1}{\lambda}$. The choice of $\lambda$ should depend on the application of interest, with the following trade-off. Smaller $\lambda$ translates to a larger effective window length, which gives a lower-variance estimate of the mean of $\bm{s}_t$ by smoothing out more noise (which generally results in better detection of small changes), but it also makes the MEWMA more sluggish (which results in longer delays in detecting large changes). If there is no need to detect changes in $P(Y|\bm{X})$ in fewer than some number (say $D$) of observations, then there is no need to have the effective window length $\frac{1}{\lambda}$ smaller than $D$, in which case one should  select $\lambda \leq \frac{1}{D}$.

Since $\bm{s}_t$ and $\bm{z}_t$ are vectors, and we desire to detect changes in the mean of $\bm{s}_t$ in any direction, our MEWMA approach monitors the Hotelling $T^2$ statistic 
\begin{align}
T_t^2 = (\bm {z}_t-\bar { \bm {s}})^T \hat {\bm { \Sigma}} ^{-1}(\bm {z}_t-\bar { \bm {s}})
\label{eqn:hotellingt2}
\end{align}
where $\bar {\bm{s}}$ and $\hat {\bm {\Sigma}}$ are the sample mean vector and covariance matrix of $\bm {s}_t$, respectively, estimated from some set of training data. If $T_t^2$ at some $t$ exceeds a specified upper control limit (UCL), this is taken to be an indication that $P(Y|\bm{X})$ in the time-vicinity of observation $t$ has changed from what it was when $\hat{\theta}^{(0)}$ was estimated.

% After obtaining the initial data set, we can use MEWMA to minimize concept drift in training data, as much as possible through retrospective analysis. With a well-trained and validated predictive model, we execute two phases for concept drift detection. In Phase-I, we {monitor Hotelling $T^2$ of EWMA of the score function for a certain period of time and ensure that those newly incoming data are in-control and} then calculate and set {upper and lower} control limits {(UCL and LCL)} based on a targeted false alarm rate. Here, the in-control data presents random fluctuation in the Phase-I without obvious trend as shown in Figure~\ref{fig:Monitoring}, because sample score vectors would fluctuate around $\bm {0}$ when environment is in stationary and model training becomes stable (as discussed in Section~\ref{ss:sgd_score}). This step ensures that no concept drift happens in Phase-I and the obtained control limits are trustworthy. The false alarm rate is usually chosen small (i.e. $0.1\%$), so that in monitoring the likelihood of encountering false alarms is small. In Phase-II, we continue to monitor sample score vectors of incoming data, while using control limits calculated from Phase-I. If new {monitoring statistics} significantly falls outside of control limits for a long sequence or there are some obvious deviation pattern from the normal ($0$ in this case), the alarm of concept drift is set off.

% \begin{figure}[!htbp]
% \centering
%  \begin{subfigure}[t]{0.6\linewidth}
%          \centering
%          \includegraphics[width = 1\linewidth, trim=.35in .69in .35in .69in, clip]{../figures/v14/flow_chart/Retrospective_1.png}
%          \caption{Retrospective Analysis.}
%          \label{fig:retro_analysis}
%   \end{subfigure}
%   \begin{subfigure}[t]{0.6\linewidth}
%          \centering
%          \includegraphics[width = 1\linewidth, trim=.35in .49in .35in .49in, clip]{../figures/v14/flow_chart/Monitoring_1.png}
%          \caption{Monitoring.}
%          \label{fig:Monitoring}
%   \end{subfigure}
%   \begin{subfigure}[t]{0.6\linewidth}
%          \centering
%          \includegraphics[width = 1\linewidth, trim=.35in .44in .35in .44in, clip]{../figures/v14/flow_chart/Diagnose_1.png}
%          \caption{Diagnosis.}
%          \label{fig:diagnosis}
%   \end{subfigure}
%   \caption{{The framework of monitoring/detecting concept drift based on the score function. (a) Conducting retrospective analysis using MEWMA and/or score function clustering to ensure there is no significant concept drift in the data set used to train and set control limits. The size of data set can be recursively reduced if significant concept drift exists. (b) Monitoring concept drifts using the model and control limits obtained by processing training and Phase-I data sets, which will be demonstrated in Section~\ref{s:demon_cd} and~\ref{s:real_data}. In this subplot, three examples of possible results are given: gradual and abrupt concept drift and in-control cases. (c) Visualization of diagnosing concept drift: The MEWMA for score vectors and the EWMA for individual predictors are visualized to show the origin of concept drift, which will be illustrated in Section~\ref{s:demon_cd} and~\ref{s:real_data}.}}
%   \label{fig:proc_mon_score}
% \end{figure}


\begin{figure}[!htbp]
\centering
\includegraphics[width = 1\linewidth, trim=.35in .69in .35in .69in, clip]{../figures/v14/flow_chart/Retrospective_1.png}
\caption{The framework of monitoring/detecting concept drift based on the score function. Conducting retrospective analysis using MEWMA and/or score function clustering to ensure there is no significant concept drift in the data set used to train and set control limits. The size of data set can be recursively reduced if significant concept drift exists.}
  \label{fig:proc_mon_score_retro}
\end{figure}

\begin{figure}
\centering
\includegraphics[width = 1\linewidth, trim=.35in .49in .35in .49in, clip]{../figures/v14/flow_chart/Monitoring_1.png}
\caption{The framework of monitoring/detecting concept drift based on the score function. Monitoring concept drifts using the model and control limits obtained by processing training and Phase-I data sets, which will be demonstrated in Section~\ref{s:demon_cd} and~\ref{s:real_data}. In this subplot, three examples of possible results are given: gradual and abrupt concept drift and in-control cases.}
\label{fig:proc_mon_score_monitoring}
\end{figure}

\begin{figure}
\centering
\includegraphics[width = 1\linewidth, trim=.35in .44in .35in .44in, clip]{../figures/v14/flow_chart/Diagnose_1.png}
\caption{The framework of monitoring/detecting concept drift based on the score function. Visualization of diagnosing concept drift: The MEWMA for score vectors and the EWMA for individual predictors are visualized to show the origin of concept drift, which will be illustrated in Sections ~\ref{s:decou_cd},~\ref{s:demon_cd}, and~\ref{s:real_data}.}
\label{fig:proc_mon_score_diagnosis}
\end{figure}

The process of monitoring the score function using MEWMA control chart is shown in Figures~\ref{fig:proc_mon_score_retro},~\ref{fig:proc_mon_score_monitoring}, and~\ref{fig:proc_mon_score_diagnosis}. For other scalar metrics or transformed components of the score function (see Section~\ref{s:decou_cd}), we use univariate EWMA~(\cite{roberts1959control}) control chart because the statistics calculated by Equation~(\ref{eqn:ewma}) becomes scalar. The main difference is that in MEWMA control chart the monitored statistics is a summary statistics of vectors and is positive, but in EWMA control chart the statistics can fluctuate to both sides of zero. Other monitoring steps are similar. More specifically, in the Step 1, we collect a batch of responses and covariates in time order, $\{\bm {x}_i, y_i\} _{i=1} ^{n}$, where $n$ is the sample size. Then, the retrospective analysis using MEWMA or score clustering is conducted to rule out concept drift as much as possible, by deleting part of the data set that is not stationary as shown in Figure~\ref{fig:proc_mon_score_retro}. After that, the data set without concept drift is divided into two parts: $ \mathcal{D}_1 \vcentcolon= \{\bm {x}_i, y_i\} _{i=1} ^{n_1}$ and $\mathcal{D}_2 \vcentcolon= \{\bm {x}_i, y_i\} _{i=n_1+1} ^{n}$. The first data set, $\mathcal{D}_1$, is used to train a parametric model and the second one, $\mathcal{D}_2$, is used in Phase-I for monitoring statistics and calculating control limits ($UCL$ and lower control limits($LCL$)). The Step 1 can iterate recursively by reducing the size of the data set, $n$, to ensure that the control chart in Phase-I detects no significant out-of-control data and trustworthy control limits are obtained. In the Step 2, given control limits from Phase-I, in Phase-II, the control chart is applied to sample score vectors or other metrics of testing data points $\tilde{\mathcal{D}}\vcentcolon=\{\tilde{\bm{x}}_i,\tilde{y}_i\}_{i = 1}^{\tilde{n}}$ to signal the starting position of a concept drift, if it should happen, as shown in Figure~\ref{fig:proc_mon_score_monitoring}. The Phase-II analysis can be used in data exploration when all the data are available and the existence and starting position of concept drift is interested in; or in prediction when data points come one-by-one and the aim is to monitor concept drifts. Finally, in the Step 3, we can diagnose the origin of concept drifts if detected as shown in Figure~\ref{fig:proc_mon_score_diagnosis}, which will be explained in details in Section~\ref{s:decou_cd}.

Notice that the EWMA is obtained first, followed by Hotelling $T^2$ calculation. The advantage of this is that the EWMA would reduce the random noise in raw score vectors, so that in Phase-II of detection using Hotelling $T^2$, small drifts would be easier to be detected due to higher signal-to-noise ratio. Because we are interested in applications with a large size of data sets, the usage of empirical control limits of the MEWMA control chart based on the Phase-I data is reasonable.

\begin{figure}[!htbp]
\centering
\begin{subfigure}[t]{0.49\linewidth}
     \centering
     \includegraphics[width=\textwidth, trim=.2in .2in .7in .45in, clip]{../figures/v14/demons_fig/2D_err_logi_trunc_norm.png}
     \captionsetup{width=.95\linewidth}
     \caption{The error rate function, Equation~(\ref{eqn:logi_err_rate}), before and after concept drift by monitoring error. The drifted data generating process decreases the unconditional error rate by those blue shaded area but adds the red shaded area as new error. Because the two areas are equal, the expected error rate remains the same. If we only monitor the error rate or any metrics derived from it, the concept drift would be missed.}
     \label{fig:logi_err_rate_penal}
\end{subfigure}
\begin{subfigure}[t]{0.49\linewidth}
     \centering
    \includegraphics[width = \textwidth, trim=.15in .2in .7in .45in, clip]{../figures/v14/demons_fig/2D_score_logi_modi_trunc_norm_0_7.png}
     \captionsetup{width=.95\linewidth}
     \caption{The the expected Hotelling $T^2$, Equation~(\ref{eqn:logi_dev_rate}), before and after concept drift by monitoring Hotelling $T^2$ of EWMA of the score function. The drifted data generating process has the increased area (red) larger than the decreased area (blue) resulting in net positive change in the expected the expected Hotelling $T^2$ of the score function, which indicates that it is more sensitive for monitoring concept drift than error-based methods.}
     \label{fig:logi_score_rate_penal}
\end{subfigure}
  \caption{The comparison of penalty functions by monitoring classification error and Hotelling $T^2$ of EWMA of the score function for the logistic regression model.}
  \label{fig:logi_med_penal}
\end{figure}

Here, we revisit the illustrative example of simple logistic regression introduced in Section~\ref{ss:score_func} to complete the discussion on how we use MEWMA to resolve the limitation error-based methods have. According to Equation~(\ref{eqn:logi_mod_score}), the Hotelling $T^2$ for the logistic regression model can be written as:
\begin{align}
T_t^2 = (Y_t-p^{(0)}(X_t))^2 \bm{X}_t^T\hat {\bm { \Sigma}} ^{(0)-1}\bm{X}_t
\label{eqn:logi_hotellingt2}
\end{align}
where the notations follow Equations~(\ref{eqn:logi_mod_score}) and~(\ref{eqn:simp_nota_p}). Similarly to Equation~(\ref{eqn:logi_err_rate}), we can take expectation to the quantity above to evaluate the change of monitoring statistics before and after concept drift, which we refer to as the expected Hotelling $T^2$. Notice that here we substitute $\bm{\Sigma}^{(0)}=E[p^{(0)}(X)(1-p^{(0)}(X))\bm{XX}^T]$ for the estimated covariance $\hat{\bm{\Sigma}}^{(0)}$ in Equation~(\ref{eqn:hotellingt2}) to obtain the following equations to simplify the analysis. 
% \begin{align}
% \begin{aligned}
% dev(x;\bm{\theta}^{(j)})\vcentcolon=&E_{\bm{\theta}^{(j)}}[T^2|\bm{X}=[1,x]^T] \\
% =& (p^{(j)}(x)-2p^{(j)}(x)p^{(0)}(x)+p^{(0)2}(x)) \bm{x}^T \bm { \Sigma}^{(0)-1}\bm{x} \\
% dev(\bm{\theta}^{(j)})\vcentcolon=&E[dev(X;\bm{\theta}^{(j)})]
% \end{aligned}
% \label{eqn:logi_dev_rate}
% \end{align}
\begin{align}
\begin{aligned}
&E_{\bm{\theta}^{(j)}}[T^2|\bm{X}=[1,X]^T]
= (p^{(j)}(X)-2p^{(j)}(X)p^{(0)}(X)+p^{(0)2}(X)) \bm{X}^T \bm { \Sigma}^{(0)-1}\bm{X} \\
&E_{\bm{\theta}^{(j)}}[T^2] = E[E_{\bm{\theta}^{(j)}}[T^2|\bm{X}=[1,X]^T]] = \int E_{\bm{\theta}^{(j)}}[T^2|\bm{X}=[1,x]^T]q(x)dx
\end{aligned}
\label{eqn:logi_dev_rate}
\end{align}
After concept drift, assuming that the covariance matrix does not change much before and after concept drift ($\bm{\Sigma}^{(1)}\approx\bm{\Sigma}^{(0)}$), the the expected Hotelling $T^2$ can be decomposed as $E_{\bm{\theta}^{(1)}}[T^2]=E[Tr(p^{(1)}(1-p^{(1)})\bm{XX}^T\bm{\Sigma}^{(0)-1})]+E[(p^{(1)}-p^{(0)})^2\bm{X}^T\bm{\Sigma}^{(0)-1}\bm{X}]=E[Tr(\bm{\Sigma}^{(1)}\bm{\Sigma}^{(0)-1})]+E[(p^{(1)}-p^{(0)})^2\bm{X}^T\bm{\Sigma}^{(0)-1}\bm{X}]\approx E_{\bm{\theta}^{(0)}}[T^2]\\+E[(p^{(1)}-p^{(0)})^2\bm{X}^T\bm{\Sigma}^{(0)-1}\bm{X}]\gtrapprox E_{\bm{\theta}^{(0)}}[T^2]$, where $Tr(\cdot)$ is the trace of a matrix. This can be visualized in Figure~\ref{fig:logi_med_penal}. The shaded areas under curves represent the monitoring statistics. As shown in Figure~\ref{fig:logi_err_rate_penal} of the error rate of the error-based method, it does not change before and after concept drift (the difference between Figures~\ref{fig:logi_err_rate_unch_a}/\ref{fig:logi_err_rate_unch_b} and Figure~\ref{fig:logi_err_rate_penal} is that here we change the vertical axis label from $P(Y=1|X=x;\bm{\theta})$ to $E_{\bm{\theta}}[I(Y\neq \hat{y})|X=x]$ for ease of comparison with the score-based method); while in Figure~\ref{fig:logi_score_rate_penal} of the expected Hotelling $T^2$ of the score-based method, it increases after concept drift, which would be detected using our MEWMA monitoring method. After detection, the model can be updated to improve the performance as in Figure~\ref{fig:logi_err_rate_unch_d}. In Section~\ref{s:demon_cd}, a numerical example would be presented to further support the analysis here.

% Why we want it to be normally distributed?
After chosen a EWMA parameter $\lambda$, a better method should have shorter delay in detecting concept drift. Intuitively, the score function is the first order gradient of the negative log-likelihood so that monitoring the score function is more sensitive than directly monitoring the negative log-likelihood. That is because in training a complex model, a learning rate of small number will be multiplied on the gradient to update parameters so that a score with large deviation from zero mean can still slowly progress in updating parameters and thus the error rate or residual. To quantitatively evaluate the delay of any specific monitoring statistics, we conduct a Monte Carlo simulation to calculate median run length when data are in-control ($MRL_0$) and out-of-control ($MRL_1$). The median run length is defined as ``the median number of points that must be monitored before a point indicates an out-of-control condition"~(\cite{montgomery2007introduction}). A good statistics should have a long $MRL_0$ to minimize false positive rate and short $MRL_1$ to reduce the time delay for detection. In literature, average run length (ARL) is also common. However, it is less robust to the heavy-tailed distribution of run length than $MRL$. Details about simulation setups can be found in Appendix~\ref{ss:simu_MRL}. 

% \subsection{Implementation of Monitoring the Score Function and Other Metrics}
% \label{ss:MEWMA}

\subsection{Handling High-Dimensional and Regularized Models}
\label{ss:high_dim_score}
One of the advancement of machine learning is that models become increasingly complex. Some state-of-the-art models can have millions of parameters, like convolutional neural network. With such high-dimension of parameters, the sample covariance matrix in Equation~(\ref{eqn:hotellingt2}), $\hat {\bm { \Sigma}}$, is very likely to be close to singular. For example, when the sample size of our training data (denoted as $n_1$ as in Section~\ref{ss:MEWMA}) is smaller than the dimension of parameters, $dim(\bm { \theta})$, the sample covariance would be singular. 

To solve this problem, we can add a nugget parameter on all diagonal entries of $\hat {\bm { \Sigma}}$ or use pseudo-inverse of the sample covariance. For the method of adding a nugget parameter $ \delta$, we substitute $\tilde {\bm { \Sigma}} = \hat {\bm { \Sigma}}+ \delta \bm {I}$ for $\hat {\bm { \Sigma}}$ as the approximated covariance matrix. To understand the effect of this nugget parameter, denote the eigen-decomposition of the sample covariance matrix as $\hat {\bm { \Sigma}} = \bm {Q}\bm { \Lambda} \bm {Q}^T$ and obtain the eigen-values as $ diag(\bm{\Lambda}) = [ \lambda_1, \lambda_2,\cdots, \lambda_n]$ in a non-increasing order. Then, we can write the approximated sample covariance matrix as $\tilde {\bm { \Sigma}} = \bm {Q}\tilde{\bm { \Lambda}} \bm {Q}^T$, where $\tilde{\bm { \Lambda}} = \bm { \Lambda} + \delta \bm {I}$. The EWMA of the new statistics with this modified covariance matrix in Equation~(\ref{eqn:hotellingt2}) would not have the issue of ill-conditioning by suppressing unimportant directions of variation. 

Setting a condition number achieves a similar purpose. By setting a maximum condition number, $ \gamma$, we set all $ \lambda_i$ equal to $0$, if $ \gamma \lambda_i \leq \lambda_1$. Denote the maximum of the index of $ \lambda_i$ which is not set to $0$ as $k$ and a new diagonal matrix $\bm { \Lambda} ^{-}$ with diagonal entries as $[1/\lambda_1,1/\lambda_2, \cdots, 1/\lambda_k, 0, \cdots, 0]$. Then, a pseudo-inverse of the sample covariance matrix is defined as $\hat {\bm { \Sigma}} ^{-} = \bm {Q}\bm { \Lambda}^{-}\bm {Q}^T$. This is equivalent to applying PCA onto $\bm {z}_t$, so that the most important variations in $\bm {z}_t$ are kept. 

Comparing with adding a nugget parameter, even though both methods give similar results, setting the maximum condition number is more intuitive in terms of controlling the behavior of inverting the covariance matrix, while adding the nugget allows the concept drift to be detected in those directions, that would be otherwise set to $0$ in hard thresholding by setting the maximum condition number. So we choose to add a nugget parameter.

% \subsection{Score function of Regularized Models}
% \label{ss:score_regu}
Another related issue is regularization of complex models, which is almost always required to combat overfitting. The regularization term is used to penalize complex models and large parameters, by minimizing the negative log-likelihood plus a norm of parameters, for example, $L_2$ norm of all parameters: $l(\bm{\theta})=-\frac{1}{n}\sum_{i=1}^n \log P(y_i|\bm{x}_i;\bm{\theta})+\frac{c}{2}||\bm{\theta}||_2^2$, where $c>0$ is a regularization parameter. The regularization term would change the score function of the original model: $\nabla_{\bm{\theta}}l(\bm{\theta}) = -\frac{1}{n}\sum_{i=1}^n\bm{s}(\bm{\theta};(\bm{x}_i,y_i))+c\bm{\theta}$. It can be looked as a prior on parameters from Bayesian perspective, which can be ``distributed" among all data: $\nabla_{\bm{\theta}}l(\bm{\theta})=-\frac{1}{n}\sum_{i=1}^n(\bm{s}(\bm{\theta};(\bm{x}_i,y_i))-c\bm{\theta})=-\frac{1}{n}\nabla_{\bm{\theta}}\sum_{i=1}^n\log(P(y_i|\bm{x}_i;\bm{\theta})\exp\{-\frac{c}{2}||\bm{\theta}||_{2}^2\})$. We can treat the likelihood times prior as the new model: $P(Y|\bm{X};\bm{\theta})\exp\{-\frac{c}{2}||\bm{\theta}||_{2}^2\}$. Then, all methods of calculating score vectors and applying EWMA and Hotelling $T^2$ follows. In some popular form of penalization, this would not change the monitoring statistics. For example, adding a $L_2$ regularization term of all parameters would only add a constant vector to all score vectors: $\bm{s}(\bm{\theta};(\bm{x}_i,y_i))-c\bm{\theta}$, which would be canceled after minus the mean of score vectors in Equation~(\ref{eqn:hotellingt2}). Adding regularization would have another good effect. When $dim ( \bm { \theta})$ is too large, $L_2$ penalization would add a diagonal matrix with positive entries to the sample covariance matrix: $-\nabla_{\bm{\theta}}(\bm{s}(\bm{\theta};(\bm{x}_i,y_i))-c\bm{\theta})=-\nabla_{\bm{\theta}}\bm{s}(\bm{\theta};(\bm{x}_i,y_i))+c\bm{I}=\mathbf{I}(\bm{\theta})+c\bm{I}$, where $\mathbf{I}$ and $\bm{I}$ are Fishier information and identity matrix respectively, automatically resulting in a well-conditioned matrix. 

\section{Decoupling of Concept Drift for Multivariate Regression and Classification}
\label{s:decou_cd}
After detecting the concept drift, we want to pinpoint which {covariates} have significant impact on the change of {the} conditional distribution {$P(Y| \bm {X}, \bm{\theta})$}. This can provide interpretability for concept drift for updating or fixing models. In some industry (e.g. financial, insurance, health care), model interpretability is valued or even required by laws. For example, in financial industry, there are many laws to ensure that some key business processes such as determining who qualifies for lines of credit must comply with fair lending laws~(\cite{chen2018fair}) such as the Equal Credit Opportunity Act (ECOA)~(\cite{hsia1978credit}). Ideally, after decoupling, univariate control charts should truly reflect whether covariates have concept drifts or not. However, even for those covariates without concept drifts, the variance would increase due to variance of $\bm {X}$. More detailed analysis will follow in this section.

\subsection{Concept Drift Decoupling}
\label{ss:diagnosis}
To detect which covariates have concept drift, naively monitoring component-wise mean {drift} of the score function would fail when {covariates considerably correlate with each other} or nonlinear predictive models are used. To demonstrate this, look at the linear model and logistic regression. Assuming the vector of the true parameters {before concept drift $\bm { \theta}^ { (0)}$} is known (we ignore the difference between the true parameter, $\bm { \theta}^ { (0)}$, and the estimator of it, $\hat{\bm { \theta}}^ { (0)}$, as discussed in Section~\ref{ss:sgd_score}), the linear regression and the corresponding score function imply that {$E_{\bm{ \theta}^{ (0)}}[(Y - \bm {X}^T\bm { \theta}^{ (0)} ) \bm {X}|\bm {X}]=0$}. Note the notation that, the subscript, ${ \bm{\theta}}^{ (0)}$, of the expectation is the vector of parameters of the underlying distribution the expectation is taken with respect to. After concept drift, {assuming the parameter $\bm { \theta}^{ (0)}$ changes to $\bm { \theta} ^{ (1)}$, denote the change in the vector of parameters as $ \Delta \bm { \theta} = \bm { \theta} ^{ (1)} - \bm { \theta}^ { (0)}$ for the rest context.} {The} joint expectation for the score function is $E [\bm {X}\bm {X}^T] \Delta \bm { \theta}$, where the concept drift only decouples when $E [\bm {X}\bm {X}^T]$ is a diagonal matrix, meaning all components of $\bm{X}$ are uncorrelated. For the logistic regression, the expectation of the score function, $E[(\sigma ( \bm {  \bm {X}^T \theta}^{ (1)}) - \sigma ( \bm {X}^T\bm { \theta}^{ (0)} )) \bm {X}]$, no longer has a clean format, because it depends on the unknown parameter vector, $\bm { \theta} ^{ (1)}$, after concept drift. In the next section, results from simulated data show that this mean {drift} is not decoupled even when $X _{i} (i = 1,2, \cdots, p)$ are uncorrelated. 

To decouple the interleaved drifts of parameters, for the linear regression, we can simply premultiply sample score vectors by the {the inverse of the estimated} $E [\bm {X}\bm {X}^T]$, because this matrix does not depend on the actual drift in parameters (otherwise, it is generally hard to estimate when concept drift starts). However, this is not universally viable for more complex models, like in the logistic regression or neural networks. In general, the mean {drift} of the score function after concept drift is
\begin{align}
\begin{aligned}
E _{\bm { \theta}^{ (1)}}[\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y))] 
= & E[E _{\bm { \theta}^{ (1)}}[\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y))| \bm {X}] ] \\
= & E[\int \bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, y)) P(y | \bm {X}, \bm{\theta} ^{ (1)}) dy ]
\end{aligned}
\label{eqn:cd_mean_shift}
\end{align}
Apply Taylor expansion on the $\bm{s}(\bm { \theta}^{ (0)}; (\bm {x}, y))$ around $\bm { \theta} ^{ (1)}$
\begin{align}
\begin{aligned}
 &\bm{s}(\bm { \theta}^{ (0)}; (\bm {x}, y)) = \bm{s}(\bm { \theta}^{ (1)}; (\bm {x}, y)) 
 + \nabla _{\bm { \theta}}{ \bm{s}(\bm { \theta}^{ (1)}; (\bm {x}, y))}(\bm { \theta}^ { (0)} - \bm { \theta} ^{ (1)}) 
 + o(\bm { \theta}^ { (0)} - \bm { \theta} ^{ (1)} ) 
\end{aligned}
\label{eqn:sc_ty_expa}
\end{align}
Plug in Equation~(\ref{eqn:sc_ty_expa}) into~(\ref{eqn:cd_mean_shift}) and use the property of the score function for $\bm { \theta} ^{ (1)}$.
\begin{align}
\begin{aligned}
& E _{\bm { \theta}^{ (1)}}[\bm{s}(\bm { \theta}^{ (0)}; (\bm {X}, Y))] \\
= & E[\int \bm{s}(\bm { \theta}^{ (1)}; (\bm {X}, y))P (y| \bm {X}, \bm{\theta}^{ (1)}) d y ] \\ 
 +&  E[\int [  \nabla _{\bm { \theta}}{ \bm{s}(\bm { \theta}^{ (1)}; (\bm {X}, y))}(\bm { \theta}^ { (0)} - \bm { \theta} ^{ (1)}) 
 + o(\bm { \theta}^ { (0)} - \bm { \theta} ^{ (1)} ) ] P (y| \bm {X}, \bm{\theta}^{ (1)}) d y]\\ 
= & E[E _{\bm { \theta}^{ (1)}}[\bm{s}(\bm { \theta}^{ (1)}; (\bm {X}, Y))| \bm {X}]] \\
+ & E[\int [ \nabla_{\bm { \theta}} \nabla^T _{\bm { \theta}}{ \log P(y|\bm{X};\bm { \theta}^{ (1)})}(\bm { \theta}^ { (0)} - \bm { \theta} ^{ (1)})
+ o(\bm { \theta}^ { (0)}- \bm { \theta}^{ (1)})]P (y| \bm {X}, \bm{\theta}^{ (1)}) dy] \\
= & \bm{0} -  \{E[E _{\bm { \theta}^{ (1)}}[\nabla_{\bm { \theta}} \nabla ^T_{\bm { \theta}}{ \log{P}(Y|\bm{X};\bm { \theta}^{ (1)})} | \bm {X}]] + o(1)\}(\bm { \theta}^{ (1)} - \bm { \theta}^ { (0)}) \\
= & [{\mathbf {I}}(\bm { \theta}^{ (1)})+ o(1)]\Delta\bm{ \theta} \\
= & [{\mathbf {I}}(\bm { \theta}^{ (0)})+ o(1)]\Delta\bm{ \theta} \\
\end{aligned}
\label{eqn:cd_decomp_fisher_approx}
\end{align}
where {${\mathbf {I}}(\bm { \theta}^{ (j)})=-E[E _{\bm { \theta}^{ (j)}}[\nabla_{\bm { \theta}} \nabla^T _{\bm { \theta}}{ \log{P}(Y|\bm{X};\bm { \theta}^{ (j)})} | \bm {X}]],j\in\{0,1\}$} is the (expected) Fisher Information Matrix {at parameter $\bm { \theta} ^{ (j)}$} and $o(\cdot)$ represents asymptotically negligible quantity comparing with the argument. Here, we use the fact that the expectation of the score function is $\bm {0}$ when {$\bm { \theta}^{ (1)}$} is true and assume $\Delta\bm{ \theta}$ small. To approximately decouple concept drift, we can premultiply sample score vectors by {the} inverse of {the} estimated {${\mathbf {I}}(\hat{\bm { \theta}}^{ (0)})$}, or the sample covariance matrix of {the score function.}

\subsection{Variance Inflation for Covariates without Concept Drift}
\label{ss:var_infla}
{Notice that the expectation in Equation~(\ref{eqn:score_exp_zero}) is with respect to the conditional distribution, but the sample score vector, {$\bm{s} (\bm { \theta} ^{ (0)};(\bm {x}_i, y_i))$}, has randomness from not only $P_{\bm {\theta}} (Y|\bm {X};\bm{\theta}^{(0)})$ (where $\bm { \theta} = \bm { \theta}^{(0)}$ when no concept drift) but also $P (\bm {X})$. In other words, monitoring mean {drift} of {$\bm{s} (\bm { \theta}^{ (0)};(\bm {x}_i, y_i))$} using control charts is practically implemented by monitoring drift of {$E _{ \bm { \theta}}[\bm{s} (\bm { \theta}^{ (0)};(\bm {X}, Y))]$}.

To understand the relation, using an iterative expectation, we have}
\begin{align}
E_{ \bm { \theta} }[\bm{s} (\bm { \theta} ^{ (0)};(\bm {X}, Y))] = E [E _{ \bm { \theta} }[\bm{s} (\bm { \theta} ^{ (0)};(\bm {X}, Y))|\bm {X}]]
\label{eqn:joint_expe}
\end{align}
{According to the definition, a concept drift corresponds to change in $E _{ \bm { \theta} }[\bm{s} (\bm { \theta} ^{ (0)};(\bm {X}, Y))| \bm {X}]$ from $\bm{0}$, because this indicates the change in $P(Y|\bm {X};\bm{\theta})$. In practice, we usually can not directly monitor $E _{ \bm { \theta} }[\bm{s} (\bm { \theta} ^{ (0)};(\bm {X}, Y))| \bm {X}]$, because of the lack of control in randomness from $\bm {X}$. Instead, we actually monitor $E_{ \bm { \theta} }[\bm{s} (\bm { \theta} ^{ (0)};(\bm {X}, Y))]$. This actually makes sense, because the {drift} of the left-hand-side in Equation~(\ref{eqn:joint_expe}) implies the {drift} of the inner expectation of the right-hand-side, meaning concept drift; the reverse is generally true except that {$E _{ \bm { \theta} }[\bm{s} (\bm { \theta} ^{ (0)};(\bm {X}, Y))| \bm {X}]$} has a specific form and non-zero values at different realizations of $\bm {X}$ cancel out after taking an expectation with respect to $\bm {X}$. As shown in Section~\ref{ss:score_func}, under assumptions of small changes of and smooth conditions with respect to the parameter, for generalized linear models, concept drift in parametric models implies non-zero mean of the score function.}

As mentioned before, the univariate score function corresponding to those covariates without concept drifts will have zero mean in univariate control charts, but the variance would be inflated if concept drifts exist in other covariates so that the monitored statistics will still fall outside of control limits. Because of the clear pattern without mean drifts, components corresponding to these covariates should not be included as candidates of origins of concept drifts, and after recovering those covariates from concept drifts as required by some applications, the inflated variance in control charts should go back to normal. 

To give an example here, the linear regression is used to show the origin of variance inflation. For a linear regression model, assuming that the data generation process after training phase is $y' = \bm {x}^T\bm { \theta}^{ (1)} + \epsilon$, the variance of the $k$th component of the score function after the concept drift is

\begin{align}
\begin{aligned}
&Var _{ \bm{ \theta}^{(1)}}[ (Y'- \bm {X}^T \bm{\theta}^{(0)}  ) X_{k}]   \\
%= & E  [Var  _{ \bm{ \theta}^{(1)}}[(Y' - \mu ) X_{k}]] + Var  [E  _{ \bm{ \theta}^{(1)}}[(Y' - \mu ) X_{k}]]   \\
%= & 
= & E _{\bm{ \theta}^{(1)}} [(\bm {X}^T \bm{\theta}^{(1)}+\epsilon -  \bm {X}^T \bm{\theta}^{(0)}  )^2 X_{k}^2] - E _{\bm{ \theta}^{(1)}}^2 [(\bm {X}^T \bm{\theta}^{(1)}+\epsilon - \bm {X}^T \bm{\theta}^{(0)}) X_{k}]   \\
= & E   [E _{\bm{ \theta}^{(1)}}[(\bm {X}^T\Delta \bm { \theta}  +\epsilon)^2|\bm {X}] X_{k}^2] - \Delta \bm{\theta}^T E   [\bm {X}  X_{k}]E   [\bm {X} ^T X_{k}] \Delta \bm { \theta}    \\
= & E   [ (\Delta \bm { \theta}^T \bm {X} \bm {X} ^T \Delta \bm { \theta} + \sigma^2)X_{k}^2] - \Delta \bm{\theta}^T E   [\bm {X}  X_{k}]E   [\bm {X} ^T X_{k}] \Delta \bm { \theta}    \\
= & \sigma^2 \sigma_{x,k}^2 + \Delta \bm{\theta}^TVar [\bm {X}  X_{k}] \Delta \bm{\theta}
\end{aligned}
\label{eqn:var_aft_cd}
\end{align}
where $ \sigma$ is the variance of random noise and $ \sigma _{x,k}$ is the variance of the $k$th component of the covariates. Clearly, $\Delta \bm{\theta}^TVar [\bm {X}  X_{k}]\Delta \bm{\theta}$ is generally non-zero if $ \Delta \bm { \theta} \neq \bm {0}$, resulting in variance inflation for covariates without concept drift. Notice that because the variance of monitored statistics in control charts depends on the covariates, ``covariate drift" (meaning change in the distribution of $\bm {X}$) will also affect the variance in the control charts, but not mean. In this study, we assume no covariate drift for simplicity.

\section{Summary of Simulation Comparison Results}
\label{s:demon_cd}
The concept drift in real data sets is notoriously hard to testify unless there is some strong prior or posterior knowledge on when drifts happened. To test the properties mentioned in Section~\ref{s:theory_analysis_score} and~\ref{s:decou_cd}, we first use simulated data sets. Here, we focus on simple but commonly used models like linear model, logistic regression, multinomial regression, poisson regression, and MLP, for the simplicity of proof-of-concept and analytical explanation. We expect that those properties can be extended to more complex models, because our previous analyses on the score function are mostly model and distribution agnostic. 

Several experiments are shown the efficacy and properties of score-based method and its superior performance comparing with other metrics, in the appendix. The results are summarized and highlighted in this section. First in Section~\ref{ss:cd_no_err_change}, simulations shown that concept drift can result in no change in expected error (thus no detection) on control charts, but large mean drift in the score function. Then in Appendix~\ref{ss:simu_MRL}, experiments of Monte Carlo simulation are conducted to calculate Median Run Length ($MRL$) for both in-control and out-of-control regions, showing that the score-based method has higher sensitivity in detection. At the last in Appendix~\ref{ss:cd_diag}, data sets with partially correlated covariates are simulated to show that the score-based method can decouple the concept drift, with proper transformation.

We also simulate abrupt and gradual concept drifts to test the performance of detecting and tracking concept drifts by monitoring the score function, as well as other metrics like EWMA of the prediction error or absolute residual. Those results show that monitoring the score function is a better and more versatile method in dealing with concept drift problems. 

\subsection{Concept Drifts with No Error Rate Change}
\label{ss:cd_no_err_change}
\begin{figure}[!htp]
\centering
\begin{subfigure}[t]{0.49\linewidth}
         \centering
           \includegraphics[width = \linewidth]{../figures/v14/sim_11/non_nnet_nonunif_ch_f_0_2/1_sim11_logi_1e-08_0_0015_1.png}
         \captionsetup{width=.95\linewidth}
         \caption{Concept drift results in mean change in score-based method but no mean change in error rate.}
         \label{fig:exp_no_err_ch_a}
  \end{subfigure}
\begin{subfigure}[t]{0.49\linewidth}
         \centering
	       \includegraphics[width = \linewidth]{../figures/v14/sim_11/non_nnet_nonunif_ch_f_0_2_followup/1_sim11_logi_1e-08_0_0015_1.png}
         \captionsetup{width=.95\linewidth}
         \caption{After retraining on the drifted data set, the performance of model increased, according to the lower control limit in prediction error.}
         \label{fig:exp_no_err_ch_b}
  \end{subfigure}
  \caption{A logistic regression example with two covariates, where before blue vertical line is Phase-I and the green vertical line is the boundary between before and after concept drift in the Phase-II. This example shows that our score-based method can detect concept drift which has no change in error. This detection provides an opportunity for improvement as shown in the lower limits on the right plots, which is consistent with Figure~\ref{fig:logi_err_rate_unch}.}
  \label{fig:exp_no_err_ch}
\end{figure}
In Sections~\ref{ss:score_func} and~\ref{ss:MEWMA}, simple logistic regression has been shown that concept drift may not result in mean change in error rate which defeats the purpose of detecting and monitoring it for concept drift, while concept drifts always change the mean of the score function. Here, for a simple demonstration, we simulate a data set drawn from Bernoulli distribution with two covariates. The concept drift shown in Figure~\ref{fig:exp_no_err_ch} is similar to Figure~\ref{fig:logi_err_rate_unch}. As shown in Figure~\ref{fig:logi_err_rate_unch_a}, prediction error rate has no significant signal for successful detection on concept drift (Figure~\ref{fig:exp_no_err_ch_a}). However, the score-based method shows large deviation (Figure~\ref{fig:exp_no_err_ch_a}). After retraining a model over the drifted data set, the results shown in Figures~\ref{fig:logi_err_rate_unch_d} and~\ref{fig:exp_no_err_ch_b} indicate that the accuracy has been increased because of the lower control limit on testing prediction error rate. This example directly proves that our score-based method has advantages over the error-based method. 

\subsection{Summary of Simulations on Median Run-Length ($MRL$) and Concept Drift Diagnoses}
\label{ss:summ_simu}
In order to compare how quickly a method can detect concept drifts for different models, like linear model, logistic regression, multinomial regression, poisson regression, and MLP, we artificially generated data sets for them and repeated the examples for many times. Then, we calculated $MRL$ as explained in Section~\ref{ss:MEWMA} (refer to Appendix~\ref{ss:simu_MRL} for details of experiments). According to results of experiments, the score-based method would have longer $MRL_1$ than the error-based method, given that they have similar $MRL_0$, which indicates that the score-based method is quicker in detecting concept drift. 

We also use simulation to demonstrate the diagnosis capability of the score-based method. We focus on the linear regression and logistic regression for proof-of-concept here, to show the challenges and resolutions. For the linear regression, EWMA control charts of components of transformed score vectors as described in Section~\ref{s:decou_cd} would correctly show whether or not covariates have concept drift, even when multicolinearity exists among covariates, as shown in Figure~\ref{fig:lin_reg_not_ind_X}. For the logistic regression, the nonlinearity of the sigmoid function would further complicate the diagnosis problem. Even for uncorrelated covariates, concept drifts of some covariates would falsely show up in EWMA control charts of those without concept drift, as shown in Figure~\ref{fig:log_reg_ind_X}. However, if we assume the argument of the sigmoid function $\bm{x}^T\bm{\theta}$ is small enough (i.e. $\ll 4$), Equation~(\ref{eqn:cd_decomp_fisher_approx}) approximately holds and we can use the Fisher information matrix to decouple the concept drift as done in the linear regression, as shown in Figure~\ref{fig:log_reg_not_ind_X_1}. We argue that this assumption about the argument $\bm{x}^T\bm{\theta}$ is practical valuable. When $\bm{x}^T\bm{\theta}$ is large the model is very confident with predictions (the sigmoid function is in plateau regions) and a small concept drift would not make much difference in the overall prediction accuracy. However, when $\bm{x}^T\bm{\theta}$ is small the data are most confusing for the model and small concept drift can have big impact on prediction error, so that people may want to understand the origins of concept drift. One thing worth highlighting is that we obtained consistent results across experimental data sets with abrupt and gradual concept drifts, which further justify the capability of the score-based method in different parametric models (Figures~\ref{fig:lin_reg_not_ind_X_grad_cd},~\ref{fig:lin_reg_ind_X_grad_cd_comp},~\ref{fig:log_reg_not_ind_X_grad_cd},~\ref{fig:log_reg_ind_X_grad_cd_comp}). Details of experiments and analyses can be traced in Appendix~\ref{ss:cd_diag}.

\section{Real Data Examples}
\label{s:real_data}
Concept drifts are usually hard to directly testify in real life, because the existence of concept drift results from some hidden effects or contexts that are not available beforehand. However, for the purpose of testing our methods, we choose two examples that has generally accepted posterior that can be used as evidence of concept drifts. One is a data set of credit default from a major financial company from $2003$ to $2008$ during which the subprime mortgage crisis happened. And the other is a data set of Capital Bikeshare from $2010$ to $2020$ during which ``sharing economy" is expanding rapidly. Those two data sets correspond to ``abrupt" and ``gradual" concept drift, which will be elaborated in details after we giving examples.

In this section, we will apply the monitoring and diagnosis procedures mentioned above to two real data sets, and provide a real case study in how these methods can improve our understanding or model performance.

\subsection{Credit Risk data set}
\label{ss:cr_ds}
\begin{figure}[!htbp]
\centering
\includegraphics[width = 0.49\linewidth]{../figures/v14/credit_default/logi_scal_train_PI/credit_logi_1e-08_0_0001_0_001_99_0.png}
\includegraphics[width = 0.49\linewidth]{../figures/v14/credit_default/logi_nnet_scal_train_PI/credit_logi_0_002_0_0001_0_001_99_0.png}
  \caption{
Control charts monitoring Hotelling $T^2$ of EWMA of the score function and EWMA of the prediction error are compared using the credit risk data set. The left plots are from the logistic regression, and the right from MLP with one hidden layer of $50$ nodes. The tilted numbers along the x-axis, as the indices of month, are the entry time of data. For example, $6-1$ stands for 2006-Jan. So the index $7-12$ is for the 2007-Dec, right after which a $15$-month significant drop began (S\&P 500 from $1478.49$ on 2007-Dec.28 to $683.38$ on 2009-Mar.28).
}
\label{fig:credit_default}
\end{figure}
Each row of the data set of credit risk corresponds to a unique credit card customer from a major financial company. The covariates include customer information ($\bm {x}_i$) and the response indicates whether the customer defaults or not with in $9$-month after opening the account ($y_i$). Each row $i$ is associated with one time stamp when the response, $y_i$, first becomes available. Specifically, the set of all customers who are associated with one day, which will be referred to as their ``entry day", consists of customers who defaulted within $9$-month of opening an account ($y=1$) and customers who didn't default within $9$-month of opening an account ($y=0$), on that entry day. The meaning of customer information ($\bm {x}$) has been disguised for confidentiality reasons, but include customer credit scores, bank balance amount, the number of inquiries in the past years, and so on. The data is originally used in~(\cite{im2012time}) where $10$ covariates used in that paper are inherited in this study. 
To detect the concept drift within this data set, we trained a model and validated it on around half of data starting from $2003$-Jan to $2005$-Dec (monthly indexed from $3-1$ to $5-12$). Data from $2006$-Jan to $2006$-Dec are used in Phase-I to calculate control limits and the rest of data until $2008$-Aug are used for Phase-II to test the capability of detecting the concept drift due to the subprime mortgage crisis. Two models are tested for this examples to show the generality of our method: a logistic regression model and classification MLP (one hidden layer with $50$ activation nodes). Depending on the tolerance of delay for detection in different applications, the EWMA parameter, $\lambda$ can be chosen accordingly. Here, we set $ \lambda = 0.001$, which corresponds to $1$-week effective window or delay.

The underlying concept drift for predictive models trained on covariates and responses can be resulted from many hidden effects. The S\&P 500 declined for a $15$-month by more than $50\%$ in total, which is an outward symptom of the economic crash. The prevailing view is that the root causes (the subprime crisis) had been developing gradually prior to that. The overall economic downturn is probably one of major reasons and we can use this to test the efficacy of our model. In this example, the concept drift evolves very fast within a few months, so that it is closer to an ``abrupt concept drift".

\begin{figure}[!htbp]
\centering
 \includegraphics[width = 0.48\linewidth]{../figures/v14/credit_default/logi_scal_train_PI/PII_pos_single_credit_mlines_with_regu_1e-08_0_0001_0_001_99_0.png}
  \includegraphics[width = 0.48\linewidth]{../figures/v14/credit_default/logi_scal_train_PI/PII_pos_single_credit_fisher_mlines_with_regu_1e-08_0_0001_0_001_99_0.png}
  \caption{
  MEWMA and EWMA control charts for the logistic regression of the credit risk data set (lines are in different colors in electronic version). Comparison are made between before (left) and after (right) being transformed by the inverse of the estimated Fisher Information Matrix, $\mathbf {I} ( {\bm{\theta}} ^{(0)})$. For legibility, here only show MEWMA Hotelling $T^2$ (black) and EWMA control charts for the $1$st (blue), $2$nd (red), $3$rd (green), $8$th (cyan), intercept (orange), from the top to the bottom. The lines of monitored statistics in control charts have the same style and color with lines of their control limits correspondingly.
}
\label{fig:credit_default_diag}
\end{figure}

As shown in Figure~\ref{fig:credit_default}, the score-based method can provide an alert on the economic crash almost $6$-month earlier than the breakout. In details, Figure~\ref{fig:credit_default} consists of Phase-I and Phase-II MEWMA Hotelling $T^2$ chart of the score function for the logistic regression and MLP. We also compare our method with EWMA of the prediction error, which is state-of-the-art metric in terms of early detection among many current concept drift detection methods~(\cite{barros2018large}). From Phase-II control charts, the increasing trend starts around the index $7-6$, which corresponds to $2007$-Jun. This time is $6$-month earlier than the breakout of then economic crash, featured by the beginning of a $15$-month falling of S\&P 500 index by more than $50\%$ in total. This suggests the drift might started even a few months earlier, which is reasonable because the economic crisis was unlikely to happen in a sudden way. Both models of the logistic regression and MLP work well with our method in terms of early detecting the concept drift, suggesting that our method is compatible with models which have a large number of parameters. From EWMA of the prediction error, the detection points are later than that of monitoring the score function. The overall pattern of our control charts indicates that using control chart to monitor the score function leads to early detection of concept drift. 

Figure~\ref{fig:credit_default_diag} shows the capability of diagnosis of our method. In Figure~\ref{fig:credit_default_diag}, EWMA of sample score vectors of the logistic regression model for several covariates are compared before and after transformation with the the inverse of the estimated Fisher Information Matrix, $\mathbf {I} ( {\bm{\theta}} ^{(0)})$. More specifically, EWMA of sample score vectors of several representative scalar covariates are plotted before and after transformation. The MEWMA Hotelling $T^2$ (black solid) are also put at the top row for the ease of comparison, even though the transformation has no effect on it. As shown, the $1$st (the blue line in the $2$nd row) and $2$nd (the red line in the $3$rd row) covariates have significant mean drift before transformation, but mean drift of the $2$nd predictor is largely shrunk to zero after transformation. Those two covariates has $0.45$ correlation, and the $2$nd one has more general thus less direct impact than the $1$st on the credit risk assessment. Thus it is reasonable to think that the concept drift is more likely due to the change of effect of the $1$st covariate instead of that of the $2$nd. Lines of the $3$rd (the green line in the $4$th row) and $8$th (the cyan line in the $5$th row) covariates do not have much mean drift in Phase-II, meaning the overall concept drift does not have much relation with those covariates. The intercept (the orange line in the $6$th row) after transformation also has concept drift, even though the magnitude of this mean drift is much smaller than that of the $1$st predictor. This means the overall default rate is changing during economic crash, which makes sense, because with the same customer's features (covariates) the default rate may increase due to the economic downturn. 

\subsection{Bike Sharing data set}
\label{ss:bs_ds}

\begin{figure}[!htbp]
\centering
\begin{subfigure}[t]{0.48\linewidth}
     \centering
     \begin{subfigure}[t]{\linewidth}
     \centering
         \includegraphics[width=\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_12_tr_5_new/train_bike_reg_1e-08_0_0001_0_01_99_99.png}
         \caption{Training phase starting from year $2012$.}
         \label{fig:bs_tr_2012}
     \end{subfigure}
     \begin{subfigure}[t]{\linewidth}
     \centering
         \includegraphics[width=\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_12_tr_5_new/bike_reg_1e-08_0_0001_0_01_99_99.png}
         \caption{Phase-I and Phase-II up to year $2018$.}
         \label{fig:bs_PIPII_tr_2012}
     \end{subfigure}
\end{subfigure}
\begin{subfigure}[t]{0.48\linewidth}
     \centering
     \begin{subfigure}[t]{\linewidth}
     \centering
        \includegraphics[width=\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_13_tr_5_new/train_bike_reg_1e-08_0_0001_0_01_99_99.png}
        \caption{Training phase starting from year $2013$.}
        \label{fig:bs_tr_2013}
     \end{subfigure}
     \begin{subfigure}[t]{\linewidth}
     \centering
        \includegraphics[width=\textwidth, trim=.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_13_tr_5_new/bike_reg_1e-08_0_0001_0_01_99_99.png}
        \caption{Phase-I and Phase-II up to year $2018$.}
        \label{fig:bs_PIPII_tr_2013}
     \end{subfigure}
\end{subfigure}
\caption{
Control charts monitoring Hotelling $T^2$ of EWMA of the score function and EWMA of the absolute residual are compared using the bike sharing data set. The left column is from a model trained with data starting from year $2012$, while the right from year $2013$. The upper row is from training phase, and the lower from Phase-I and Phase-II. The training data starting from $2012$ show stronger non-stationarity than that starting from $2013$, likely because in year $2012-2013$($2012$ Aug.) the Captial Bikeshare program expanded to Alexandria City and the population affected by this expansion increases by $\approx45\%$(from $\approx550000$ to $\approx800000$). This strong movement of expansion very likely results in the change in how people participate in bike sharing everyday. The model trained on data with less non-stationarity has Phase-II testing $r^2\approx0.81$ which is much higher than the other $r^2\approx0.74$, as the less degree of concept drift shown in the right column. Even though the data has concept drift inherently and both models succeed in capturing that, the model trained on data with less non-stationarity can still have higher predictive power. Also, by comparing the plots of the score function and absolute residual, we can see that the score-based method is more sensitive to concept drifts.
}
\label{fig:bike_sharing}
\end{figure}

The bike sharing data set\footnote{https://www.capitalbikeshare.com/system-data} comes from hourly-aggregated loggings of the Captial Bikeshare system in $7$ jurisdictions nearby Washington Metropolitan Area from year $2010$ to $2020$, integrated with hourly weather data\footnote{http://www.freemeteo.com}. The total sample size is $n=82093$, corresponding $82093$ hours of bike rental and weather data. Each row corresponds to the number of rentals of bikes ($y$) and features related to time and environment conditions at that time ($\bm {x}$), following the same definition and preprocessing procedures from a UCI data set\footnote{https://archive.ics.uci.edu/
ml/datasets/Bike+Sharing+Dataset}~(\cite{fanaee2014event}). The $d=11$ covariates include year($x_1$, categorical with $11$ categories from 2010 to 2020), month($x_2$, categorical with $12$ categories: $1=Jan, 2=Feb, \cdots, 12=Dec$), hour($x_3$, categorical with $24$ categories: $\{0,1,\cdots,23\}$), holiday($x_4$, binary: $0=non-holiday,1=holiday$), weekday($x_5$, categorical with $7$ categories: 
$0=Sun,1=Mon,\cdots,6=Sat$), workingday($x_6$, binary: $1=neither~weekend~nor~holiday,0=otherwise$), weather situation($x_7$, categorical with $3$ categories: $1=(clear|few~clouds\\|partly~cloudy),2=(cloudy|mist),3=(rain|thunder|snow|freezing~fog)$), temp($x_8$, numerical: temperature in Celsius), hum($x_{10}$, numerical: humidity), windspeed($x_{11}$, numerical: wind speed). We trained regression models and validated them on data with various length of time window. Data within one year after that are used for Phase-I for establishing the control limits and the rest of data are used for Phase-II to detect concept drifts. The EWMA parameter $ \lambda = 0.01$, which means the effective window size is about half a week. Several columns clearly dependent on the others, like date and season, are excluded from the model. Some of the covariates, like temp and atemp, are still highly correlated, but we keep them by following the procedure in~(\cite{apley2016visualizing}). 

To maximize the predictive power, we take extra care in modeling the problem. We standardized all numerical covariates and responses to have range close to $[0,1]$. In linear regression, since the model cannot capture the interactions between covariates, we add several interaction terms (interactions between month and hour, weather and month, weather and hour and so on) into the model and obtain a $5$-fold cross-validation $r^2\approx 0.87$, which is consistent between \texttt{R} package \texttt{glmnet} and \texttt{python} package \texttt{sklearn}. In the MLP model, since it inherently captures interaction, we only use those $11$ covariates and treated all categorical ones as numerical, because it would apply some smoothness constraint on the relationship between responses and those covariates, make the neural network simpler, and reduce overfitting. The $5$-fold cross-validation $r^2\approx 0.95$ is consistent between \texttt{R} package \texttt{nnet} and \texttt{python} package \texttt{tensorflow}.


Within the past decade, many bike-sharing businesses were under continuous and fast-paced expansion~(\cite{shaheen2012public}). This kind of gradual drift in the context can be a good example for testing the capability of detecting and diagnosing ``gradual concept drift" of our method. We apply both retrospective and prospective analysis to detect and understand the origin of concept drifts, and in turn develop a better model with those insights. Here, the models we used are the linear regression and MLP, with two hidden layers of $10$ and $5$ nodes, respectively.

\begin{figure}[!htbp]
\centering
\begin{subfigure}[c]{0.54\linewidth}
     \centering
     \begin{subfigure}[t]{\linewidth}
     \centering
         \includegraphics[width=\textwidth, trim=0.0in 4.6in .0in .0in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_10_tr_10/train_bike_reg_1e-08_0_0001_0_01_99_99.png}
     \end{subfigure}
     % \hspace{3.0cm}
     \caption{MEWMA control chart.}
     \label{fig:bs_retro}
     \end{subfigure}
\begin{subfigure}[c]{0.45\linewidth}
     \centering
     \begin{subfigure}[t]{\linewidth}
     \centering
         \includegraphics[width=\textwidth, trim=0.0in .0in .0in .0in, clip]{../figures/v14/bike_sharing/plot_cnt.png}
     \end{subfigure}
     \caption{Hourly bike rental counts over time.}
     \label{fig:bs_cnt}
\end{subfigure}
  \caption{
(a) The MEWMA control charts of the score function of bike sharing data from retrospective analysis. The entire data set is used to train a model and the Hotelling $T^2$ of the score vectors are calculated and visualized. The plot shows non-stationarity of the data set. (b) The hourly bike rental counts over time.
}
\label{fig:bike_sharing_retro}
\end{figure}

From Figure~\ref{fig:bike_sharing}, the upper left plot shows that the training data from year $2012$ has strong non-stationarity, from both the MEWMA of the score function and EWMA of absolute residual, in year $2012-2013$ and $2015-2016$. The high metrics near the beginning and the end of training phase shows an ``U-shape", in contrast to a case of stationary training phase where metrics should randomly fluctuate around a constant value. This is because that the data gradually drifted during the time of training data and minimizing the loss function of training data would result in high error at both ends of the training period. According to the plot in Figure~\ref{fig:bs_tr_2012}, because the data in year $2012-2013$ introduce too much non-stationarity, after deleting them and training a model using data after year $2013$, we should expect the plot of metrics more stationary, which is shown in Figure~\ref{fig:bs_tr_2013}. This better model should further give better testing performance. Indeed, we can see that the Hotelling $T^2$ and absolute residual are lower with the model trained using data after $2013$. More quantitatively, we found the testing $r^2$ (Phase-II) for models trained with data from $2012$ and $2013$ are $\approx0.74$ and $\approx0.81$, respectively. Even though, according to the analysis and plots above, the data has concept drift inherently and both models succeed in capturing that, the model trained on the data set with less non-stationarity would still has higher predicting power. This result is also observed in neural network model, but omitted here for space limitation. The reason behind this non-stationarity in the data will be discussed in details later.

% \begin{figure}[!htbp]
% \centering
% % \captionsetup[subfigure]{justification=top}
% \begin{subfigure}[c]{0.266\linewidth}
%      \centering
%      \begin{subfigure}[t]{\linewidth}
%      \centering
%          \includegraphics[width=0.8\textwidth, trim=.0in .4in .5in 1.2in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_10_tr_exp/PII_pos_single_retro_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
%      \end{subfigure}
%      % \hspace{3.0cm}
%      \captionsetup{width=.95\linewidth}
%      \caption{Retrospective analysis on the entire data set by training on middle part of data set.}
%      \label{fig:bs_retro_mid}
%      \end{subfigure}
% \begin{subfigure}[c]{0.357\linewidth}
%      \centering
%      \begin{subfigure}[t]{\linewidth}
%      \centering
%          \includegraphics[width=0.8\textwidth, trim=.0in .0in .0in 1.2in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_10_tr_3/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
%          \includegraphics[width=.8\textwidth, trim=12.5in 0.5in 3.0in 1.0in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_10_tr_3/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99_72.png}
%      \end{subfigure}
%      \captionsetup{width=.95\linewidth}
%      \caption{Diagnosis plots for a model trained using raw bike rental counts.}
%      \label{fig:bs_raw_cnt}
% \end{subfigure}
% \begin{subfigure}[c]{0.357\linewidth}
%      \centering
%      \begin{subfigure}[t]{\linewidth}
%      \centering
%         \includegraphics[width=0.8\textwidth, trim=.0in .0in .0in 1.2in, clip]{../figures/v14/bike_sharing/reg_lin_cat_norm_syr_10_pow_tr_3/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
%         \includegraphics[width=.8\textwidth, trim=12.5in 0.5in 3.0in 1.0in, clip]{../figures/v14/bike_sharing/reg_lin_cat_norm_syr_10_pow_tr_3/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99_72.png}
%      \end{subfigure}
%      \captionsetup{width=.95\linewidth}
%      \caption{Diagnosis plots for a model trained using normalized rental counts, by trailing yearly mean.}
%      \label{fig:bs_norm_cnt}
% \end{subfigure}
%   \caption{
%   (a) Retrospective analysis using a model trained upon data from around $2014$ to the end of $2016$. These plots show a very strong non-stationarity, and are more clear than Figure~\ref{fig:bs_retro}. The EWMA is much more stable in the middle period and shows stronger concept drift at both ends of period. (b)(c) MEWMA and EWMA control charts for linear regression of bike sharing data set (lines are in different colors in electronic version) for detecting and diagnosing concept drifts. The left column (b) is from a model trained on the raw bike rental count, while the right column (c) the normalized bike rental count. The upper row shows some representative lines of components, while the lower row includes more lines to overview the pattern of concept drifts. The results from raw bike rental count show strong concept drifts, while the concept drift is largely reduced in results from normalized bike rental count, because the change of hidden context is learned by the model. The Phase-II testing $r^2$ for the left and right column are $\approx0.26$ and $\approx0.85$, respectively.
% }
% \label{fig:bike_sharing_diag}
% \end{figure}

\begin{figure}[!htbp]
    \centering
         \includegraphics[width=0.5\textwidth, trim=.0in .4in .4in 1.2in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_10_tr_exp/PII_pos_single_retro_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     % \hspace{3.0cm}
    %  \captionsetup{width=.95\linewidth}
     \caption{Retrospective analysis using a model trained upon data from around $2014$ to the end of $2016$. These plots show a very strong non-stationarity, and are more clear than Figure~\ref{fig:bs_retro}. The EWMA is much more stable in the middle period and shows stronger concept drift at both ends of period. This means training on a subset of data set which is more stationary would build a better model which in turn is more sensitive to concept drifts in the retrospective analysis.}
     \label{fig:bs_retro_mid}
\end{figure}

\begin{figure}[!htbp]
\centering
    \begin{subfigure}[t]{0.49\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in 1.2in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_10_tr_3/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Diagnosis plots for a model trained using raw bike rental counts.}
     \label{fig:bs_raw_cnt}
\end{subfigure}
\begin{subfigure}[t]{0.49\linewidth}
     \centering
        \includegraphics[width=1.0\textwidth, trim=.0in .0in .0in 1.2in, clip]{../figures/v14/bike_sharing/reg_lin_cat_norm_syr_10_pow_tr_3/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99.png}
     \captionsetup{width=.95\linewidth}
     \caption{Diagnosis plots for a model trained using normalized rental counts, by trailing yearly mean.}
     \label{fig:bs_norm_cnt}
\end{subfigure}
\caption{
MEWMA and EWMA control charts for linear regression of bike sharing data set (lines are in different colors in electronic version) for detecting and diagnosing concept drifts. The left column (a) is from a model trained on the raw bike rental count, while the right column (b) the normalized bike rental count. Here we show some representative lines of components. The results from raw bike rental count show strong concept drifts, while the concept drift is largely reduced in results from normalized bike rental count, because the change of hidden context is learned by the model. The Phase-II testing $r^2$ for the left and right column are $\approx0.26$ and $\approx0.85$, respectively.
}
\label{fig:bike_sharing_diag}
\end{figure}


\begin{figure}[!htbp]
    \begin{subfigure}[t]{1.0\linewidth}
     \centering
         \includegraphics[width=1.0\textwidth, trim=12.5in 0.5in 3.0in 1.0in, clip]{../figures/v14/bike_sharing/reg_lin_cat_syr_10_tr_3/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99_72.png}
    %  \captionsetup{width=.95\linewidth}
     \caption{Diagnosis plots for a model trained using raw bike rental counts.}
     \label{fig:bs_raw_cnt_more}
\end{subfigure}
\begin{subfigure}[t]{1.0\linewidth}
     \centering
        \includegraphics[width=1.0\textwidth, trim=12.5in 0.5in 3.0in 1.0in, clip]{../figures/v14/bike_sharing/reg_lin_cat_norm_syr_10_pow_tr_3/neg_single_bike_fisher_mlines_with_regu_1e-08_0_0001_0_01_99_99_72.png}
    %  \captionsetup{width=.95\linewidth}
     \caption{Diagnosis plots for a model trained using normalized rental counts, by trailing yearly mean.}
     \label{fig:bs_norm_cnt_more}
\end{subfigure}
  \caption{
  MEWMA and EWMA control charts for linear regression of bike sharing data set (lines are in different colors in electronic version) for detecting and diagnosing concept drifts. The left column (a) is from a model trained on the raw bike rental count, while the right column (b) the normalized bike rental count. Here, more plots of Figure~\ref{fig:bs_raw_cnt} are shown to give a overview of the pattern in diagnostic plots.
}
\label{fig:bike_sharing_diag_more}
\end{figure}


Clearly, the retrospective analysis above provides deeper understanding in the data set, which likely helps us build a better model. Both Hotelling $T^2$ and absolute residuals can achieve this purpose. To further illustrate the retrospective analysis and mimic real usage of our method in applications, this time we use the entire data set. As in Figure~\ref{fig:bs_retro}, the MEWMA control chart for the score function shows strong non-stationarity as the data in the beginning and the end of the period has high monitoring statistics, presenting an ``U-shape". This is because the data set has a gradual concept drift over the entire period, and the trained model converges to a solution best fitting data from the middle period of time. To make the pattern of non-stationarity more sensitive for interpretation, we can choose the training data set based on the preliminary retrospective analysis in Figure~\ref{fig:bs_retro}. This is because when we train our model on the entire data set, there is no single model that can fit all the data very well and the solution may not be very meaningful. Instead, if we can restrict our data set within a period where data are approximately stationary or with a small concept drift, the model would be more meaningful and the pattern of concept drift would also be more obvious. As shown in the top plot of Figure~\ref{fig:bs_retro_mid}, we train a model using data from the beginning of year $2014$ to the end of year $2016$, during which data are more or less stationary (the choice of period is at users' discretion, as long as the data chosen is more stationary than the entire data set and the level of stationarity is acceptable). The ``U-shape" we observed in Figure~\ref{fig:bs_retro} also shows up and is clearer. The cross-validation $r^2$ of the plot in Figure~\ref{fig:bs_retro} is $0.7928$ and the cross-validation $r^2$ for training data and $r^2$ for the entire data set in Figure~\ref{fig:bs_retro_mid} are $0.8776$ and $0.7590$, meaning the model trained on more stationary data can fit data better and be more sensitive to the concept drift in the entire data set. 

The control charts of components of score function show the directions in which corresponding coefficients should change to better fit the data in the local time window, providing more interpretation. As shown in the second row of Figure~\ref{fig:bs_retro_mid} for the predictor year, the deviation is strong in the beginning and the end, but with different signs. The model captures the slope of year in the middle period but slopes before and after that are larger and smaller, according to the bike count over time in Figure~\ref{fig:bs_cnt}. This observation is consistent with that in the last row of control chart for the intercept, where it is close to zero in the middle and negative at both ends. Because we centered the predictor year and all other covariates, the intercept should represent the mean of bike rental count in the middle period. By extrapolating from the middle to both ends in Figure~\ref{fig:bs_cnt} using learned coefficient of year (slope), we can imagine that the intercept should be smaller to better fit the data. Also in the third and the fourth rows of Figure~\ref{fig:bs_retro_mid}, the effects of month May and September show similar trend as that of year, but because month is treated as factor, during the time outside those months, the monitored statistics in control charts are close to zero.

As mentioned, the period of data chosen to train a model in retrospective analysis is not unique. We can also choose the beginning period from year $2010$ to the end of year $2013$. As shown in Figure~\ref{fig:bike_sharing_diag}, the Figure~\ref{fig:bs_raw_cnt} has plots of EWMA of components of the score function after transformation with Fisher information matrix as described in Section~\ref{s:decou_cd}. We see that all components have similar pattern of drifts, from which we can naturally infer that the concept drift probably results from overall increase of the popularity of bike sharing. We select several plots in Figure~\ref{fig:bike_sharing_diag} and put more EWMA of components together in Figure~\ref{fig:bike_sharing_diag_more} to give a bird's-eye view on this pattern. With this understanding in mind, we consider normalizing the raw bike rental count, $y$, with the trailing moving average of the hourly rental count within one year, $y_{MA}$. More specifically, we generate a normalized response, denoted as $\tilde{y}=y/y_{MA}^\beta$, where $\beta=0.78$ is a tuning parameter\footnote{This idea comes from the following observation. Assume a smooth version of response (bike rental count) over time as $f(t)$. A proper normalization should satisfy a constraint $\frac{f(t)}{\int_{t-1}^{t}f(\tau)d\tau}\approx const$ or $f'(t)\approx const*(f(t)-f(t-1))$. Obviously, a function of time oscillating around a constant value satisfies this. In order to obtain a solution with increasing trend without blowing up, we can see that a $f(t)$ with
monotonically decreasing $f'(t)$ may satisfy the constraint.} and found by maximizing testing $r^2$. The model trained on this normalized response has diagnosis plots as in Figure~\ref{fig:bs_norm_cnt}. And by comparing them with those in Figure~\ref{fig:bs_raw_cnt}, we see that the concept drift is largely reduced because the change of hidden context is learned after normalizing response. We obtained the Phase-II testing $r^2$ as $\approx0.26$ and $\approx0.85$ without and with this normalization, respectively.

\begin{figure}[!htbp]
\centering
\includegraphics[width=.5\textwidth, trim=.0in 4.58in .0in .0in, clip]{../figures/v14/bike_sharing/reg_nnet_numer_norm_syr_10_pow_tr_10/marked_train_bike_reg_1e-06_0_0001_0_0005_99_99.png}
\includegraphics[width = 0.53\linewidth, trim=.0in .in .0in 0.0in, clip]{../figures/v14/bike_sharing/reg_lin_cat_norm_syr_10_pow_tr_10/mark_training_pos_single_train_bike_fisher_mlines_with_regu_1e-08_0_0001_0_0005_99_99.png}
\includegraphics[width = 0.47\linewidth]{../figures/v14/bike_sharing/marked_plot_num_counties.png}\linebreak
\includegraphics[width = 0.5\linewidth]{../figures/v14/bike_sharing/marked_plot_num_bikes.png}
  \caption{
The MEWMA control charts of bike sharing data set with normalized response shows some local non-stationarity in training phase. The three major peaks are marked in the upper plot. In the second plot, the diagnostic plot of intercept of the model is drawn. In the third plot, the number of counties in the program of Capital Bikeshare over time is drawn, and the name of those counties are marked. In the bottom plot, the three periods where the number of bikes increases fast due to expansion of the program are also marked, corresponding to the three major peaks in the upper plot.
}
\label{fig:bike_sharing_interp}
\end{figure}

After normalizing response, the global concept drift has been learned by the model and is no longer significant in the top plot in Figure~\ref{fig:bs_norm_cnt}. However, as shown in the first plot in Figure~\ref{fig:bike_sharing_interp}, there are still some local concept drift (here, we use smaller EWMA parameter $\lambda=0.0005$, to average out more noise and make the local concept drift clearer).
To interpret it, we look deeper into the plots of training phase (retrospective analysis), the data set, and how those data are processed from original loggings of bike rentals. In the first plot of Figure~\ref{fig:bike_sharing_interp}, we plot the Hotelling $T^2$ of EWMA of the score function from the model trained using the normalized responses of the entire data set. There are three major peaks in the metrics, happening in periods $2012-2013$, $2016-2017$, and $2018-2020$. In the second plot, we include the intercept of the model, which shows some similar peaks. In the third plot, we show the number of jurisdictions nearby Washington Metropolitan Area in the Capital Bikeshare program over time, and in the bottom plot, the number of bikes available in the program. We see that those three peaks correspond to three periods when the program of Capital Bikeshare expanded to new jurisdictions and increased the number of bikes dramatically. Such fast expansion in a short period of time can temporarily introduce non-stationarity into the data set, probably because of factors of media exposures, psychology effects, and adaptation of habits of commuting.

\section{Conclusion}
Predictive models are often trained on historical data sets, but due to potential change of the conditional distribution of $P (Y|\bm {X})$ (a.k.a concept drift) the performance of those models may degrade. Here, we propose to monitor the score function of models using MEWMA and EWMA control charts. The score-based method has several advantages, comparing against the error-based methods. First, concept drifts can result in no change in error, but it must change the mean of score function for parametric models. Failing to detect those concept drifts using the error-based methods would miss potential opportunities to improve model performance, which can be resolved using the score-based methods. Second, score-based methods are shown to have smaller $MRL_1$ (out-of-control median run-length), given the same $MRL_0$ (in-control median run-length), comparing with other metrics, indicating that the score-based method is more sensitive to concept drifts. Third, score-based method provides a comprehensive framework to detect and diagnose concept drift and can be generalized to any parametric models, without incurring much extra computation. 

The advantages of this method manifest in perspectives of theory, computation, and practicality in real applications. The insights obtained can in turn help improve the model performance and thus mitigate effect of concept drifts. For future works, we plan to extend this method to other predictive models with biased estimators and deal with interpretability for the MLP or deep neural network models.

\acks{}
%\acks{We would like to acknowledge support for this project
%from the National Science Foundation (NSF grant IIS-9988642)
%and the Multidisciplinary Research Program of the Department
%of Defense (MURI N00014-00-1-0637). }

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

%\newpage

%
%\vskip 0.2in
%\bibliography{sample} % For PC
\bibliography{sample.bib} % For mac

\vspace{1in}

\begin{appendix}
% \textbf{Appendix A}
% \label{app:sgd_ewma}
% The EWMA of the score function in definition (\ref{eqn:ewma}) should have the same mean as the mean of the score function, $E[\bm {z}_t]=E[\bm {s}_t]$, if $\bm {s}_t$ follows its stationary distribution. However, because here the score function $\bm {s}_t$ comes from SGD, so that it takes some time before the parameters of the score function converging to the true value. To rigorously argue that the $\lim _{t\to +\infty}\bm {z}_t=\bm{0}$, we need to incorporate the dynamics of SGD. We can expand the EWMA as following:
% \begin{align}
% \bm {z}_t = \alpha \sum _{i=1}^t (1- \alpha)^{t-i}\bm {s}_i + (1- \alpha)^t \bm {z}_0 
% \label{eqn:ewma_expa}
% \end{align}
% Take total expectation on both sides, we have:
% \begin{align}
% E[\bm {z}_t] = \alpha \sum	_{i=1}^t (1- \alpha) ^{t-i} E[\bm {s}_i] + (1- \alpha)^t E[\bm {z}_0]
% \label{eqn:exp_ewma_expa}
% \end{align}
% To argue that $\lim _{t\to +\infty}E[\bm {z}_t]=\bm{0}$, we need to argue that $\lim _{t\to +\infty} E[\bm {s}_i] =\bm {0}$. According to the assumptions in paper (\cite{bottou2018optimization}), with strong convexity on the expectation of log-likelihood function, $E[\ln f ( \bm { \theta}| (\bm {X}, Y))]$, we have:
% \begin{align}
% \frac{1}{2}c||\bm { \theta}_t - \bm { \theta}^*||_2^2 \leq E[\ln f ( \bm { \theta}_t| (\bm {X}, Y))]-E[\ln f ( \bm { \theta}^*| (\bm {X}, Y))] \leq \frac{ \nu}{ \gamma+t} 
% \end{align}
% where the constants, $ \nu$ and $ \gamma$, are according to Theorem 4.7. So we have $\lim _{t\to+\infty} \bm { \theta}_t = \bm { \theta}^*$, where the $ \bm { \theta}^*$ is the value when $E[\ln f ( \bm { \theta}| (\bm {X}, Y))]$ achieve maximum, that is, when $\bm { \theta}$ takes the true parameter value (according to KL-divergence). So we have the parameter sequence of $\{\bm { \theta}_t\}_t$ converge to the true value of $\bm { \theta}$. With continuity conditions on the score function, we have $\lim _{t\to +\infty} E[\bm {s}_i] =\bm {0}$, and thus $\lim _{t\to +\infty}E[\bm {z}_t]=\bm{0}$.


% \textbf{Appendix A}
\section{Simulations on Median Run-Length ($MRL$)}
\label{ss:simu_MRL}
To quantitatively show higher sensitivity of the score-based method, the Monte Carlo simulations are conducted for different models to calculate $MRL$. Data sets are generated according to linear, logistic, multinomial, and poisson regression model as data generating processes. Then, corresponding training models are fitted to those simulated data sets.

For a fair comparison, error rate, residual, and score-based metrics are used in the control charts with the same EWMA parameter $\lambda$, as defined in Section~\ref{ss:MEWMA}. In SPC, different sizes of mean drift have different optimal $ \lambda$'s, because of the trade-off of sensitivity in drift size and detection delay, as mentioned in Section~\ref{ss:MEWMA}. Here, we choose three drift sizes and $ \lambda$'s so that $MRL_0$'s (in-control $MRL$) equal to a given value. The shorter $MRL_1$ (out-of-control $MRL$) for a given $ \lambda$, the concept drift is detected earlier. Here, examples of the linear(Table~\ref{tab:lin_MRL}), logistic(Table~\ref{tab:logi_MRL}), multinomial(Table~\ref{tab:multi_logi_MRL}), and poisson(Table~\ref{tab:pois_MRL}) regressions are tested. The neural network models (Table~\ref{tab:lin_nnet_MRL},~\ref{tab:logi_nnet_MRL},~\ref{tab:multi_logi_nnet_MRL}) are also applied on those data sets to see if the score-based method can also be used in highly nonlinear models. In those tables, $ \alpha$ is the changing factor that represents the size of concept drift or changes of parameters (larger $ \alpha$ means larger concept drift). As shown in results, the score-based method has smaller $MLR_1$, which indicates higher sensitivity. The gap between $MRL_1$ of the score-based metric and other metrics is larger when concept drift size is smaller, meaning that the detection becomes more challenging. This means the score-based method shows stronger advantage in detecting smaller drift size. Overall, those simulations prove the generality and superior capability of the score-based method.

\begin{table}[H]
\centering
\begin{tabular}{ccccc}
\toprule
\multicolumn{2}{c}{($ \alpha$, $ \lambda$)} & {$  \lambda_1$} & {$ \lambda_2$} & {$ \lambda_3$} \\
\midrule
\multirow{3}{*}{$\alpha = 0$} & score &$3500.0$ & $3499.0$ & $3502.5$ \\
& err &$3499.5$ & $3501.0$ & $3500.0$ \\
%& dev &$3499.5$ & $3499.5$ & $3501.0$ \\
\midrule
\multirow{3}{*}{$\alpha = 0.3$} & score &$\bm{391.0}$ & $\bm{377.5}$ & $\bm{464.0}$ \\
& err &$835.0$ & $1050.0$ & $1278.0$ \\
%& dev &$575.0$ & $633.0$ & $814.0$ \\
\midrule
\multirow{3}{*}{$\alpha = 0.5$} & score &$\bm{184.0}$ & $\bm{148.0}$ & $\bm{153.0}$ \\
& err &$359.0$ & $360.5$ & $442.0$ \\
%& dev &$244.0$ & $218.0$ & $239.0$ \\
\midrule
\multirow{3}{*}{$\alpha = 0.7$} & score &$\bm{111.0}$ & $\bm{82.0}$ & $\bm{75.0}$ \\
& err &$205.0$ & $177.0$ & $186.0$ \\
%& dev &$134.0$ & $108.0$ & $105.0$ \\
\midrule
\end{tabular}
\caption{Comparison of Phase-II $MRL$'s of the logistic regression using the score and classification error, with $10000$ simulations. The $MRL_0$'s (in-control median run lengths) are matched as close as possible. $ \lambda$ is the EWMA parameter ({$ \lambda_1 = 0.0028369$}, {$ \lambda_2 = 0.0087330$}, {$ \lambda_3 = 0.018546$}), and $ \alpha$ is the changing factor ($ \alpha=0$ corresponds to the in-control case).}
\label{tab:logi_MRL}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{ccccc}
\toprule
\multicolumn{2}{c}{($ \alpha$, $ \lambda$)} & $ \lambda_1$ & $ \lambda_2$ & $ \lambda_3$ \\
\midrule
\multirow{3}{*}{$\alpha=0$} & score &$5500.0$ & $5498.5$ & $5499.5$ \\
& err &$5499.0$ & $5499.5$ & $5498.5$ \\
%& dev &$5502.5$ & $5498.5$ & $5498.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.5$} & score &$\bm{246.0}$ & $\bm{260.0}$ & $\bm{302.0}$ \\
& err &$929.0$ & $1164.0$ & $1444.5$ \\
%& dev &$557.0$ & $649.0$ & $807.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.7$} & score &$\bm{149.0}$ & $\bm{141.0}$ & $\bm{148.0}$ \\
& err &$482.0$ & $567.0$ & $694.0$ \\
%& dev &$285.0$ & $300.0$ & $342.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.9$} & score &$\bm{110.0}$ & $\bm{98.0}$ & $\bm{97.0}$ \\
& err &$316.0$ & $345.0$ & $406.0$ \\
%& dev &$182.0$ & $176.0$ & $184.0$ \\
\midrule
\end{tabular}
\caption{Comparison of Phase-II $MRL$'s of multinomial regression using the score and classification error, with $10000$ simulations. The $MRL_0$'s (in-control median run lengths) are matched as close as possible. $ \lambda$ is the EWMA parameter ({$ \lambda_1 = 0.0060052$}, {$ \lambda_2 = 0.010923$}, {$ \lambda_3 = 0.016641$}), and $ \alpha$ is the changing factor ($ \alpha=0$ corresponds to the in-control case).}
\label{tab:multi_logi_MRL}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{ccccc}
\toprule
\multicolumn{2}{c}{($ \alpha$, $ \lambda$)} & {$ \lambda_1$} & {$ \lambda_2$} & {$ \lambda_3$} \\
\midrule
\multirow{3}{*}{$ \alpha=0$} & score &$1199.5$ & $1200.5$ & $1200.0$ \\
& abs\_resi &$1200.0$ & $1199.5$ & $1200.0$ \\
%& dev &$1199.5$ & $1199.5$ & $1199.5$ \\
\midrule
\multirow{3}{*}{$\alpha=0.1$} & score &$\bm{247.0}$ & $\bm{339.0}$ & $\bm{516.0}$ \\
& abs\_resi &$898.5$ & $1003.5$ & $1023.5$ \\
%& dev &$866.5$ & $986.0$ & $989.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.3$} & score &$\bm{69.0}$ & $\bm{46.0}$ & $\bm{52.0}$ \\
& abs\_resi &$127.0$ & $111.0$ & $135.0$ \\
%& dev &$113.0$ & $100.0$ & $127.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.5$} & score &$\bm{38.0}$ & $\bm{20.5}$ & $\bm{19.0}$ \\
& abs\_resi &$48.0$ & $30.0$ & $27.0$ \\
%& dev &$38.0$ & $26.0$ & $25.0$ \\
\midrule
\end{tabular}
\caption{Comparison of Phase-II $MRL$'s of the linear regression using the score and absolute residual, with $10000$ simulations. The $MRL_0$'s (in-control median run lengths) are matched as close as possible. $ \lambda$ is the EWMA parameter ( {$ \lambda_1 = 0.0038876$}, {$ \lambda_2 = 0.028477$}, {$ \lambda_3 =0.065955$}), and $ \alpha$ is the changing factor ($ \alpha=0$ corresponds to the in-control case).}
\label{tab:lin_MRL}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{ccccc}
\toprule
\multicolumn{2}{c}{($ \alpha$, $ \lambda$)} & {$ \lambda_1$} & {$ \lambda_2$} & {$ \lambda_3$} \\
\midrule
\multirow{3}{*}{$\alpha=0$} & score &$6495.5$ & $6506.0$ & $6506.5$ \\
& abs\_resi &$6501.0$ & $6500.0$ & $6502.0$ \\
%& dev &$6501.0$ & $6498.0$ & $6507.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.3$} & score &$\bm{321.0}$ & $\bm{381.0}$ & $\bm{460.0}$ \\
& abs\_resi &$1661.5$ & $1772.0$ & $1807.5$ \\
%& dev &$3146.0$ & $3474.5$ & $3659.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.5$} & score &$\bm{171.0}$ & $\bm{176.5}$ & $\bm{195.0}$ \\
& abs\_resi &$453.0$ & $475.5$ & $508.0$ \\
%& dev &$718.0$ & $794.0$ & $875.5$ \\
\midrule
\multirow{3}{*}{$\alpha=0.7$} & score &$\bm{119.0}$ & $\bm{116.0}$ & $\bm{123.0}$ \\
& abs\_resi &$233.0$ & $228.0$ & $232.0$ \\
%& dev &$290.0$ & $286.0$ & $295.0$ \\
\midrule
\end{tabular}
\caption{Comparison of Phase-II $MRL$'s of poisson regression using the score and absolute residual, with $10000$ simulations. The $MRL_0$'s (in-control median run lengths) are matched as close as possible. $ \lambda$ is the EWMA parameter ({$ \lambda_1=0.0050701$} , {$ \lambda_2=0.0090466$}, {$ \lambda_3=0.013069$}), and $\alpha$ is the changing factor ($ \alpha=0$ corresponds to the in-control case).}
\label{tab:pois_MRL}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ccccc}
\toprule
\multicolumn{2}{c}{($ \alpha$, $ \lambda$)} & {$ \lambda_1$} & {$ \lambda_2$} & {$ \lambda_3$} \\
\midrule
\multirow{3}{*}{$\alpha=0$} & score &$3494.0$ & $3505.5$ & $3494.5$ \\
& err &$3503.5$ & $3508.0$ & $3521.5$ \\
%& dev &$3500.5$ & $3508.5$ & $3507.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.3$} & score &$\bm{476.0}$ & $\bm{390.0}$ & $\bm{385.0}$ \\
& err &$800.5$ & $834.5$ & $1064.0$ \\
%& dev &$616.5$ & $576.5$ & $645.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.5$} & score &$\bm{252.5}$ & $\bm{177.0}$ & $\bm{153.5}$ \\
& err &$403.0$ & $346.5$ & $349.5$ \\
%& dev &$288.0$ & $230.0$ & $218.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.7$} & score &$\bm{159.0}$ & $\bm{110.0}$ & $\bm{84.0}$ \\
& err &$251.0$ & $198.0$ & $177.0$ \\
%& dev &$173.0$ & $136.0$ & $112.0$ \\
\midrule
\end{tabular}
\caption{Comparison of Phase-II $MRL$'s of neural network ($1$ hidden layer with $10$ nodes) for logistic binary classification data using the score and classification error, with $1000$ simulations. The $MRL_0$'s (in-control median) run lengths are matched as close as possible. $ \lambda$ is the EWMA parameter ({$ \lambda_1=0.0010545$}, {$ \lambda_2=0.0035807$}, {$ \lambda_3=0.0082376$}), and $ \alpha$ is the changing factor ($ \alpha=0$ corresponds to the in-control case).}
\label{tab:logi_nnet_MRL}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{ccccc}
\toprule
\multicolumn{2}{c}{($ \alpha$, $ \lambda$)} & {$ \lambda_1$} & {$ \lambda_2$} & {$ \lambda_3$} \\
\midrule
\multirow{3}{*}{$\alpha=0$} & score &$5519.5$ & $5470.5$ & $5517.0$ \\
& err &$5503.5$ & $5491.5$ & $5509.5$ \\
%& dev &$5495.0$ & $5494.0$ & $5474.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.5$} & score &$ \bm{341.0}$ & $\bm{434.0}$ & $\bm{553.0}$ \\
& err &$955.0$ & $1164.5$ & $1432.5$ \\
%& dev &$626.5$ & $721.0$ & $880.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.7$} & score &$\bm{202.0}$ & $\bm{208.0}$ & $\bm{249.0}$ \\
& err &$507.0$ & $569.5$ & $711.0$ \\
%& dev &$336.5$ & $347.0$ & $375.5$ \\
\midrule
\multirow{3}{*}{$\alpha=0.9$} & score &$\bm{136.0}$ & $\bm{132.0}$ & $\bm{140.0}$ \\
& err &$313.0$ & $325.5$ & $376.5$ \\
%& dev &$205.5$ & $197.0$ & $202.5$ \\
\midrule
\end{tabular}
\caption{Comparison of Phase-II $MRL$'s (median run lengths) of neural network ($1$ hidden layer with $10$ nodes) for multinomial regression data using the score and classification error, with $1000$ simulations. The $MRL_0$'s (in-control median run lengths) are matched as close as possible. $ \lambda$ is the EWMA parameter ({$ \lambda_1 =0.0052546$}, {$ \lambda_2=0.0092416$}, {$ \lambda_3 =0.014028$}), and $ \alpha$ is the changing factor ($ \alpha=0$ corresponds to the in-control case).}
\label{tab:multi_logi_nnet_MRL}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{ccccc}
\toprule
\multicolumn{2}{c}{($ \alpha$, $ \lambda$)} & {$ \lambda_1$} & {$ \lambda_2$} & {$ \lambda_3$} \\
\midrule
\multirow{3}{*}{$\alpha=0$} & score &$1192.5$ & $1202.0$ & $1199.5$ \\
& abs\_resi &$1200.0$ & $1198.5$ & $1201.5$ \\
%& dev &$1199.0$ & $1200.0$ & $1199.0$ \\
\midrule
\multirow{3}{*}{$\alpha=0.1$} & score &$\bm {518.5}$ & $\bm{1031.5}$ & $1429.5$ \\
& abs\_resi &$1313.5$ & $1431.5$ & $\bm{1412.0}$ \\
%& dev &$1635.0$ & $1911.5$ & $1998.5$ \\
\midrule
\multirow{3}{*}{$\alpha=0.3$} & score &$\bm{134.0}$ & $\bm{122.5}$ & $\bm{225.0}$ \\
& abs\_resi &$223.0$ & $218.5$ & $274.0$ \\
%& dev &$297.0$ & $487.0$ & $1655.5$ \\
\midrule
\multirow{3}{*}{$\alpha=0.5$} & score &$\bm{66.0}$ & $\bm{46.0}$ & ${52.0}$ \\
& abs\_resi &$76.0$ & $56.0$ & $52.0$ \\
%& dev &$84.0$ & $72.0$ & $92.5$ \\
\midrule
\end{tabular}
\caption{Comparison of Phase-II $MRL$'s (median run lengths) of neural network ($1$ hidden layer with $10$ nodes) for linear regression data using the score and absolute residual, with $1000$ simulations. The $MRL_0$' (in-control median run lengths) are matched as close as possible. $ \lambda$ is the EWMA parameter ({$ \lambda_1=0.0021882$}, {$ \lambda_2=0.012062$}, {$ \lambda_3=0.027374$}), and $ \alpha$ is the changing factor ($ \alpha=0$ corresponds to the in-control case).}
\label{tab:lin_nnet_MRL}
\end{table}


\section{Concept Drift Diagnoses}
\label{ss:cd_diag}
To demonstrate diagnosis capability of the score-based method, we simulate various kinds of data sets for regression and classification. We also generate data sets with abrupt and gradual concept drifts to mimic more cases in real applications. Examples are presented from simple to complex ones, to help build up intuitions for the score-based method.

\subsection{Simulated data set for Linear Regression}
\label{sss:lin_exp}
% Change nugget parameter to psudo-inverse.
\begin{enumerate}[(I)]
\item
\textbf{Abrupt Concept Drifts with Independent Covariates}
\label{ssss:lin_ind_pred}

\begin{figure}[!hpt]
\centering
  \includegraphics[width = 0.6\linewidth]{../figures/v14/sim_2/reg/neg_single_1_sim2_mlines_with_regu_1e-08_0_005.png}
  \caption{Abrupt concept drift of the linear model with independent covariates (lines are in different colors in electronic version). For conciseness, here only show MEWMA control chart of Hotelling $T^2$ (black) and EWMA control charts for the $1$st (blue), $5$th (red), $7$th (green), intercept (cyan), from the top to the bottom. The blue (first) and green (second) vertical lines mark the boundaries of Phase-I/II and before/after concept drift. The lines of monitored statistics in control charts have the same style and color with lines of their control limits correspondingly.}
  \label{fig:lin_reg_ind_X}
\end{figure}
In this simulation, the data set is generated by a {linear} model $y = \bm {x}^T\bm { \theta} + \epsilon$. The $10$ {covariates} {$\bm {x}=[x_1, x_2, \cdots, x _{10}]$} are independently distributed as Gaussian with mean $0$ and variance $1$; the variance of random error $ \epsilon$ is $1$; the EWMA parameter is $\lambda=0.027562$ chosen by ensuring around $50\%$ signal ratio of absolute residual after concept drift; there is no regularization in training the model because the number of parameters is small comparing to the sample size; the alarm rate for calculating the control limits is $99.9\%$; the sizes of the data set used in training, validation, Phase-I, and Phase-II are $10000$, $2000$, $20000$, and $40000$. The responses in training, validation, and Phase-I are generated with coefficients all equal to $1$. In the first half of Phase-II, the coefficients are unchanged, but in the second half, the first four coefficients are multiplied by $1- \alpha=0.7$, where $ \alpha=0.3$ is defined as the changing factor. Other coefficients are unchanged.
\begin{figure}[!htp]
\centering
\includegraphics[width = 0.6\linewidth]{../figures/v14/sim_2/reg/1_sim2_lin_1e-08_0_005_1.png}
  \caption{Abrupt concept drift of the linear model with independent covariates. Hotelling $T^2$ of EWMA of the score function and EWMA of the absolute residual are compared.}
  \label{fig:lin_reg_ind_X_comp}
\end{figure}

Figure~\ref{fig:lin_reg_ind_X} shows Phase-I and Phase-II of monitoring Hotelling $T^2$ and individual components of the score function. Before the blue (first) vertical lines (Phase-I), all lines of monitored statistics are well mixed in their range indicating no concept drift in training and Phase-I. After blue vertical lines (Phase-II), the EWMA lines corresponding to the first four coefficients have significant drifts (only show the first one here) and the change of mean is $0.3$ corresponding to the mean drift of coefficients, contributing to the entire line of MEWMA Hotelling $T^2$ (the black line in the top plot). In this example, all covariates are independent with others, so that there is not coupling, meaning only coefficients with concept drifts would have mean drifts. For brevity, only representative lines are shown. Lines of the rest of coefficients (including intercept) do not have mean drift but have larger deviation (or variance) after the concept drift, for the reason mentioned in Section~\ref{ss:var_infla}. The increase of deviation in control charts of those unchanged parameters results from the drifts ($\Delta \bm { \theta}$) coupling with variation of covariates ($X_k\bm {X}^T$).

To compare Hotelling $T^2$ of EWMA of the score function and EWMA of the absolute residual, we plot it with the score function respectively as shown in Figure~\ref{fig:lin_reg_ind_X_comp}. Both methods can capture the abrupt concept drift pretty well: in Phase-I they are all mixed well and in Phase-II mean drifts are close to each other. Here, we also see inflated variance of lines after the concept drift, even though they plateau very quickly. Both metrics capture the concept drift quite well, but the score-based method has higher signal ratio after concept drift, indicating that its superior performance in detecting small size changes, which is consistent with the results from Appendix~\ref{ss:simu_MRL}. Later, we will show results of gradual concept drift.

\item
\textbf{Abrupt Concept Drifts with Correlated Covariates}

\label{ssss:lin_not_ind_pred}
\begin{figure}[!htbp]
\centering
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_4/reg/PII_neg_single_1_sim4_mlines_with_regu_1e-08_0_005.png}
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_4/reg/PII_neg_single_1_sim4_fisher_mlines_with_regu_1e-08_0_005.png}
  \caption{Abrupt concept drift of the linear model with correlated {covariates} (lines are in different colors in electronic version). Comparison are made between before (left) and after (right) being transformed by {the inverse of the estimated} Fisher Information Matrix, {$\mathbf {I} ( {\bm{\theta}} ^{(0)})$.} For legibility, here only show MEWMA control chart of Hotelling $T^2$ (black) and EWMA control charts for the $1$st (blue), $5$th (red), $7$th (green), intercept (cyan), from the top to the bottom. The lines of monitored statistics in control charts have the same style and color with lines of their control limits correspondingly.}
  \label{fig:lin_reg_not_ind_X}
\end{figure}
Then, we manually introduce correlation between {covariates} by substituting $0.5 x_1 + 0.5 x_5$ and $ 0.3 x_1 + 0.3 x_2 + 0.4 x_6$ for $x_5$ and $x_6$ respectively, with others remaining the same. The EWMA and MEWMA control charts before (left) and after (right) transformed by {the inverse of the estimated} ${\mathbf {I}}(\bm { \theta}^{(0)})$ is as shown in Figure~\ref{fig:lin_reg_not_ind_X}. The blue (the $2$nd row) and green lines (the $4$th row) are for $x_1$ and $x_7$: transforming or not has no effect on those lines, where the coefficient for $x_1$ has mean {drift} but $x_7$ does not. However, according to the red line (the $3$rd row) for $x_5$, transformation changes it from having mean {drift} to not, indicating that this transformation successfully decouples concept drift from correlated covariates. This is not surprising, because ${\mathbf {I}} ^{-1}(\bm { \theta}^{(0)})$ equals to $E ^{-1} [\bm {X}\bm {X}^T]$, which does not depend on $ \bm { \theta} ^{(0)}$ or $ \bm { \theta} ^{(1)}$, meaning high-ordered term in Equation~(\ref{eqn:cd_decomp_fisher_approx}) is zero. However, in the next experiment for logistic regression, the concept drift is harder to decouple because multiplying ${\mathbf {I}} ^{-1}(\bm { \theta}^{(0)})$ can only approximately decouple concept drifts.

\item
\textbf{Gradual Concept Drifts with Correlated Covariates}
\label{ssss:lin_not_ind_pred_grad_cd}
\begin{figure}[!htbp]
\centering
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_6/reg/PII_neg_single_1_sim6_mlines_with_regu_1e-08_0_005.png}
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_6/reg/PII_neg_single_1_sim6_fisher_mlines_with_regu_1e-08_0_005.png}
 \caption{Gradual concept drift of the linear model with correlated {covariates} (lines are in different colors in electronic version). Comparison are made between before (left) and after (right) being transformed by {the inverse of the estimated} Fisher Information Matrix, {$\mathbf {I} ( {\bm{\theta}} ^{(0)})$.} For legibility, here only show MEWMA control chart of Hotelling $T^2$ (black) and EWMA control charts for the $1$st (blue), $5$th (red), $7$th (green), intercept (cyan), from the top to the bottom. The lines of monitored statistics in control charts have the same style and color with lines of their control limits correspondingly.}
  \label{fig:lin_reg_not_ind_X_grad_cd}
\end{figure}

In real applications, many concept drifts happen in a gradual way. In this data set, all conditions are the same with that in Experiment~\ref{ssss:lin_not_ind_pred} above except that the concept drifts happen linearly in the second half time of Phase-II. The starting and ending coefficients of the linear concept drift period correspond to those before and after concept drifts in the abrupt case. 
\begin{figure}[!htbp]
\centering
\includegraphics[width = 0.6\linewidth]{../figures/v14/sim_6/reg/1_sim6_lin_1e-08_0_005_1.png}
  \caption{Gradual concept drift of the linear model with correlated covariates. Hotelling $T^2$ of EWMA of the score function and EWMA of the absolute residual are compared.}
  \label{fig:lin_reg_ind_X_grad_cd_comp}
\end{figure}

Shown in Figure~\ref{fig:lin_reg_not_ind_X_grad_cd} are the EWMA and MEWMA control charts before (left) and after (right) transformed by {the inverse of the estimated} ${\mathbf {I}}(\bm { \theta}^{(0)})$. Similarly to that in Experiment~\ref{ssss:lin_not_ind_pred} above, lines corresponding to covariates with true concept drifts or independent with others are not affected by transformation in terms whether concept drifts show up in control charts. In Figure~\ref{fig:lin_reg_ind_X_grad_cd_comp}, we see that monitoring the score vectors gives an earlier detection of the starting position of the concept drift than EWMA of the absolute residual.
\end{enumerate}

\subsection{Simulated data set for Logistic Regression}
\label{sss:logi_exp}
\begin{enumerate}[(I)]
\item
\textbf{Abrupt Concept Drifts with Independent Covariates}
\label{ssss:log_ind_pred}
\begin{figure}[!htbp]
\centering
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_5/logi_no_muco/PII_pos_single_1_sim5_mlines_with_regu_1e-08_0_005.png}
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_5/logi_no_muco/PII_pos_single_1_sim5_fisher_mlines_with_regu_1e-08_0_005.png}
  \caption{Abrupt concept drift of the logistic regression with independent {covariates} (lines are in different colors in electronic version). Comparison are made between before (left) and after (right) being transformed by {the inverse of the estimated} Fisher Information Matrix, {$\mathbf {I} ( {\bm{\theta}} ^{(0)})$.} For legibility, here only show MEWMA control chart of Hotelling $T^2$ (black) and EWMA control charts for the $1$st (blue), $5$th (red), $7$th (green), intercept (cyan), from the top to the bottom. The lines of monitored statistics in control charts have the same style and color with lines of their control limits correspondingly.}
  \label{fig:log_reg_ind_X}
\end{figure}

This simulated data set has the same parameter as that in the corresponding Experiment~\ref{ssss:lin_ind_pred} in Appendix~\ref{sss:lin_exp}, except that here the model becomes the {logistic model} {$p(y=1|\bm {x})= \sigma (\bm {x}^T\bm { \theta})$,} where $\sigma (\cdot)$ is the sigmoid function.
\begin{figure}[!htbp]
\centering
\includegraphics[width = 0.6\linewidth]{../figures/v14/sim_5/logi_no_muco/1_sim5_logi_1e-08_0_005_1.png}
  \caption{Abrupt concept drift of the logistic regression with independent covariates. Hotelling $T^2$ of EWMA of the score function and EWMA of the error rate are compared.}
  \label{fig:log_reg_ind_X_comp}
\end{figure}

As shown in Figure~\ref{fig:log_reg_ind_X}, even though components of $\bm {x}$ are independent, the concept drifts of the first four components still propagate to the rest (with a smaller magnitude) which do not have drifts, except the intercept (because of the symmetry of distribution of our simulated $\bm {x}$). This is because that $E [ (\sigma ( \bm {X}^T\bm { \theta}^{(1)} ) - \sigma ( \bm {X}^T\bm { \theta}^{(0)} )) \bm {X}] $ is nonlinear and depends on parameters. This is supported by the comparison of the left and right column in Figure~\ref{fig:log_reg_ind_X}: applying transformation matrix cannot recover zero mean drift for those covariates without concept drift (the red line in the $3$rd row for $x_5$) meaning this coupling is due to the nonlinearity rather than correlation between covariates in Experiment~\ref{ssss:lin_not_ind_pred} in Appendix~\ref{sss:lin_exp}. During experiments, we find an interesting special case that if the concept drift is restricted to flipping sign of coefficients, the covariates without concept drift will not have mean {drift} (no coupling effect) in those control charts, due to the same reason that the intercept has no mean drift, mentioned above.

As the comparison in Experiment~\ref{ssss:lin_ind_pred} in Appendix~\ref{sss:lin_exp}, in Figure~\ref{fig:log_reg_ind_X_comp}, Hotelling $T^2$ of EWMA of the score function and EWMA of the prediction error, both methods can capture the abrupt concept drift, but the EWMA of the prediction error has much less signal ratio. This is because in Phase-I the prediction error has a wider range between control limits than that of the score function, so that in Phase-II the control chart is less sensitive to the deviation.

\item
\textbf{Abrupt Concept Drifts with Correlated Covariates}
\label{ssss:log_not_ind_pred}

\begin{figure}[!htbp]
\centering
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_5/logi/PII_pos_single_1_sim5_mlines_with_regu_1e-08_0_005.png}
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_5/logi/PII_pos_single_1_sim5_fisher_mlines_with_regu_1e-08_0_005.png}
  \caption{Abrupt concept drift of the logistic regression with correlated {covariates} (lines are in different colors in electronic version). Comparison are made between before (left) and after (right) being transformed by {the inverse of the estimated} Fisher Information Matrix, {$\mathbf {I} ( {\bm{\theta}} ^{(0)})$.} For legibility, here only show MEWMA control chart of Hotelling $T^2$ (black) and EWMA control charts for the $1$st (blue), $5$th (red), $7$th (green), intercept (cyan), from the top to the bottom. The lines of monitored statistics in control charts have the same style and color with lines of their control limits correspondingly.
}
  \label{fig:log_reg_not_ind_X}
\end{figure}

As in Experiment~\ref{ssss:lin_not_ind_pred} above, we introduce correlation between {covariates.} The comparison between before and after transformation with {the inverse of the estimated} {$\mathbf {I} ( {\bm{\theta}} ^{ (0)}) = E [{p} (\bm {X},\bm { \theta} ^{ (0)}) (1-{p}(\bm {X},\bm { \theta} ^{ (0)})) \bm {X} \bm {X}^T] \Delta \bm{ \theta}$}, where {$p (\bm {X},\bm { \theta} ^{ (0)}) = \sigma ( \bm {X}^T\bm { \theta} ^{ (0)})$}, is shown in Figure~\ref{fig:log_reg_not_ind_X}. The transformation makes the mean {drift} uniformly significant on control charts over all {covariates,} no matter whether they have concept drift (here only show representative lines). For the $x_7$ component (green in the $4$th row), previously mild concept drift due to nonlinearity becomes even larger (worse) after transformation. This indicates that the approximation in Equation (\ref{eqn:cd_decomp_fisher_approx}) is not accurate. This makes sense, because the concept drift is large (coefficients from $1$ to $-1$), violating the assumption of ``small higher-ordered terms of concept drift".

\item
\textbf{Concept Drifts with Correlated Covariates and an Assumption}
\label{ssss:log_not_ind_pred_assum}
\begin{figure}[!htbp]
\centering
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_5/logi_small/PII_pos_single_1_sim5_mlines_with_regu_1e-08_0_005.png}
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_5/logi_small/PII_pos_single_1_sim5_fisher_mlines_with_regu_1e-08_0_005.png}
  \caption{
 Abrupt concept drift of the logistic regression with correlated covariates (lines are in different colors in electronic version). Simulated data are modified to reduce the magnitude of $ \bm {x}_i^T\bm { \theta}^{(1)}$ and $  \bm {x}_i^T\bm { \theta}^{(0)}$. Comparison are made between before (left) and after (right) being transformed by the inverse of the estimated Fisher Information Matrix, $\mathbf {I} ( {\bm{\theta}} ^{(0)})$. For legibility, here only show MEWMA control chart of Hotelling $T^2$ (black) and EWMA control charts for the $1$st (blue), $5$th (red), $7$th (green), intercept (cyan), from the top to the bottom. The lines of monitored statistics in control charts have the same style and color with lines of their control limits correspondingly.
}
  \label{fig:log_reg_not_ind_X_1}
\end{figure}

To resolve the failure of diagnosing true concept drifts in Experiment~\ref{ssss:log_not_ind_pred}, we observe that, according to the mean drift of the logistic regression $E [ (\sigma (\bm {X}^T\bm { \theta}^{ (1)}) - \sigma ( \bm {X}^T\bm { \theta}^{ (0)})) \bm {X}] $, if $ \bm {x}^T\bm { \theta}^{ (1)}$ and $ \bm {x}^T\bm { \theta}^{ (0)}$ are all relative small (say $\ll 4$ in absolute value), the sigmoid function $ \sigma (\cdot)$ are in the linear region, so that we can approximate the mean drift by $E [ (\sigma ( \bm {X}^T\bm { \theta}^{ (1)} ) - \sigma ( \bm {X}^T\bm { \theta}^{ (0)} )) \bm {X}] \approx E [ \eta( \bm {X}^T\bm { \theta}^{ (1)} -  \bm {X}^T\bm { \theta}^{ (0)} ) \bm {X}] = \eta E[\bm{XX}^T] \Delta \bm { \theta} $, where $ \eta$ is just a scalar constant factor, reducing to the case of the linear model, as in Appendix~\ref{sss:lin_exp}.  

To test this argument, we follow the data set generating process in Experiment~\ref{ssss:log_not_ind_pred} in Appendix~\ref{sss:lin_exp} but make modifications as below: the coefficients of all covariates in training, validation, Phase-I, and the first half period of Phase-II changes to $0.2$; in the second half period of Phase-II, the coefficients of the first four covariates are multiplied by $-1$. In Figure~\ref{fig:log_reg_not_ind_X_1}, we observe that, transforming or not does not change the lines corresponding to covariates which are independent with others or truly have concept drifts in terms of whether to show concept drifts in control charts (the coefficient for $x_1$ (blue in the $2$nd row) has mean drift but $x_7$ (green in the $4$th row) and intercept (cyan in the last row) does not); but transformation changes the line $x_5$ (red in the $3$rd row) in the control charts from having mean drift to not. In other words, the concept drifts are successfully decoupled. Due to the reduced variance of covariates and smaller concept drifts, the variance inflation after concept drifts is also mitigated, as shown in the second half of Phase-II. Even though this requires some assumption, this property of the logistic regression is still practically valuable, because when arguments of the sigmoid function have a large absolute value, large concept drifts may not have much impact on the performance of predictive models (the sigmoid function is in the plateau regions), while when the sigmoid function is in the linear region, small drifts in coefficients can significantly change predictions, so that we might want to understand which covariates contribute to such change.

This property of the logistic regression can be extended to gradual concept drifts. Modifying the above data set as done in Experiment~\ref{ssss:lin_not_ind_pred_grad_cd} in Appendix~\ref{sss:lin_exp}, meaning change the abrupt concept drift to the linear one.
\begin{figure}[!htbp]
\centering
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_7/logi_small/PII_pos_single_1_sim7_mlines_with_regu_1e-08_0_005.png}
\includegraphics[width = 0.4\linewidth]{../figures/v14/sim_7/logi_small/PII_pos_single_1_sim7_fisher_mlines_with_regu_1e-08_0_005.png}
 \caption{Gradual concept drift of the logistic regression with correlated {covariates} (lines are in different colors in electronic version). Comparison are made between before (left) and after (right) being transformed by {the inverse of the estimated} Fisher Information Matrix, {$\mathbf {I} ( {\bm{\theta}} ^{(0)})$.} For legibility, here only show MEWMA control chart of Hotelling $T^2$ (black) and EWMA control charts for the $1$st (blue), $5$th (red), $7$th (green), intercept (cyan), from the top to the bottom. The lines of monitored statistics in control charts have the same style and color with lines of their control limits correspondingly.}
  \label{fig:log_reg_not_ind_X_grad_cd}
\end{figure}
\begin{figure}[!htbp]
\centering
\includegraphics[width = 0.6\linewidth]{../figures/v14/sim_7/logi_small/1_sim7_logi_1e-08_0_005_1.png}
  \caption{Gradual concept drift of the logistic regression with correlated covariates. Hotelling $T^2$ of EWMA of the score function and EWMA of the absolute residual are compared.}
  \label{fig:log_reg_ind_X_grad_cd_comp}
\end{figure}

The MEWMA and EWMA control charts before (left) and after (right) transformed by {the inverse of the estimated} ${\mathbf {I}}(\bm { \theta}^{(0)})$ is as shown in Figure~\ref{fig:log_reg_not_ind_X_grad_cd}. Similarly with that in Figure \ref{fig:log_reg_not_ind_X_1}, lines corresponding to covariates with true concept drifts or independent with others are not affected by transformation in terms whether to show concept drifts in figures, confirming the validity of the assumption in this section. In Figure~\ref{fig:log_reg_ind_X_grad_cd_comp}, we see that monitoring the score vectors renders an even much earlier detection of the starting position of the concept drift than EWMA of the prediction error rate.

In summary, we demonstrated the efficacy of using MEWMA control charts to monitor the score function in detecting concept drifts. And we also show how to diagnose the concept drifts by applying proper transformation and EWMA control charts. Also, we discussed the coupling due to multicollinearity and nonlinearity and how to resolve those issues. The practicability of the assumption of small argument for decoupling concept drifts in the logistic regression was also justified.
\end{enumerate}

\end{appendix}

%\listofchanges
\end{document}