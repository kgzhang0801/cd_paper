\ChangesListline {replaced}{Replaced\nobreakspace {}(Anh)}{Supervised learning models}{1}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Kungang)}{could be}{1}
\ChangesListline {deleted}{Deleted\nobreakspace {}(Anh)}{, as known as dataset drift}{1}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Kungang)}{, potentially rendering}{1}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{, as known as concept drift}{1}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Anh)}{concept drift}{1}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Kungang)}{supervised learning}{1}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Anh)}{a powerful statistical process control (SPC) technique, multivariate EWMA chart}{1}
\ChangesListline {deleted}{Deleted\nobreakspace {}(Anh)}{in Phase-I and Phase-II respectively}{1}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Kungang)}{the diagnosis method proposed can}{1}
\ChangesListline {deleted}{Deleted\nobreakspace {}(Anh)}{This is the first time in the concept drift literatures that a score-based method is developed.}{1}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Kungang)}{any parametric supervised learning}{1}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Kungang)}{as shown in experiments with various models on simulated and real datasets.}{2}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Kungang)}{The impressive performance of early detection and interpretability demonstrate the superiority of the proposed method over current popular ones.}{2}
\ChangesListline {deleted}{Deleted\nobreakspace {}(Anh)}{We also discuss some related results from econometric literatures and draw a connection between our and their methods.}{2}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Anh)}{stationary}{2}
\ChangesListline {added}{Added\nobreakspace {}(Anh)}{, where the stationarity refers to the conditional distribution, $P(Y|\bm {X})$}{2}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Anh)}{Concept drift}{2}
\ChangesListline {deleted}{Deleted\nobreakspace {}(Anh)}{The change in $P (Y|\bm {X}, \bm { \theta })$ can happen in training and serving models.}{2}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Kungang)}{potentially rule out}{2}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{as much as possible, through so-called retrospective analysis}{2}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{or diagnose}{2}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Kungang)}{The case with scale response can}{2}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{Concept drift is fairly common in practical applications. For example, in spam filtering, machine learning models can use features like words, spelling, and addresses to determine whether an email comes from a suspicious source. However, when spammers find their outgoing emails are blocked, they can invent new templates of emails trying to bypass the spam checking. Or some emails previously not regarded as spams become ones after a while. These phenomena in one case can originate from hidden effects or contexts not captured by machine learning models. When hidden effects don't change, the current model can approximate relationship between covariates and responses, if well-trained and validated. However, after the context drifts, the same model may have very poor predictability based on input variables. Battling against performance degradation resulting from concept drift is especially important in streaming data setting. Considering large amount of data and big scale of models, a good method for solving concept drift problems should be able to handle different kinds of drifts very fast without requiring too many data points. Also, to quickly pinpoint specific features or covariates contributing to a drift, a good method should also provide interpretive guidance.}{3}
\ChangesListline {deleted}{Deleted\nobreakspace {}(Anh)}{The concept drift problem have emerged in different fields, including machine learning\nobreakspace {}(cite{gama2004learning}), statistics\nobreakspace {}(cite{brown1975techniques}), and econometrics\nobreakspace {}(cite{kuan1995generalized}). Classification problems are especially interested in by machine learning communities.}{3}
\ChangesListline {deleted}{Deleted\nobreakspace {}(Kungang)}{In fields of statistics and econometrics, score-based testing methods\nobreakspace {}(cite{zeileis2005unified}) are developed for both regression and classification problems to handle changes of parameters, where the score function is defined as gradient of cost function (e.g., negative log-likelihood) of single observation.}{3}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{In this paper, we will discuss the limitation of using error-based methods, like missing potential concept drifts and opportunities of improving models (as illustrated in Figure\nobreakspace {}\ref {fig:logi_err_rate_unch}) and so on. To overcome those problems, we choose to monitor stochastic score function, defined as the gradient of log-likelihood.}{3}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{and quantitative}{3}
\ChangesListline {deleted}{Deleted\nobreakspace {}(Kungang)}{across the fields}{3}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{and diagnose}{3}
\ChangesListline {deleted}{Deleted\nobreakspace {}(Kungang)}{In this study, we develop a method of detecting concept drift .}{4}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Kungang)}{monitors}{4}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{descent (SGD)}{4}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Kungang)}{can detect the change of vectors in a direction invariant manner and the exponentially decayed weights smoothly distributed higher weights on recent data samples over earlier ones without relying too much on a single data point. The value of decaying parameters depends on specific applications and a good choice can find balance between false-alarm rate and detection speed.}{4}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{error-based}{4}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{for parametric models}{4}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Kungang)}{the gradient of log-likelihood as a function}{4}
\ChangesListline {deleted}{Deleted\nobreakspace {}(Kungang)}{Besides interests from mathematical perspective, concept drift is fairly common in practical applications. For example, in spam filtering, machine learning models can use features like words, spelling, and addresses to determine whether an email comes from a suspicious source. However, when spammers find their outgoing emails are blocked, they can invent new templates of emails trying to bypass the spam checking. Or some emails previously not regarded as spams become ones after a while. Another example is predicting efficacy of antibiotics using predictive models. With more usage of a certain antibiotics, bacterias become more resistant offsetting the efficacy, up to a point when it becomes virtually useless. In other cases, it can also be captured by some context variables which, instead of being explicitly used as covariates in training, are converted to weights to select historical data having similar context for training\nobreakspace {}(cite{barakat2018context}). The main difference in the two settings is that the former assumes no explicit knowledge in concept drift, while in the later case concept drifts are identified given candidate context variables. We clarify that this paper focuses on the former setting, which no prior knowledge of concept drift is required. These phenomena in one case can originate from hidden effects or contexts not captured by machine learning models. When hidden effects don't change, the current model can approximate relationship between covariates and responses as well as the one being trained. However, after the context drifts, the same model may have very poor predictability based on input variables. Battling against performance degradation resulting from concept drift is especially important in streaming data setting. Considering large amount of data and big scale of models, a good method for solving concept drift problems should be able to handle different kinds of drift very fast without requiring too many data points. Also, to quickly pinpoint specific features or covariates contributing to a drift, a good method should also provide interpretive guidance.}{5}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{through retrospective analysis}{5}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{Simulated datasets are also used to illustrate the advantages of score-based over error-based methods, in terms of detection speed and interpretability, under circumstances of abrupt/gradual concept drifts and with/without multi-colinearity. Overall, we show that monitoring score instead of error would give overall better performance and understanding in handling concept drift with very little extra cost.}{6}
\ChangesListline {deleted}{Deleted\nobreakspace {}(Anh)}{Four parametric models, linear, logistic, multinomial, and Poisson regression, are used as examples to investigate this method. For non-parametric or semi-parametric model, we investigate multi-layer perceptron (MLP) as an example. The MEWMA can be used to track overall concept drift with special treatment to handle models with a large amount of parameters, like MLP. For linear and mildly nonlinear models, it can also track concept drift of individual covariates. Simulated datasets are carefully designed to demonstrate that, for linear models, component-wise concept drift can be completely recovered even with multi-collinearity; for nonlinear models like logistic regression, our method can still be practically useful because, when concept drift happens and a model being trained is mostly affected (will be clarified later), we can linearize the decoupling matrix to get a good approximation and obtain decoupled concept drifts for covariates. {Furthermore, other methods monitoring EWMA of prediction residual or error rate are compared with our method showing that monitoring the score function can provide earlier detection.} Experimental studies on real datasets of credit risk and bike sharing prove that tracking sample score vectors is better than simply monitoring prediction error or residual and offers more interpretive results for improving performance of predictive models.}{6}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{in econometrics literatures}{7}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{can achieve earlier detection and}{7}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{Another restriction is that those literatures mainly focus on simple model, like generalized linear models, without touching more nonlinear ones, like neural networks.}{7}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{Those methods have an advantage of continuity, without a cold start once a good model has been trained. And because they have some kind of forgetting mechanism, they automatically track new concepts. However, because, each time a new data point or data batch coming in, they need to update the ensemble of models or add new models into the ensemble no matter whether concept drift happens, those methods can waste lots of computation budget. This disadvantage and the complexity of parameter tuning are probably the reason of the lack of popularity of those methods in industry.}{7}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{Another type of method in the concept drift handling methods is incremental learning. One common property of incremental learning is that it usually involves only one model and the model is maintained updated to incorporate current state of concept. Such learning algorithms are usually specifically designed for a subset of models, so that the applications of them are restricted.}{7}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{Both categories of methods are not designed to find the exact time when concept drift happened nor to provide diagnostic information.}{7}
\ChangesListline {deleted}{Deleted\nobreakspace {}(Kungang)}{In\nobreakspace {}(cite{katakis2008ensemble}), classifiers are trained using separated incoming batches of datasets. Those batches have vectors representing concepts and are accordingly clustered so that batches with a similar concept can be used to train a classifier in the ensemble. The author claimed that this method can incrementally cluster historical batches. Instead of using majority voting,\nobreakspace {}(cite{wang2003mining}) carefully weights members in an ensemble so that those with higher validation error would receive lower weights, so that the prediction accuracy would be higher, training ensembles using batches is less computationally demanding than training a single off-line model, and the error variance is smaller. This weighting strategy can further be refined using local accuracy, which is obtained by finding k-nearest neighbors of testing data in validation datasets, as shown in\nobreakspace {}(cite{tsymbal2008dynamic}). The accuracy is higher in this way, at the expense of more computation. One of latest reviews on ensemble learning for concept drift is\nobreakspace {}(cite{gomes2017survey}). Those methods have an advantage of continuity, without a cold start once a good model has been trained. And because they have some kind of forgetting mechanism, they automatically track new concepts. However, because, each time a new data point or data batch coming in, they need to update the ensemble of models or add new models into the ensemble no matter whether concept drift happens, those methods can waste lots of computation budget. This disadvantage and the complexity of parameter tuning are probably the reason of the lack of popularity of those methods in industry. Another type of method in the concept drift handling methods is incremental learning. One common property of incremental learning is that it usually involves only one model and the model is maintained updated to incorporate current state of concept. Such learning algorithms are usually specifically designed for a subset of models. For example, Naive Bayes is suitable for such kind of incremental learning, because the class-conditional distributions can be easily updated by counting\nobreakspace {}(cite{barakat2016context}, cite{katakis2006dynamic}). Two recent review papers on this topic are\nobreakspace {}(cite{losing2018incremental}, cite{lemaire2014survey}). Both categories of methods are not designed to find the exact time when concept drift happened nor to provide diagnostic information.}{8}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{and detect the time when it happens with as less delay as possible}{8}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{Notice that those methods can still be applied in online updating context, but here we focus on the performance of them after models has been well-trained and validated, because this problem setting is clearer and more fundamental.}{8}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{error-based}{9}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{}{9}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{error-based}{10}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Kungang)}{mini-batch Gradient Descent}{10}
\ChangesListline {deleted}{Deleted\nobreakspace {}(Anh)}{Notice that the expectation in (\ref {eqn:score_exp_zero}) is w.r.t. the conditional distribution, but the sample score vector, {$\bm {s} (\bm { \theta } ^{ (0)};(\bm {x}_i, y_i))$}, has randomness from not only $P_{\bm {\theta }} (Y|\bm {X})$ (where $\bm { \theta } = \bm { \theta }^{(0)}$ when no concept drift) but also $P (\bm {X})$. In other words, monitoring mean {drift} of {$\bm {s} (\bm { \theta }^{ (0)};(\bm {x}_i, y_i))$} using control charts (which will be formally introduced later) is practically implemented by monitoring drift of {$E _{ \bm { \theta }}[\bm {s} (\bm { \theta }^{ (0)};(\bm {X}, Y))]$}. To understand the relation, using iterative expectation, we have}{10}
\ChangesListline {deleted}{Deleted\nobreakspace {}(Anh)}{According to the definition, a concept drift corresponds to change in $E _{ \bm { \theta } }[\bm {s} (\bm { \theta } ^{ (0)};(\bm {X}, Y))| \bm {X}]$ from $\bm {0}$, because this indicates the change in $P(Y|\bm {X})$. In practice, we can not directly monitor $E _{ \bm { \theta } }[\bm {s} (\bm { \theta } ^{ (0)};(\bm {X}, Y))| \bm {X}]$, because of lack of control in randomness from $\bm {X}$. Instead, we actually monitor $E_{ \bm { \theta } }[\bm {s} (\bm { \theta } ^{ (0)};(\bm {X}, Y))]$. This actually makes sense, because the {drift} of the left-hand-side in (\ref {eqn:joint_expe}) implies the {drift} of inner expectation of the right-hand-side, meaning concept drift; the reverse is generally true except that {$E _{ \bm { \theta } }[\bm {s} (\bm { \theta } ^{ (0)};(\bm {X}, Y))| \bm {X}]$} has a specific form and non-zero values at different realizations of $\bm {X}$ cancel out after taking expectation w.r.t. $\bm {X}$. Under assumption of small changes and smooth conditions w.r.t the parameter, for generalized linear model, we can prove that concept drift in parametric models implies non-zero mean of score function. }{11}
\ChangesListline {deleted}{Deleted\nobreakspace {}(Kungang)}{linear model, logistic,}{11}
\ChangesListline {deleted}{Deleted\nobreakspace {}(Kungang)}{, and Poisson}{11}
\ChangesListline {deleted}{Deleted\nobreakspace {}(Kungang)}{all}{11}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Kungang)}{is}{11}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{or mini-batch Gradient Descent}{11}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{or mini-batch Gradient Descent}{11}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{After we obtaining the initial dataset, we can use MEWMA or score clustering to minimize concept drift in training data, as much as possible through retrospective analysis.}{13}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Kungang)}{With}{13}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{and validated}{13}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{}{15}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Kungang)}{The framework of monitoring/detecting concept drift based on the score function. (a) Conducting retrospective analysis using MEWMA and/or score function clustering to ensure there is no concept drift in the dataset used to train and set control limits. The size of dataset can be recursively reduced if concept drift exists. (b) Monitoring concept drifts using the model and control limits obtained by processing training and Phase-I datasets. In this subplot, three examples of possible results are given: gradual and abrupt concept drift and in-control case. (c) Visualization of diagnosing concept drift: The MEWMA for score vectors and the EWMA for individual predictors are visualized to show the origin of concept drift.}{16}
\ChangesListline {replaced}{Replaced\nobreakspace {}(Kungang)}{needs justification}{19}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{Notice that the expectation in (\ref {eqn:score_exp_zero}) is w.r.t. the conditional distribution, but the sample score vector, {$\bm {s} (\bm { \theta } ^{ (0)};(\bm {x}_i, y_i))$}, has randomness from not only $P_{\bm {\theta }} (Y|\bm {X})$ (where $\bm { \theta } = \bm { \theta }^{(0)}$ when no concept drift) but also $P (\bm {X})$. In other words, monitoring mean {drift} of {$\bm {s} (\bm { \theta }^{ (0)};(\bm {x}_i, y_i))$} using control charts (which will be formally introduced later) is practically implemented by monitoring drift of {$E _{ \bm { \theta }}[\bm {s} (\bm { \theta }^{ (0)};(\bm {X}, Y))]$}. To understand the relation, using iterative expectation, we have}{24}
\ChangesListline {added}{Added\nobreakspace {}(Kungang)}{According to the definition, a concept drift corresponds to change in $E _{ \bm { \theta } }[\bm {s} (\bm { \theta } ^{ (0)};(\bm {X}, Y))| \bm {X}]$ from $\bm {0}$, because this indicates the change in $P(Y|\bm {X})$. In practice, we can not directly monitor $E _{ \bm { \theta } }[\bm {s} (\bm { \theta } ^{ (0)};(\bm {X}, Y))| \bm {X}]$, because of lack of control in randomness from $\bm {X}$. Instead, we actually monitor $E_{ \bm { \theta } }[\bm {s} (\bm { \theta } ^{ (0)};(\bm {X}, Y))]$. This actually makes sense, because the {drift} of the left-hand-side in (\ref {eqn:joint_expe}) implies the {drift} of inner expectation of the right-hand-side, meaning concept drift; the reverse is generally true except that {$E _{ \bm { \theta } }[\bm {s} (\bm { \theta } ^{ (0)};(\bm {X}, Y))| \bm {X}]$} has a specific form and non-zero values at different realizations of $\bm {X}$ cancel out after taking expectation w.r.t. $\bm {X}$. As shown in Section\nobreakspace {}\ref {ss:non_zero_mean_score}, under assumption of small changes and smooth conditions w.r.t the parameter, for generalized linear model, concept drift in parametric models implies non-zero mean of score function.}{24}
